// Autogenerated using stratification.
requires "x86-configuration.k"

module PADDQ-XMM-XMM
  imports X86-CONFIGURATION

  rule <k>
    execinstr (paddq R1:Xmm, R2:Xmm,  .Typedoperands) => .
  ...</k>
    <regstate>
RSMap:Map => updateMap(RSMap,
convToRegKeys(R2) |-> (concatenateMInt(extractMInt(getParentValue(R2, RSMap), 0, 128), concatenateMInt(extractMInt(addMInt(concatenateMInt(mi(1, 0), extractMInt(getParentValue(R1, RSMap), 128, 192)), concatenateMInt(mi(1, 0), extractMInt(getParentValue(R2, RSMap), 128, 192))), 1, 65), extractMInt(addMInt(concatenateMInt(mi(1, 0), extractMInt(getParentValue(R1, RSMap), 192, 256)), concatenateMInt(mi(1, 0), extractMInt(getParentValue(R2, RSMap), 192, 256))), 1, 65))) )


)

    </regstate>
endmodule

module PADDQ-XMM-XMM-SEMANTICS
  imports PADDQ-XMM-XMM
endmodule
/*
TargetInstr:
paddq %xmm2, %xmm1
RWSet:
maybe read:{ %xmm1 %xmm2 }
must read:{ %xmm1 %xmm2 }
maybe write:{ %xmm1 }
must write:{ %xmm1 }
maybe undef:{ }
must undef:{ }
required flags:{ sse2 }

Circuit:
circuit:callq .move_128_064_xmm2_r10_r11  #  1     0     5      OPC=callq_label
circuit:callq .move_128_064_xmm1_r8_r9    #  2     0x5   5      OPC=callq_label
circuit:addq %r10, %r8                    #  3     0xa   3      OPC=addq_r64_r64
circuit:xchgw %r8w, %r8w                  #  4     0xd   4      OPC=xchgw_r16_r16
circuit:addq %r11, %r9                    #  5     0x11  3      OPC=addq_r64_r64
circuit:callq .move_064_128_r8_r9_xmm1    #  6     0x14  5      OPC=callq_label
BVF:
WARNING: No live out values provided, assuming { }
WARNING: No def in values provided; assuming { %mxcsr::rc[0] }
Target

paddq %xmm2, %xmm1

  maybe read:      { %xmm1 %xmm2 }
  must read:       { %xmm1 %xmm2 }
  maybe write:     { %xmm1 }
  must write:      { %xmm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { sse2 }

-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r8_r9

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r10, %r8

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r8/%r8: %ymm1_paddq_xmm_xmm[127:0][63:0]

%cf: %cf_paddq_xmm_xmm
%pf: %pf_paddq_xmm_xmm
%af: %af_paddq_xmm_xmm
%zf: %zf_paddq_xmm_xmm
%sf: %sf_paddq_xmm_xmm
%of: %of_paddq_xmm_xmm

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r8
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

Final state
%r8/%r8: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_016_008_bx_r10b_r11b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_008_cx_r8b_r9b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r10b_r11b_cx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r8b_r9b_bx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
=====================================
Computing circuit for xchgw %r8w, %r8w

.target:
callq .move_016_008_bx_r10b_r11b
callq .move_016_008_cx_r8b_r9b
callq .move_008_016_r10b_r11b_cx
callq .move_008_016_r8b_r9b_bx
retq 

Initial state:
%r8/%r8w: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

State for specgen instruction: xchgw %cx, %bx:
%rcx/%cx: %rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])
%rbx/%bx: %rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])

Register        -> %cx
  translates to => %r8w
Value is               -> (%rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Register        -> %bx
  translates to => %r8w
Value is               -> (%rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Final state
%r8/%r8w: ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

=====================================
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r11, %r9

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r9/%r9: %ymm1_paddq_xmm_xmm[127:0][127:64]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r9
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

Final state
%r9/%r9: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[67:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[67:64])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[127:127] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for paddq %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
callq .move_128_064_xmm1_r8_r9
addq %r10, %r8
xchgw %r8w, %r8w
addq %r11, %r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1[127:0]

State for specgen instruction: paddq %xmm2, %xmm1:
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

Final state
%xmm1: (%ymm1[255:128] ∘ ((0x0₁ ∘ %ymm2[127:64] + 0x0₁ ∘ %ymm1[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2[63:0] + 0x0₁ ∘ %ymm1[63:0])[63:0]))[127:0]

=====================================
Circuits:

%ymm1  : %ymm1[255:128] ∘ ((0x0₁ ∘ %ymm2[127:64] + 0x0₁ ∘ %ymm1[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2[63:0] + 0x0₁ ∘ %ymm1[63:0])[63:0])

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv

*/