// Autogenerated using stratification.
requires "x86-configuration.k"

module PHADDD-XMM-XMM
  imports X86-CONFIGURATION

  rule <k>
    execinstr (phaddd R1:Xmm, R2:Xmm,  .Typedoperands) => .
  ...</k>
    <regstate>
RSMap:Map => updateMap(RSMap,
convToRegKeys(R2) |-> (concatenateMInt(extractMInt(getParentValue(R2, RSMap), 0, 128), concatenateMInt(concatenateMInt(extractMInt(addMInt(concatenateMInt(mi(32, 0), extractMInt(getParentValue(R1, RSMap), 128, 160)), concatenateMInt(mi(32, 0), extractMInt(getParentValue(R1, RSMap), 160, 192))), 32, 64), extractMInt(addMInt(extractMInt(getParentValue(R1, RSMap), 192, 256), concatenateMInt(mi(32, 0), extractMInt(getParentValue(R1, RSMap), 192, 224))), 32, 64)), concatenateMInt(extractMInt(addMInt(concatenateMInt(extractMInt(getParentValue(R2, RSMap), 128, 160), extractMInt(getParentValue(R2, RSMap), 128, 160)), extractMInt(getParentValue(R2, RSMap), 128, 192)), 32, 64), extractMInt(addMInt(concatenateMInt(extractMInt(getParentValue(R2, RSMap), 192, 224), extractMInt(getParentValue(R2, RSMap), 192, 224)), extractMInt(getParentValue(R2, RSMap), 192, 256)), 32, 64)))) )


)

    </regstate>
endmodule

module PHADDD-XMM-XMM-SEMANTICS
  imports PHADDD-XMM-XMM
endmodule
/*
TargetInstr:
phaddd %xmm2, %xmm1
RWSet:
maybe read:{ %xmm1 %xmm2 }
must read:{ %xmm1 %xmm2 }
maybe write:{ %xmm1 }
must write:{ %xmm1 }
maybe undef:{ }
must undef:{ }
required flags:{ ssse3 }

Circuit:
circuit:movshdup %xmm1, %xmm0                           #  1     0     4      OPC=movshdup_xmm_xmm
circuit:callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11  #  2     0x4   5      OPC=callq_label
circuit:paddq %xmm0, %xmm1                              #  3     0x9   4      OPC=paddq_xmm_xmm
circuit:vpaddq %xmm9, %xmm2, %xmm6                      #  4     0xd   5      OPC=vpaddq_xmm_xmm_xmm
circuit:vunpcklpd %xmm9, %xmm1, %xmm4                   #  5     0x12  5      OPC=vunpcklpd_xmm_xmm_xmm
circuit:vmovhlps %xmm1, %xmm8, %xmm5                    #  6     0x17  4      OPC=vmovhlps_xmm_xmm_xmm
circuit:vpaddq %xmm10, %xmm11, %xmm7                    #  7     0x1b  5      OPC=vpaddq_xmm_xmm_xmm
circuit:callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1    #  8     0x20  5      OPC=callq_label
BVF:
WARNING: No live out values provided, assuming { }
WARNING: No def in values provided; assuming { %mxcsr::rc[0] }
Target

phaddd %xmm2, %xmm1

  maybe read:      { %xmm1 %xmm2 }
  must read:       { %xmm1 %xmm2 }
  maybe write:     { %xmm1 }
  must write:      { %xmm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { ssse3 }

-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_movshdup_xmm_xmm
%rdx/%rdx: %rdx_movshdup_xmm_xmm

%xmm0: %ymm0_movshdup_xmm_xmm[127:0]
%xmm1: %ymm1_movshdup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_movshdup_xmm_xmm
%rdx/%rdx: %rdx_movshdup_xmm_xmm

%xmm0: %ymm0_movshdup_xmm_xmm[127:0]
%xmm1: (%ymm1_movshdup_xmm_xmm[255:128] ∘ (%ymm2_movshdup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movshdup_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vmovshdup_xmm_xmm
%rdx/%rdx: %rdx_vmovshdup_xmm_xmm

%xmm0: %ymm0_vmovshdup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovshdup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovsldup_xmm_xmm
%rdx/%rdx: %rdx_vmovsldup_xmm_xmm

%xmm0: %ymm0_vmovsldup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovsldup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm2, %xmm9

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm13, %xmm5

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm5: %ymm5_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm5: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm5, %xmm9, %xmm1

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovsldup_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

=====================================
=====================================
Computing circuit for vmovsldup %xmm11, %xmm11

.target:
callq .move_128_64_xmm2_xmm12_xmm13
vbroadcastss %xmm2, %xmm9
vbroadcastss %xmm13, %xmm5
vpunpcklqdq %xmm5, %xmm9, %xmm1
retq 

Initial state:
%ymm11: %ymm11_vmovshdup_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovshdup_xmm_xmm[127:0][127:96])

State for specgen instruction: vmovsldup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

Final state
%ymm11: 0x0₁₂₈ ∘ (0x0₆₄ ∘ (%ymm2_vmovshdup_xmm_xmm[127:96] ∘ %ymm2_vmovshdup_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm11, %xmm3

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm3: %ymm3_vmovshdup_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ (0x0₆₄ ∘ (%ymm2_vmovshdup_xmm_xmm[127:96] ∘ %ymm2_vmovshdup_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovsldup_xmm_xmm
%rdx/%rdx: %rdx_vmovsldup_xmm_xmm

%xmm0: %ymm0_vmovsldup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovsldup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm2, %xmm9

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm13, %xmm5

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm5: %ymm5_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm5: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm5, %xmm9, %xmm1

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovsldup_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

=====================================
=====================================
Computing circuit for vmovsldup %xmm9, %xmm1

.target:
callq .move_128_64_xmm2_xmm12_xmm13
vbroadcastss %xmm2, %xmm9
vbroadcastss %xmm13, %xmm5
vpunpcklqdq %xmm5, %xmm9, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovshdup_xmm_xmm

State for specgen instruction: vmovsldup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

Final state
%ymm1: 0x0₁₂₈ ∘ (0x0₆₄ ∘ (%ymm2_vmovshdup_xmm_xmm[63:32] ∘ %ymm2_vmovshdup_xmm_xmm[63:32]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_unpcklpd_xmm_xmm
%rdx/%rdx: %rdx_unpcklpd_xmm_xmm

%xmm0: %ymm0_unpcklpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpcklpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm11: %ymm11_unpcklpd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_unpcklpd_xmm_xmm[127:0][127:64])

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_unpcklpd_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_64_128_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_unpcklpd_xmm_xmm
%rdx/%rdx: %rdx_unpcklpd_xmm_xmm

%xmm0: %ymm0_unpcklpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpcklpd_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ %ymm2_unpcklpd_xmm_xmm[127:0])[127:0][63:0] ∘ (%ymm10_unpcklpd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_unpcklpd_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpcklpd %xmm3, %xmm1

.target:
callq .move_128_64_xmm1_xmm10_xmm11
vmovdqu %xmm2, %xmm11
callq .move_64_128_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ (0x0₆₄ ∘ (%ymm2_vmovshdup_xmm_xmm[63:32] ∘ %ymm2_vmovshdup_xmm_xmm[63:32])))[127:0]

State for specgen instruction: unpcklpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpcklpd_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ %ymm2_unpcklpd_xmm_xmm[127:0])[127:0][63:0] ∘ (%ymm10_unpcklpd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_unpcklpd_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ (0x0₆₄ ∘ (%ymm2_vmovshdup_xmm_xmm[63:32] ∘ %ymm2_vmovshdup_xmm_xmm[63:32])))[255:128] ∘ (%ymm2_vmovshdup_xmm_xmm[127:96] ∘ %ymm2_vmovshdup_xmm_xmm[127:96] ∘ (%ymm2_vmovshdup_xmm_xmm[63:32] ∘ %ymm2_vmovshdup_xmm_xmm[63:32])))[127:0]

=====================================
=====================================
Computing circuit for vmovshdup %xmm1, %xmm8

.target:
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovsldup %xmm11, %xmm11
vmovdqa %xmm11, %xmm3
vmovsldup %xmm9, %xmm1
unpcklpd %xmm3, %xmm1
retq 

Initial state:
%ymm8: %ymm8_movshdup_xmm_xmm

State for specgen instruction: vmovshdup %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (0x0₆₄ ∘ (%ymm2_vmovshdup_xmm_xmm[63:32] ∘ %ymm2_vmovshdup_xmm_xmm[63:32])))[255:128] ∘ (%ymm2_vmovshdup_xmm_xmm[127:96] ∘ %ymm2_vmovshdup_xmm_xmm[127:96] ∘ (%ymm2_vmovshdup_xmm_xmm[63:32] ∘ %ymm2_vmovshdup_xmm_xmm[63:32]))

Final state
%ymm8: 0x0₁₂₈ ∘ (%ymm2_movshdup_xmm_xmm[127:96] ∘ %ymm2_movshdup_xmm_xmm[127:96] ∘ (%ymm2_movshdup_xmm_xmm[63:32] ∘ %ymm2_movshdup_xmm_xmm[63:32]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: %ymm1_movaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movaps %xmm8, %xmm1

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: (%ymm1_movshdup_xmm_xmm[255:128] ∘ (%ymm2_movshdup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movshdup_xmm_xmm[127:0][63:0][63:0]))[127:0]

State for specgen instruction: movaps %xmm2, %xmm1:
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: ((%ymm1_movshdup_xmm_xmm[255:128] ∘ (%ymm2_movshdup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movshdup_xmm_xmm[127:0][63:0][63:0]))[255:128] ∘ (%ymm2_movshdup_xmm_xmm[127:96] ∘ %ymm2_movshdup_xmm_xmm[127:96] ∘ %ymm2_movshdup_xmm_xmm[63:32] ∘ %ymm2_movshdup_xmm_xmm[63:32]))[127:0]

=====================================
=====================================
Computing circuit for movshdup %xmm1, %xmm0

.target:
callq .move_128_064_xmm2_r10_r11
callq .move_064_128_r10_r11_xmm1
vmovshdup %xmm1, %xmm8
movaps %xmm8, %xmm1
retq 

Initial state:
%xmm0: %ymm0_phaddd_xmm_xmm[127:0]

State for specgen instruction: movshdup %xmm2, %xmm1:
%xmm1: ((%ymm1_movshdup_xmm_xmm[255:128] ∘ (%ymm2_movshdup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movshdup_xmm_xmm[127:0][63:0][63:0]))[255:128] ∘ (%ymm2_movshdup_xmm_xmm[127:96] ∘ %ymm2_movshdup_xmm_xmm[127:96] ∘ %ymm2_movshdup_xmm_xmm[63:32] ∘ %ymm2_movshdup_xmm_xmm[63:32]))[127:0]

Final state
%xmm0: (%ymm0_phaddd_xmm_xmm[255:128] ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_phaddd_xmm_xmm
%rdx/%rdx: %rdx_phaddd_xmm_xmm

%xmm0: (%ymm0_phaddd_xmm_xmm[255:128] ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]))[127:0]
%xmm1: %ymm1_phaddd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r8_r9

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r10, %r8

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r8/%r8: %ymm1_paddq_xmm_xmm[127:0][63:0]

%cf: %cf_paddq_xmm_xmm
%pf: %pf_paddq_xmm_xmm
%af: %af_paddq_xmm_xmm
%zf: %zf_paddq_xmm_xmm
%sf: %sf_paddq_xmm_xmm
%of: %of_paddq_xmm_xmm

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r8
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

Final state
%r8/%r8: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_016_008_bx_r10b_r11b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_008_cx_r8b_r9b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r10b_r11b_cx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r8b_r9b_bx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
=====================================
Computing circuit for xchgw %r8w, %r8w

.target:
callq .move_016_008_bx_r10b_r11b
callq .move_016_008_cx_r8b_r9b
callq .move_008_016_r10b_r11b_cx
callq .move_008_016_r8b_r9b_bx
retq 

Initial state:
%r8/%r8w: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

State for specgen instruction: xchgw %cx, %bx:
%rcx/%cx: %rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])
%rbx/%bx: %rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])

Register        -> %cx
  translates to => %r8w
Value is               -> (%rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Register        -> %bx
  translates to => %r8w
Value is               -> (%rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Final state
%r8/%r8w: ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

=====================================
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r11, %r9

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r9/%r9: %ymm1_paddq_xmm_xmm[127:0][127:64]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r9
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

Final state
%r9/%r9: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[67:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[67:64])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[127:127] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for paddq %xmm0, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
callq .move_128_064_xmm1_r8_r9
addq %r10, %r8
xchgw %r8w, %r8w
addq %r11, %r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_phaddd_xmm_xmm[127:0]

State for specgen instruction: paddq %xmm2, %xmm1:
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

Final state
%xmm1: (%ymm1_phaddd_xmm_xmm[255:128] ∘ ((0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[63:0])[63:0]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r8_r9

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r10, %r8

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r8/%r8: %ymm1_paddq_xmm_xmm[127:0][63:0]

%cf: %cf_paddq_xmm_xmm
%pf: %pf_paddq_xmm_xmm
%af: %af_paddq_xmm_xmm
%zf: %zf_paddq_xmm_xmm
%sf: %sf_paddq_xmm_xmm
%of: %of_paddq_xmm_xmm

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r8
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

Final state
%r8/%r8: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_016_008_bx_r10b_r11b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_008_cx_r8b_r9b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r10b_r11b_cx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r8b_r9b_bx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
=====================================
Computing circuit for xchgw %r8w, %r8w

.target:
callq .move_016_008_bx_r10b_r11b
callq .move_016_008_cx_r8b_r9b
callq .move_008_016_r10b_r11b_cx
callq .move_008_016_r8b_r9b_bx
retq 

Initial state:
%r8/%r8w: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

State for specgen instruction: xchgw %cx, %bx:
%rcx/%cx: %rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])
%rbx/%bx: %rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])

Register        -> %cx
  translates to => %r8w
Value is               -> (%rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Register        -> %bx
  translates to => %r8w
Value is               -> (%rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Final state
%r8/%r8w: ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

=====================================
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r11, %r9

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r9/%r9: %ymm1_paddq_xmm_xmm[127:0][127:64]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r9
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

Final state
%r9/%r9: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[67:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[67:64])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[127:127] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for paddq %xmm2, %xmm3

.target:
callq .move_128_064_xmm2_r10_r11
callq .move_128_064_xmm1_r8_r9
addq %r10, %r8
xchgw %r8w, %r8w
addq %r11, %r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm3: %ymm3_vpaddq_xmm_xmm_xmm[127:0]

State for specgen instruction: paddq %xmm2, %xmm1:
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

Final state
%xmm3: (%ymm3_vpaddq_xmm_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[127:64] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[63:0] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[63:0])[63:0]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpaddq_xmm_xmm_xmm

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[127:64] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[63:0] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[63:0])[63:0])

=====================================
=====================================
Computing circuit for vpaddq %xmm9, %xmm2, %xmm6

.target:
paddq %xmm2, %xmm3
vmovups %xmm3, %xmm1
retq 

Initial state:
%ymm6: %ymm6_phaddd_xmm_xmm

State for specgen instruction: vpaddq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[127:64] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[63:0] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[63:0])[63:0])

Final state
%ymm6: 0x0₁₂₈ ∘ ((0x0₁ ∘ %ymm2_phaddd_xmm_xmm[127:64] + 0x0₁ ∘ 0x0₆₄)[63:0] ∘ (0x0₁ ∘ %ymm2_phaddd_xmm_xmm[63:0] + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[63:32]))[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm3, %r8

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r8/%r8: %r8_vunpcklpd_xmm_xmm_xmm

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r8
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

Final state
%r8/%r8: %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vunpcklpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpcklpd_xmm_xmm_xmm

%xmm0: %ymm0_vunpcklpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vunpcklpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vunpcklpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpcklpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vunpcklpd_xmm_xmm_xmm[63:0][63:0] ∘ %ymm2_vunpcklpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vunpcklpd %xmm9, %xmm1, %xmm4

.target:
movq %xmm3, %r8
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm4: %ymm4_phaddd_xmm_xmm

State for specgen instruction: vunpcklpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vunpcklpd_xmm_xmm_xmm[63:0][63:0] ∘ %ymm2_vunpcklpd_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm4: 0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[63:32] ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[63:0])[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vmovhlps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovhlps_xmm_xmm_xmm

%xmm0: %ymm0_vmovhlps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vmovhlps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovhlps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovhlps_xmm_xmm_xmm

%xmm0: %ymm0_vmovhlps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vmovhlps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r11, %r12

Final state:
%r12/%r12: %ymm3_vmovhlps_xmm_xmm_xmm[127:0][127:64]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovhlps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovhlps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovhlps_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vmovhlps_xmm_xmm_xmm[127:0][127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovhlps %xmm1, %xmm8, %xmm5

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r12_r13
vzeroall 
movq %r11, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm5: %ymm5_phaddd_xmm_xmm

State for specgen instruction: vmovhlps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovhlps_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vmovhlps_xmm_xmm_xmm[127:0][127:64][63:0])

Final state
%ymm5: 0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[127:64])[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r8_r9

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: %ymm1_paddq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r10, %r8

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r8/%r8: %ymm1_paddq_xmm_xmm[127:0][63:0]

%cf: %cf_paddq_xmm_xmm
%pf: %pf_paddq_xmm_xmm
%af: %af_paddq_xmm_xmm
%zf: %zf_paddq_xmm_xmm
%sf: %sf_paddq_xmm_xmm
%of: %of_paddq_xmm_xmm

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r8
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

Final state
%r8/%r8: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_016_008_bx_r10b_r11b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_008_cx_r8b_r9b

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r10b_r11b_cx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r8b_r9b_bx

Final state:
%rax/%rax: %rax_xchgw_r16_r16
%rdx/%rdx: %rdx_xchgw_r16_r16

%xmm0: %ymm0_xchgw_r16_r16[127:0]
%xmm1: %ymm1_xchgw_r16_r16[127:0]

-------------------------------------
=====================================
Computing circuit for xchgw %r8w, %r8w

.target:
callq .move_016_008_bx_r10b_r11b
callq .move_016_008_cx_r8b_r9b
callq .move_008_016_r10b_r11b_cx
callq .move_008_016_r8b_r9b_bx
retq 

Initial state:
%r8/%r8w: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0]

State for specgen instruction: xchgw %cx, %bx:
%rcx/%cx: %rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])
%rbx/%bx: %rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0])

Register        -> %cx
  translates to => %r8w
Value is               -> (%rcx_xchgw_r16_r16[63:16] ∘ ((%r11_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r10_xchgw_r16_r16[63:8] ∘ %rbx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Register        -> %bx
  translates to => %r8w
Value is               -> (%rbx_xchgw_r16_r16[63:16] ∘ ((%r9_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][15:8])[7:0][7:0] ∘ (%r8_xchgw_r16_r16[63:8] ∘ %rcx_xchgw_r16_r16[15:0][7:0])[7:0][7:0]))[15:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

Final state
%r8/%r8w: ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0]

=====================================
-------------------------------------
Getting base circuit for orq %rbx, %rbx

Final state:
%rbx/%rbx: %rbx_addq_r64_r64 | %rbx_addq_r64_r64

%cf: false
%pf: !((%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][0:0] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][1:1] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][2:2] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][3:3] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][4:4] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][5:5] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][6:6] = 0x1₁ ⊕ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[7:0][7:7] = 0x1₁)
%zf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64) = 0x0₆₄
%sf: (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcq %rcx, %rbx

Final state:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

-------------------------------------
=====================================
Computing circuit for addq %r11, %r9

.target:
orq %rbx, %rbx
adcq %rcx, %rbx
retq 

Initial state:
%r9/%r9: %ymm1_paddq_xmm_xmm[127:0][127:64]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[3:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[3:0])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[63:63] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[63:63] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:63] = 0x1₁)

State for specgen instruction: addq %rcx, %rbx:
%rbx/%rbx: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]

%cf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[64:64] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rcx_addq_r64_r64[3:0] + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0] = 0x0₆₄
%sf: ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0][63:63] = 0x1₁
%of: (%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64)[63:63] = 0x1₁) ∧ !(%rcx_addq_r64_r64[63:63] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:63] = 0x1₁)

Register        -> %rbx
  translates to => %r9
Value is               -> ((false ? 0x0₁ ∘ %rcx_addq_r64_r64 + 0x1₆₅ : 0x0₁ ∘ %rcx_addq_r64_r64) + 0x0₁ ∘ (%rbx_addq_r64_r64 | %rbx_addq_r64_r64))[63:0]
  after renaming it is => (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

Final state
%r9/%r9: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0]

%cf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[64:64] = 0x1₁
%pf: !((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[0:0] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[1:1] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[2:2] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[3:3] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[4:4] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[5:5] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[6:6] = 0x1₁ ⊕ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[7:7] = 0x1₁)
%af: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[67:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[67:64])[4:4] = 0x1₁
%zf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0] = 0x0₆₄
%sf: (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁
%of: (%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ %ymm1_paddq_xmm_xmm[127:127] = 0x1₁) ∧ !(%ymm2_paddq_xmm_xmm[127:127] = 0x1₁ ↔ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:63] = 0x1₁)

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_paddq_xmm_xmm
%rdx/%rdx: %rdx_paddq_xmm_xmm

%xmm0: %ymm0_paddq_xmm_xmm[127:0]
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for paddq %xmm2, %xmm3

.target:
callq .move_128_064_xmm2_r10_r11
callq .move_128_064_xmm1_r8_r9
addq %r10, %r8
xchgw %r8w, %r8w
addq %r11, %r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm3: %ymm3_vpaddq_xmm_xmm_xmm[127:0]

State for specgen instruction: paddq %xmm2, %xmm1:
%xmm1: (%ymm1_paddq_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_paddq_xmm_xmm[127:64] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[127:64])[63:0][63:0] ∘ (((0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[63:0][63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:16] ∘ (0x0₁ ∘ %ymm2_paddq_xmm_xmm[63:0] + 0x0₁ ∘ %ymm1_paddq_xmm_xmm[63:0])[15:0])[63:0]))[127:0]

Final state
%xmm3: (%ymm3_vpaddq_xmm_xmm_xmm[255:128] ∘ ((0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[127:64] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[63:0] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[63:0])[63:0]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpaddq_xmm_xmm_xmm

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[127:64] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[63:0] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[63:0])[63:0])

=====================================
=====================================
Computing circuit for vpaddq %xmm10, %xmm11, %xmm7

.target:
paddq %xmm2, %xmm3
vmovups %xmm3, %xmm1
retq 

Initial state:
%ymm7: %ymm7_phaddd_xmm_xmm

State for specgen instruction: vpaddq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[127:64] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ %ymm2_vpaddq_xmm_xmm_xmm[63:0] + 0x0₁ ∘ %ymm3_vpaddq_xmm_xmm_xmm[63:0])[63:0])

Final state
%ymm7: 0x0₁₂₈ ∘ ((0x0₁ ∘ 0x0₆₄ + 0x0₁ ∘ 0x0₆₄)[63:0] ∘ (0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[95:64]))[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_phaddd_xmm_xmm
%rdx/%rdx: %rdx_phaddd_xmm_xmm

%xmm0: (%ymm0_phaddd_xmm_xmm[255:128] ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]))[127:0]
%xmm1: ((%ymm1_phaddd_xmm_xmm[255:128] ∘ ((0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[63:0])[63:0]))[255:128] ∘ ((0x0₁₂₈ ∘ ((0x0₁ ∘ 0x0₆₄ + 0x0₁ ∘ 0x0₆₄)[63:0] ∘ (0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[95:64]))[63:0]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ ((0x0₁ ∘ %ymm2_phaddd_xmm_xmm[127:64] + 0x0₁ ∘ 0x0₆₄)[63:0] ∘ (0x0₁ ∘ %ymm2_phaddd_xmm_xmm[63:0] + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[63:32]))[63:0]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[127:64])[63:0]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[63:32] ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[63:0])[63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for phaddd %xmm2, %xmm1

.target:
movshdup %xmm1, %xmm0
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
paddq %xmm0, %xmm1
vpaddq %xmm9, %xmm2, %xmm6
vunpcklpd %xmm9, %xmm1, %xmm4
vmovhlps %xmm1, %xmm8, %xmm5
vpaddq %xmm10, %xmm11, %xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: %ymm1[127:0]

State for specgen instruction: phaddd %xmm2, %xmm1:
%xmm1: ((%ymm1_phaddd_xmm_xmm[255:128] ∘ ((0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[127:64])[63:0] ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[63:0])[63:0]))[255:128] ∘ ((0x0₁₂₈ ∘ ((0x0₁ ∘ 0x0₆₄ + 0x0₁ ∘ 0x0₆₄)[63:0] ∘ (0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[95:64]))[63:0]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ ((0x0₁ ∘ %ymm2_phaddd_xmm_xmm[127:64] + 0x0₁ ∘ 0x0₆₄)[63:0] ∘ (0x0₁ ∘ %ymm2_phaddd_xmm_xmm[63:0] + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[63:32]))[63:0]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[127:96] ∘ %ymm1_phaddd_xmm_xmm[127:96]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[127:64])[63:0]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_phaddd_xmm_xmm[63:32] ∘ (0x0₁ ∘ (%ymm1_phaddd_xmm_xmm[63:32] ∘ %ymm1_phaddd_xmm_xmm[63:32]) + 0x0₁ ∘ %ymm1_phaddd_xmm_xmm[63:0])[63:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1[255:128] ∘ ((0x0₁ ∘ (0x0₃₂ ∘ %ymm2[127:96]) + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2[95:64]))[31:0] ∘ (0x0₁ ∘ %ymm2[63:0] + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2[63:32]))[31:0] ∘ (0x0₁ ∘ (%ymm1[127:96] ∘ %ymm1[127:96]) + 0x0₁ ∘ %ymm1[127:64])[31:0] ∘ (0x0₁ ∘ (%ymm1[63:32] ∘ %ymm1[63:32]) + 0x0₁ ∘ %ymm1[63:0])[31:0]))[127:0]

=====================================
Circuits:

%ymm1  : %ymm1[255:128] ∘ ((0x0₁ ∘ (0x0₃₂ ∘ %ymm2[127:96]) + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2[95:64]))[31:0] ∘ (0x0₁ ∘ %ymm2[63:0] + 0x0₁ ∘ (0x0₃₂ ∘ %ymm2[63:32]))[31:0] ∘ (0x0₁ ∘ (%ymm1[127:96] ∘ %ymm1[127:96]) + 0x0₁ ∘ %ymm1[127:64])[31:0] ∘ (0x0₁ ∘ (%ymm1[63:32] ∘ %ymm1[63:32]) + 0x0₁ ∘ %ymm1[63:0])[31:0])

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv

*/