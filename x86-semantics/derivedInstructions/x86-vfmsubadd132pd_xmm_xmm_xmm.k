// Autogenerated using stratification.
requires "x86-configuration.k"

module VFMSUBADD132PD-XMM-XMM-XMM
  imports X86-CONFIGURATION

  rule <k>
    execinstr (vfmsubadd132pd R1:Xmm, R2:Xmm, R3:Xmm,  .Typedoperands) => .
  ...</k>
    <regstate>
RSMap:Map => updateMap(RSMap,
convToRegKeys(R3) |-> (concatenateMInt(mi(128, 0), concatenateMInt(Float2MInt( ( MInt2Float(extractMInt(getParentValue(R2, RSMap), 128, 192), 53, 11)  -Float  ( MInt2Float(extractMInt(getParentValue(R3, RSMap), 128, 192), 53, 11)  *Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 128, 192), 53, 11) )  ) , 64), Float2MInt( (  ( MInt2Float(extractMInt(getParentValue(R2, RSMap), 192, 256), 53, 11)  -Float  negateFloat( ( MInt2Float(extractMInt(getParentValue(R2, RSMap), 192, 256), 53, 11)  *Float  MInt2Float(concatenateMInt(mi(32, 0), extractMInt(getParentValue(R2, RSMap), 128, 160)), 53, 11) ) ) )  -Float  ( MInt2Float(extractMInt(getParentValue(R3, RSMap), 192, 256), 53, 11)  *Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 192, 256), 53, 11) )  ) , 64))) )


)

    </regstate>
endmodule

module VFMSUBADD132PD-XMM-XMM-XMM-SEMANTICS
  imports VFMSUBADD132PD-XMM-XMM-XMM
endmodule
/*
TargetInstr:
vfmsubadd132pd %xmm3, %xmm2, %xmm1
RWSet:
maybe read:{ %xmm1 %xmm2 %xmm3 }
must read:{ %xmm1 %xmm2 %xmm3 }
maybe write:{ %ymm1 }
must write:{ %ymm1 }
maybe undef:{ }
must undef:{ }
required flags:{ fma }

Circuit:
circuit:vfmsubadd231pd %xmm3, %xmm1, %xmm2  #  1     0     5      OPC=vfmsubadd231pd_xmm_xmm_xmm
circuit:callq .move_128_064_xmm2_r8_r9      #  2     0x5   5      OPC=callq_label
circuit:vzeroall                            #  3     0xa   3      OPC=vzeroall
circuit:callq .move_064_128_r8_r9_xmm1      #  4     0xd   5      OPC=callq_label
BVF:
WARNING: No live out values provided, assuming { }
WARNING: No def in values provided; assuming { %mxcsr::rc[0] }
Target

vfmsubadd132pd %xmm3, %xmm2, %xmm1

  maybe read:      { %xmm1 %xmm2 %xmm3 }
  must read:       { %xmm1 %xmm2 %xmm3 }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { fma }

-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vfmsubadd231pd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfmsubadd231pd_xmm_xmm_xmm

%xmm0: %ymm0_vfmsubadd231pd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm3

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm3: %ymm3_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm11: %ymm11_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vmaxps %ymm3, %ymm11, %ymm1

Final state:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vmaxps %xmm3, %xmm3, %xmm8

.target:
vmovdqa %xmm3, %xmm3
vmovdqa %xmm2, %xmm11
vmaxps %ymm3, %ymm11, %ymm1
retq 

Initial state:
%ymm8: %ymm8_vminpd_xmm_xmm_xmm

State for specgen instruction: vmaxps %xmm3, %xmm2, %xmm1:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm8: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vminpd_xmm_xmm_xmm

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vminpd %ymm8, %ymm1, %ymm1

Final state:
%ymm1: (mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[255:192], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[255:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[255:192] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[255:192]) ∘ ((mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[191:128], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[191:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[191:128] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[191:128]) ∘ ((mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[127:64], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[127:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[127:64] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[127:64]) ∘ (mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[63:0], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[63:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[63:0] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[63:0])))

-------------------------------------
=====================================
Computing circuit for vminpd %xmm1, %xmm1, %xmm8

.target:
vmaxps %xmm3, %xmm3, %xmm8
vmovups %xmm2, %xmm1
vminpd %ymm8, %ymm1, %ymm1
retq 

Initial state:
%ymm8: %ymm8_vfmsubadd231pd_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:0][31:0])

State for specgen instruction: vminpd %xmm3, %xmm2, %xmm1:
%ymm1: (mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[255:192], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[255:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[255:192] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[255:192]) ∘ ((mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[191:128], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[191:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[191:128] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[191:128]) ∘ ((mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[127:64], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[127:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[127:64] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[127:64]) ∘ (mincmp_double((0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[63:0], (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[63:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminpd_xmm_xmm_xmm[127:0])[63:0] : (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm3_vminpd_xmm_xmm_xmm[127:0]))))[63:0])))

Final state
%ymm8: 0x0₆₄ ∘ (0x0₆₄ ∘ %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm3, %xmm3, %xmm6

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm6: %ymm6_vfnmsub231pd_xmm_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm6: 0x0₁₂₈ ∘ %ymm3_vfnmsub231pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm1, %xmm3

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm3: %ymm3_vfnmsub231pd_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm1_vfnmsub231pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: %ymm1_movaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movaps %xmm6, %xmm1

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: %ymm1_vfnmsub231pd_xmm_xmm_xmm[127:0]

State for specgen instruction: movaps %xmm2, %xmm1:
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1_vfnmsub231pd_xmm_xmm_xmm[255:128] ∘ %ymm3_vfnmsub231pd_xmm_xmm_xmm[127:0])[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm3, %xmm14

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm14: %ymm14_vfnmsub132pd_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm14: 0x0₁₂₈ ∘ %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm8

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm8: %ymm8_vfnmsub132pd_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm1: %ymm1_vminss_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm2_vminss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_minss_xmm_xmm
%rdx/%rdx: %rdx_minss_xmm_xmm

%xmm0: %ymm0_minss_xmm_xmm[127:0]
%xmm1: %ymm1_minss_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_minss_xmm_xmm
%rdx/%rdx: %rdx_minss_xmm_xmm

%xmm0: %ymm0_minss_xmm_xmm[127:0]
%xmm1: %ymm1_minss_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vminps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm14

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm14: %ymm14_vminps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm14: 0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vminps %ymm1, %ymm14, %ymm1

Final state:
%ymm1: (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32]) ∘ (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vminps %xmm2, %xmm1, %xmm2

.target:
vmovdqa %xmm3, %xmm1
vmovdqu %xmm2, %xmm14
vminps %ymm1, %ymm14, %ymm1
retq 

Initial state:
%ymm2: %ymm2_minps_xmm_xmm

State for specgen instruction: vminps %xmm3, %xmm2, %xmm1:
%ymm1: (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32]) ∘ (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm2: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0])))))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_minps_xmm_xmm
%rdx/%rdx: %rdx_minps_xmm_xmm

%xmm0: %ymm0_minps_xmm_xmm[127:0]
%xmm1: %ymm1_minps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_minps_xmm_xmm
%rdx/%rdx: %rdx_minps_xmm_xmm

%xmm0: %ymm0_minps_xmm_xmm[127:0]
%xmm1: (%ymm1_minps_xmm_xmm[255:128] ∘ ((%ymm11_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][63:32]))[127:0][31:0] ∘ (%ymm8_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for minps %xmm4, %xmm8

.target:
vminps %xmm2, %xmm1, %xmm2
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm8: (%ymm8_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][31:0]))[127:0]

State for specgen instruction: minps %xmm2, %xmm1:
%xmm1: (%ymm1_minps_xmm_xmm[255:128] ∘ ((%ymm11_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][63:32]))[127:0][31:0] ∘ (%ymm8_minps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[127:96], %ymm2_minps_xmm_xmm[127:96]) = 0x1₁ ? %ymm1_minps_xmm_xmm[127:96] : %ymm2_minps_xmm_xmm[127:96]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[95:64], %ymm2_minps_xmm_xmm[95:64]) = 0x1₁ ? %ymm1_minps_xmm_xmm[95:64] : %ymm2_minps_xmm_xmm[95:64]) ∘ ((mincmp_single(%ymm1_minps_xmm_xmm[63:32], %ymm2_minps_xmm_xmm[63:32]) = 0x1₁ ? %ymm1_minps_xmm_xmm[63:32] : %ymm2_minps_xmm_xmm[63:32]) ∘ (mincmp_single(%ymm1_minps_xmm_xmm[31:0], %ymm2_minps_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minps_xmm_xmm[31:0] : %ymm2_minps_xmm_xmm[31:0]))))))))[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm8: ((%ymm8_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][31:0]))[255:128] ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (mincmp_single(%ymm1_minss_xmm_xmm[31:0], %ymm2_minss_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minss_xmm_xmm[31:0] : %ymm2_minss_xmm_xmm[31:0])))))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_minss_xmm_xmm
%rdx/%rdx: %rdx_minss_xmm_xmm

%xmm0: %ymm0_minss_xmm_xmm[127:0]
%xmm1: (%ymm1_minss_xmm_xmm[255:128] ∘ ((%ymm11_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ ((%ymm8_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][31:0]))[255:128] ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (mincmp_single(%ymm1_minss_xmm_xmm[31:0], %ymm2_minss_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minss_xmm_xmm[31:0] : %ymm2_minss_xmm_xmm[31:0])))))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for minss %xmm3, %xmm1

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
minps %xmm4, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ %ymm2_vminss_xmm_xmm_xmm[127:0])[127:0]

State for specgen instruction: minss %xmm2, %xmm1:
%xmm1: (%ymm1_minss_xmm_xmm[255:128] ∘ ((%ymm11_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ ((%ymm8_minss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_minss_xmm_xmm[127:0][31:0]))[255:128] ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (mincmp_single(%ymm1_minss_xmm_xmm[31:0], %ymm2_minss_xmm_xmm[31:0]) = 0x1₁ ? %ymm1_minss_xmm_xmm[31:0] : %ymm2_minss_xmm_xmm[31:0])))))[127:0][31:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ %ymm2_vminss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vminss_xmm_xmm_xmm[127:32] ∘ (mincmp_single(%ymm2_vminss_xmm_xmm_xmm[31:0], %ymm3_vminss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vminss_xmm_xmm_xmm[31:0] : %ymm3_vminss_xmm_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for vminss %xmm1, %xmm1, %xmm1

.target:
vmovdqu %xmm2, %xmm1
minss %xmm3, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vfnmsub132pd_xmm_xmm_xmm

State for specgen instruction: vminss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ %ymm2_vminss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vminss_xmm_xmm_xmm[127:32] ∘ (mincmp_single(%ymm2_vminss_xmm_xmm_xmm[31:0], %ymm3_vminss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vminss_xmm_xmm_xmm[31:0] : %ymm3_vminss_xmm_xmm_xmm[31:0]))

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm1_vfnmsub132pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vfnmsub132pd %ymm1, %ymm3, %ymm2

Final state:
%ymm2: vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0])))

-------------------------------------
-------------------------------------
Getting base circuit for vmaxpd %ymm2, %ymm2, %ymm1

Final state:
%ymm1: (maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192]) ∘ ((maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128]) ∘ ((maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64]) ∘ (maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0])))

-------------------------------------
=====================================
Computing circuit for vfnmsub213pd %ymm8, %ymm1, %ymm14

.target:
vfnmsub132pd %ymm1, %ymm3, %ymm2
vmaxpd %ymm2, %ymm2, %ymm1
retq 

Initial state:
%ymm14: 0x0₁₂₈ ∘ %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:0]

State for specgen instruction: vfnmsub213pd %ymm3, %ymm2, %ymm1:
%ymm1: (maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[255:192]) ∘ ((maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[191:128]) ∘ ((maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[127:64]) ∘ (maxcmp_double((vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0], (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0])[0:0] = 0x1₁ ? (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0] : (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm3_vfnmsub213pd_ymm_ymm_ymm[255:192], %ymm1_vfnmsub213pd_ymm_ymm_ymm[255:192]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm3_vfnmsub213pd_ymm_ymm_ymm[191:128], %ymm1_vfnmsub213pd_ymm_ymm_ymm[191:128]) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm3_vfnmsub213pd_ymm_ymm_ymm[127:64], %ymm1_vfnmsub213pd_ymm_ymm_ymm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm3_vfnmsub213pd_ymm_ymm_ymm[63:0], %ymm1_vfnmsub213pd_ymm_ymm_ymm[63:0]))))[63:0])))

Final state
%ymm14: vnfmsub132_double(0x0₆₄, 0x0₆₄, 0x0₆₄) ∘ (vnfmsub132_double(0x0₆₄, 0x0₆₄, 0x0₆₄) ∘ (vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132pd_xmm_xmm_xmm[63:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm3

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm3: %ymm3_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm11: %ymm11_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vmaxps %ymm3, %ymm11, %ymm1

Final state:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vmaxps %xmm14, %xmm14, %xmm1

.target:
vmovdqa %xmm3, %xmm3
vmovdqa %xmm2, %xmm11
vmaxps %ymm3, %ymm11, %ymm1
retq 

Initial state:
%ymm1: 0x0₁₂₈ ∘ %ymm1_vfnmsub132pd_xmm_xmm_xmm[127:0]

State for specgen instruction: vmaxps %xmm3, %xmm2, %xmm1:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm1: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132pd_xmm_xmm_xmm[63:0])))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm2, %xmm2

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm2: %ymm2_por_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ %ymm2_por_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm1, %xmm2, %xmm3

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm3: %ymm3_por_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_por_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for por %xmm14, %xmm1

.target:
vmovapd %xmm2, %xmm2
vorpd %xmm1, %xmm2, %xmm3
movdqa %xmm3, %xmm1
retq 

Initial state:
%xmm1: (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132pd_xmm_xmm_xmm[63:0]))))))[127:0]

State for specgen instruction: por %xmm2, %xmm1:
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: ((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132pd_xmm_xmm_xmm[63:0]))))))[255:128] ∘ (vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132pd_xmm_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for vfnmsub132pd %xmm1, %xmm3, %xmm2

.target:
vmovapd %xmm3, %xmm14
vmovdqu %xmm2, %xmm8
vminss %xmm1, %xmm1, %xmm1
vfnmsub213pd %ymm8, %ymm1, %ymm14
vmaxps %xmm14, %xmm14, %xmm1
por %xmm14, %xmm1
retq 

Initial state:
%ymm2: %ymm2_vfnmsub213pd_xmm_xmm_xmm

State for specgen instruction: vfnmsub132pd %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132pd_xmm_xmm_xmm[63:0]))))))[255:128] ∘ (vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132pd_xmm_xmm_xmm[63:0]))

Final state
%ymm2: 0x0₃₂ ∘ (0x0₃₂ ∘ 0x0₆₄) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm1_vfnmsub213pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm1_vfnmsub213pd_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vfnmsub213pd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfnmsub213pd_xmm_xmm_xmm

%xmm0: %ymm0_vfnmsub213pd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vfnmsub213pd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vfnmsub213pd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfnmsub213pd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₃₂ ∘ (0x0₃₂ ∘ 0x0₆₄) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm1_vfnmsub213pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm1_vfnmsub213pd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ 0x0₆₄) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm1_vfnmsub213pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm1_vfnmsub213pd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vfnmsub213pd %xmm3, %xmm2, %xmm1

.target:
vfnmsub132pd %xmm1, %xmm3, %xmm2
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfnmsub231pd_xmm_xmm_xmm[255:128] ∘ %ymm3_vfnmsub231pd_xmm_xmm_xmm[127:0]

State for specgen instruction: vfnmsub213pd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₃₂ ∘ (0x0₃₂ ∘ 0x0₆₄) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm1_vfnmsub213pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm1_vfnmsub213pd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ 0x0₆₄) ∘ (vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub213pd_xmm_xmm_xmm[127:64], %ymm1_vfnmsub213pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub213pd_xmm_xmm_xmm[63:0], %ymm1_vfnmsub213pd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (vnfmsub132_double(%ymm2_vfnmsub231pd_xmm_xmm_xmm[127:64], %ymm1_vfnmsub231pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub231pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfnmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub231pd_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vfnmsub231pd %xmm3, %xmm1, %xmm2

.target:
vorpd %xmm3, %xmm3, %xmm6
vmovapd %xmm1, %xmm3
movaps %xmm6, %xmm1
vfnmsub213pd %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm2: %ymm2_vfnmsub132sd_xmm_xmm_xmm

State for specgen instruction: vfnmsub231pd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (vnfmsub132_double(%ymm2_vfnmsub231pd_xmm_xmm_xmm[127:64], %ymm1_vfnmsub231pd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub231pd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm2_vfnmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfnmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub231pd_xmm_xmm_xmm[63:0]))

Final state
%ymm2: 0x0₁₂₈ ∘ (vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[127:64], %ymm2_vfnmsub132sd_xmm_xmm_xmm[127:64], %ymm3_vfnmsub132sd_xmm_xmm_xmm[127:64]) ∘ vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132sd_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_movsd_xmm_xmm
%rdx/%rdx: %rdx_movsd_xmm_xmm

%xmm0: %ymm0_movsd_xmm_xmm[127:0]
%xmm1: %ymm1_movsd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movups_xmm_xmm
%rdx/%rdx: %rdx_movups_xmm_xmm

%xmm0: %ymm0_movups_xmm_xmm[127:0]
%xmm1: %ymm1_movups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movups_xmm_xmm
%rdx/%rdx: %rdx_movups_xmm_xmm

%xmm0: %ymm0_movups_xmm_xmm[127:0]
%xmm1: (%ymm1_movups_xmm_xmm[255:128] ∘ ((%ymm7_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movups %xmm2, %xmm8

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm8: (%ymm8_movsd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movsd_xmm_xmm[127:0][63:0]))[127:0]

State for specgen instruction: movups %xmm2, %xmm1:
%xmm1: (%ymm1_movups_xmm_xmm[255:128] ∘ ((%ymm7_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm8: ((%ymm8_movsd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movsd_xmm_xmm[127:0][63:0]))[255:128] ∘ %ymm2_movsd_xmm_xmm[127:0])[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_64_128_xmm8_xmm9_xmm1

Final state:
%rax/%rax: %rax_movsd_xmm_xmm
%rdx/%rdx: %rdx_movsd_xmm_xmm

%xmm0: %ymm0_movsd_xmm_xmm[127:0]
%xmm1: (%ymm1_movsd_xmm_xmm[255:128] ∘ ((%ymm9_movsd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movsd_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ ((%ymm8_movsd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movsd_xmm_xmm[127:0][63:0]))[255:128] ∘ %ymm2_movsd_xmm_xmm[127:0])[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movsd %xmm2, %xmm1

.target:
callq .move_128_64_xmm1_xmm8_xmm9
movups %xmm2, %xmm8
callq .move_64_128_xmm8_xmm9_xmm1
retq 

Initial state:
%xmm1: %ymm1_vfnmsub132sd_xmm_xmm_xmm[127:0]

State for specgen instruction: movsd %xmm2, %xmm1:
%xmm1: (%ymm1_movsd_xmm_xmm[255:128] ∘ ((%ymm9_movsd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movsd_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ ((%ymm8_movsd_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movsd_xmm_xmm[127:0][63:0]))[255:128] ∘ %ymm2_movsd_xmm_xmm[127:0])[127:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_vfnmsub132sd_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132sd_xmm_xmm_xmm[63:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_vfnmsub132sd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfnmsub132sd_xmm_xmm_xmm

%xmm0: %ymm0_vfnmsub132sd_xmm_xmm_xmm[127:0]
%xmm1: (%ymm1_vfnmsub132sd_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132sd_xmm_xmm_xmm[63:0])))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm1, %xmm12

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm12: %ymm12_vfmadd231ps_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm12: 0x0₁₂₈ ∘ %ymm1_vfmadd231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm6

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm6: %ymm6_vfmadd231ps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm6: 0x0₁₂₈ ∘ %ymm2_vfmadd231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: %ymm0_vmovaps_xmm_xmm[127:0]
%xmm1: %ymm1_vmovaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovaps %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmadd231ps_xmm_xmm_xmm

State for specgen instruction: vmovaps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vfmadd231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vfmadd132ps %ymm1, %ymm3, %ymm2

Final state:
%ymm2: vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for vmaxpd %ymm2, %ymm2, %ymm1

Final state:
%ymm1: (maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192]) ∘ ((maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128]) ∘ ((maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64]) ∘ (maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0])))

-------------------------------------
=====================================
Computing circuit for vfmadd213ps %ymm12, %ymm6, %ymm1

.target:
vfmadd132ps %ymm1, %ymm3, %ymm2
vmaxpd %ymm2, %ymm2, %ymm1
retq 

Initial state:
%ymm1: 0x0₁₂₈ ∘ %ymm3_vfmadd231ps_xmm_xmm_xmm[127:0]

State for specgen instruction: vfmadd213ps %ymm3, %ymm2, %ymm1:
%ymm1: (maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[255:192]) ∘ ((maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[191:128]) ∘ ((maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[127:64]) ∘ (maxcmp_double((vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0], (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0])[0:0] = 0x1₁ ? (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0] : (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm3_vfmadd213ps_ymm_ymm_ymm[255:224], %ymm1_vfmadd213ps_ymm_ymm_ymm[255:224]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm3_vfmadd213ps_ymm_ymm_ymm[223:192], %ymm1_vfmadd213ps_ymm_ymm_ymm[223:192]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm3_vfmadd213ps_ymm_ymm_ymm[191:160], %ymm1_vfmadd213ps_ymm_ymm_ymm[191:160]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm3_vfmadd213ps_ymm_ymm_ymm[159:128], %ymm1_vfmadd213ps_ymm_ymm_ymm[159:128]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm3_vfmadd213ps_ymm_ymm_ymm[127:96], %ymm1_vfmadd213ps_ymm_ymm_ymm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm3_vfmadd213ps_ymm_ymm_ymm[95:64], %ymm1_vfmadd213ps_ymm_ymm_ymm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm3_vfmadd213ps_ymm_ymm_ymm[63:32], %ymm1_vfmadd213ps_ymm_ymm_ymm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm3_vfmadd213ps_ymm_ymm_ymm[31:0], %ymm1_vfmadd213ps_ymm_ymm_ymm[31:0]))))))))[63:0])))

Final state
%ymm1: vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmadd231ps_xmm_xmm_xmm[127:96], %ymm3_vfmadd231ps_xmm_xmm_xmm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmadd231ps_xmm_xmm_xmm[95:64], %ymm3_vfmadd231ps_xmm_xmm_xmm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmadd231ps_xmm_xmm_xmm[63:32], %ymm3_vfmadd231ps_xmm_xmm_xmm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ps_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ps_xmm_xmm_xmm[31:0]))))))

=====================================
=====================================
Computing circuit for vfmadd231ps %xmm1, %xmm2, %xmm3

.target:
vmovupd %xmm1, %xmm12
vmovdqu %xmm2, %xmm6
vmovaps %xmm3, %xmm1
vfmadd213ps %ymm12, %ymm6, %ymm1
retq 

Initial state:
%ymm3: %ymm3_vfmadd213ss_xmm_xmm_xmm

State for specgen instruction: vfmadd231ps %xmm3, %xmm2, %xmm1:
%ymm1: vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmadd231ps_xmm_xmm_xmm[127:96], %ymm3_vfmadd231ps_xmm_xmm_xmm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmadd231ps_xmm_xmm_xmm[95:64], %ymm3_vfmadd231ps_xmm_xmm_xmm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmadd231ps_xmm_xmm_xmm[63:32], %ymm3_vfmadd231ps_xmm_xmm_xmm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ps_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ps_xmm_xmm_xmm[31:0]))))))

Final state
%ymm3: vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmadd132_single(%ymm2_vfmadd213ss_xmm_xmm_xmm[127:96], %ymm3_vfmadd213ss_xmm_xmm_xmm[127:96], %ymm1_vfmadd213ss_xmm_xmm_xmm[127:96]) ∘ (vfmadd132_single(%ymm2_vfmadd213ss_xmm_xmm_xmm[95:64], %ymm3_vfmadd213ss_xmm_xmm_xmm[95:64], %ymm1_vfmadd213ss_xmm_xmm_xmm[95:64]) ∘ (vfmadd132_single(%ymm2_vfmadd213ss_xmm_xmm_xmm[63:32], %ymm3_vfmadd213ss_xmm_xmm_xmm[63:32], %ymm1_vfmadd213ss_xmm_xmm_xmm[63:32]) ∘ vfmadd132_single(%ymm2_vfmadd213ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd213ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd213ss_xmm_xmm_xmm[31:0]))))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: %ymm0_vrcpps_xmm_xmm[127:0]
%xmm1: %ymm1_vrcpps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vrcpps %ymm1, %ymm10

Final state:
%ymm10: approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vrcpps %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
vrcpps %ymm1, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmovss_xmm_xmm_xmm

State for specgen instruction: vrcpps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm8

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm8: %ymm8_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovss %xmm3, %xmm1, %xmm1

.target:
vrcpps %xmm3, %xmm1
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovdqu %xmm3, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmadd213ss_xmm_xmm_xmm

State for specgen instruction: vmovss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm1_vfmadd213ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd213ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd213ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd213ss_xmm_xmm_xmm[31:0]))

=====================================
=====================================
Computing circuit for vfmadd213ss %xmm1, %xmm2, %xmm3

.target:
vfmadd231ps %xmm1, %xmm2, %xmm3
vmovss %xmm3, %xmm1, %xmm1
retq 

Initial state:
%ymm3: %ymm3_vfmadd231ss_xmm_xmm_xmm

State for specgen instruction: vfmadd213ss %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm1_vfmadd213ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd213ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd213ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd213ss_xmm_xmm_xmm[31:0]))

Final state
%ymm3: 0x0₁₂₈ ∘ (%ymm3_vfmadd231ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ss_xmm_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: %ymm1_movss_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: %ymm0_vpmovzxdq_xmm_xmm[127:0]
%xmm1: %ymm1_vpmovzxdq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_byte_5_of_ymm1_to_r9b

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm2

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_64_128_xmm8_xmm9_xmm1

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpmovzxdq %xmm2, %xmm8

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_byte_5_of_ymm1_to_r9b
callq .move_064_128_r8_r9_xmm2
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
callq .move_64_128_xmm8_xmm9_xmm1
retq 

Initial state:
%ymm8: %ymm8_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][31:0])

State for specgen instruction: vpmovzxdq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movss %xmm3, %xmm1

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
vpmovzxdq %xmm2, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: %ymm1_vfmadd231ss_xmm_xmm_xmm[127:0]

State for specgen instruction: movss %xmm2, %xmm1:
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1_vfmadd231ss_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ss_xmm_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r8_r9

Final state:
%rax/%rax: %rax_vfmadd231ss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfmadd231ss_xmm_xmm_xmm

%xmm0: %ymm0_vfmadd231ss_xmm_xmm_xmm[127:0]
%xmm1: (%ymm1_vfmadd231ss_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ss_xmm_xmm_xmm[31:0])))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vfmadd231ss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfmadd231ss_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm1_vfmadd231ss_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ss_xmm_xmm_xmm[31:0])))[127:0][127:64][63:0] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ss_xmm_xmm_xmm[31:0])))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vfmadd231ss %xmm14, %xmm12, %xmm1

.target:
vfmadd213ss %xmm1, %xmm2, %xmm3
movss %xmm3, %xmm1
callq .move_128_064_xmm1_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: 0x0₂₅₆

State for specgen instruction: vfmadd231ss %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm1_vfmadd231ss_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ss_xmm_xmm_xmm[31:0])))[127:0][127:64][63:0] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfmadd231ss_xmm_xmm_xmm[127:32] ∘ vfmadd132_single(%ymm2_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm1_vfmadd231ss_xmm_xmm_xmm[31:0], %ymm3_vfmadd231ss_xmm_xmm_xmm[31:0])))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (0x0₉₆ ∘ vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂))

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vfnmsub132sd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfnmsub132sd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₁₂₈ ∘ (0x0₉₆ ∘ vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂)))[255:128] ∘ ((%ymm1_vfnmsub132sd_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132sd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132sd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vfnmsub132sd %xmm11, %xmm8, %xmm1

.target:
vfnmsub231pd %xmm3, %xmm1, %xmm2
movsd %xmm2, %xmm1
callq .move_128_064_xmm1_r10_r11
vzeroall 
vfmadd231ss %xmm14, %xmm12, %xmm1
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmsubadd231pd_xmm_xmm_xmm

State for specgen instruction: vfnmsub132sd %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (0x0₉₆ ∘ vfmadd132_single(0x0₃₂, 0x0₃₂, 0x0₃₂)))[255:128] ∘ ((%ymm1_vfnmsub132sd_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132sd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[255:128] ∘ (%ymm1_vfnmsub132sd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm2_vfnmsub132sd_xmm_xmm_xmm[63:0], %ymm3_vfnmsub132sd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vmovsd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovsd_xmm_xmm_xmm

%xmm0: %ymm0_vmovsd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vmovsd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm3, %xmm13

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm13: %ymm13_vmovsd_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm13: 0x0₁₂₈ ∘ %ymm3_vmovsd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm9, %xmm13, %xmm1

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovsd_xmm_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsd_xmm_xmm_xmm[127:64] ∘ %ymm3_vmovsd_xmm_xmm_xmm[63:0])

=====================================
=====================================
Computing circuit for vmovsd %xmm3, %xmm3, %xmm0

.target:
callq .move_128_64_xmm2_xmm8_xmm9
vmovapd %xmm3, %xmm13
vpunpcklqdq %xmm9, %xmm13, %xmm1
retq 

Initial state:
%ymm0: %ymm0_vfmsub132pd_xmm_xmm_xmm

State for specgen instruction: vmovsd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsd_xmm_xmm_xmm[127:64] ∘ %ymm3_vmovsd_xmm_xmm_xmm[63:0])

Final state
%ymm0: 0x0₁₂₈ ∘ %ymm3_vfmsub132pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm2, %xmm7

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm7: %ymm7_vfmsub132pd_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm7: 0x0₁₂₈ ∘ %ymm2_vfmsub132pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmaxss_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_vmaxss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmaxss_xmm_xmm_xmm

%xmm0: %ymm0_vmaxss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm3

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm3: %ymm3_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm11: %ymm11_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vmaxps %ymm3, %ymm11, %ymm1

Final state:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vmaxps %xmm3, %xmm4, %xmm13

.target:
vmovdqa %xmm3, %xmm3
vmovdqa %xmm2, %xmm11
vmaxps %ymm3, %ymm11, %ymm1
retq 

Initial state:
%ymm13: %ymm13_vmaxss_xmm_xmm_xmm

State for specgen instruction: vmaxps %xmm3, %xmm2, %xmm1:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm13: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[127:96]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[127:96]) ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[95:64]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[95:64]) ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[63:32]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[63:32]) ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0])))))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: %ymm1_movss_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: %ymm0_vpmovzxdq_xmm_xmm[127:0]
%xmm1: %ymm1_vpmovzxdq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_byte_5_of_ymm1_to_r9b

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm2

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_64_128_xmm8_xmm9_xmm1

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpmovzxdq %xmm2, %xmm8

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_byte_5_of_ymm1_to_r9b
callq .move_064_128_r8_r9_xmm2
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
callq .move_64_128_xmm8_xmm9_xmm1
retq 

Initial state:
%ymm8: %ymm8_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][31:0])

State for specgen instruction: vpmovzxdq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movss %xmm13, %xmm1

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
vpmovzxdq %xmm2, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[127:0]

State for specgen instruction: movss %xmm2, %xmm1:
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vmaxss_xmm_xmm_xmm[127:32] ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for vmaxss %xmm7, %xmm2, %xmm13

.target:
vmovupd %xmm2, %xmm1
callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7
vmaxps %xmm3, %xmm4, %xmm13
movss %xmm13, %xmm1
retq 

Initial state:
%ymm13: %ymm13_vfmsub132pd_xmm_xmm_xmm

State for specgen instruction: vmaxss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vmaxss_xmm_xmm_xmm[127:32] ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0]))

Final state
%ymm13: 0x0₁₂₈ ∘ %ymm2_vfmsub132pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm1, %xmm15

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm15: %ymm15_vfmsub132pd_xmm_xmm_xmm

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm15: 0x0₁₂₈ ∘ %ymm1_vfmsub132pd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vfmsub132pd %ymm3, %ymm1, %ymm2

Final state:
%ymm2: vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0])))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vfmsub231pd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vfmsub231pd_ymm_ymm_ymm

%xmm0: %ymm0_vfmsub231pd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vfmsub231pd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vminpd %ymm2, %ymm2, %ymm1

Final state:
%ymm1: (mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192]) ∘ ((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128]) ∘ ((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64]) ∘ (mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0])))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vfmsub231pd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vfmsub231pd_ymm_ymm_ymm

%xmm0: %ymm0_vfmsub231pd_ymm_ymm_ymm[127:0]
%xmm1: (((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192]) ∘ ((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128]) ∘ ((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64]) ∘ (mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0]))))[255:128] ∘ ((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:0][127:64][63:0] ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vfmsub231pd %ymm0, %ymm15, %ymm13

.target:
vfmsub132pd %ymm3, %ymm1, %ymm2
callq .move_128_064_xmm2_r10_r11
vminpd %ymm2, %ymm2, %ymm1
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm13: 0x0₁₂₈ ∘ %ymm2_vfmsub132pd_xmm_xmm_xmm[127:0]

State for specgen instruction: vfmsub231pd %ymm3, %ymm2, %ymm1:
%ymm1: ((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[255:192]) ∘ ((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[191:128]) ∘ ((mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:64]) ∘ (mincmp_double((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0], (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0])[0:0] = 0x1₁ ? (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0] : (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[63:0]))))[255:128] ∘ ((vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:0][127:64][63:0] ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm1_vfmsub231pd_ymm_ymm_ymm[255:192], %ymm3_vfmsub231pd_ymm_ymm_ymm[255:192]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm1_vfmsub231pd_ymm_ymm_ymm[191:128], %ymm3_vfmsub231pd_ymm_ymm_ymm[191:128]) ∘ (vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm1_vfmsub231pd_ymm_ymm_ymm[127:64], %ymm3_vfmsub231pd_ymm_ymm_ymm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm1_vfmsub231pd_ymm_ymm_ymm[63:0], %ymm3_vfmsub231pd_ymm_ymm_ymm[63:0]))))[127:0][63:0][63:0])

Final state
%ymm13: vfmsub132_double(0x0₆₄, 0x0₆₄, 0x0₆₄) ∘ vfmsub132_double(0x0₆₄, 0x0₆₄, 0x0₆₄) ∘ (vfmsub132_double(%ymm1_vfmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub132pd_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm13, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmsub132pd_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm1_vfmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub132pd_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vfmsub132pd %xmm3, %xmm1, %xmm2

.target:
vmovsd %xmm3, %xmm3, %xmm0
vmovapd %xmm2, %xmm7
vmaxss %xmm7, %xmm2, %xmm13
vmovups %xmm1, %xmm15
vfmsub231pd %ymm0, %ymm15, %ymm13
vmovupd %xmm13, %xmm1
retq 

Initial state:
%ymm2: %ymm2_vfmsub231pd_xmm_xmm_xmm

State for specgen instruction: vfmsub132pd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm1_vfmsub132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsub132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsub132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsub132pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub132pd_xmm_xmm_xmm[63:0]))

Final state
%ymm2: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm1_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub231pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub231pd_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: %ymm1_movaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movaps %xmm2, %xmm1

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: %ymm1_vfmsub231pd_xmm_xmm_xmm[127:0]

State for specgen instruction: movaps %xmm2, %xmm1:
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1_vfmsub231pd_xmm_xmm_xmm[255:128] ∘ (vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm1_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub231pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub231pd_xmm_xmm_xmm[63:0])[63:32] ∘ vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub231pd_xmm_xmm_xmm[63:0])[31:0]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vmovsd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovsd_xmm_xmm_xmm

%xmm0: %ymm0_vmovsd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vmovsd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm3, %xmm13

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm13: %ymm13_vmovsd_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm13: 0x0₁₂₈ ∘ %ymm3_vmovsd_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm9, %xmm13, %xmm1

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovsd_xmm_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsd_xmm_xmm_xmm[127:64] ∘ %ymm3_vmovsd_xmm_xmm_xmm[63:0])

=====================================
=====================================
Computing circuit for vmovsd %xmm2, %xmm1, %xmm1

.target:
callq .move_128_64_xmm2_xmm8_xmm9
vmovapd %xmm3, %xmm13
vpunpcklqdq %xmm9, %xmm13, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmsub231pd_xmm_xmm_xmm[255:128] ∘ (vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm1_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub231pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub231pd_xmm_xmm_xmm[63:0])[63:32] ∘ vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub231pd_xmm_xmm_xmm[63:0])[31:0])

State for specgen instruction: vmovsd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsd_xmm_xmm_xmm[127:64] ∘ %ymm3_vmovsd_xmm_xmm_xmm[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm1_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub231pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub231pd_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vfmsub231pd %xmm3, %xmm2, %xmm1

.target:
vfmsub132pd %xmm3, %xmm1, %xmm2
movaps %xmm2, %xmm1
vmovsd %xmm2, %xmm1, %xmm1
retq 

Initial state:
%ymm1: 0x0₁₂₈ ∘ (%ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:64] ∘ vnfmsub132_double(%ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:96]))

State for specgen instruction: vfmsub231pd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm1_vfmsub231pd_xmm_xmm_xmm[127:64], %ymm3_vfmsub231pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsub231pd_xmm_xmm_xmm[63:0], %ymm3_vfmsub231pd_xmm_xmm_xmm[63:0]))

Final state
%ymm1: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm2_vfmsubadd231pd_xmm_xmm_xmm[127:64], %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:64], %ymm3_vfmsubadd231pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsubadd231pd_xmm_xmm_xmm[63:0], vnfmsub132_double(%ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:96]), %ymm3_vfmsubadd231pd_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vfmsubadd231pd %xmm3, %xmm1, %xmm2

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
vminpd %xmm1, %xmm1, %xmm8
vfnmsub132sd %xmm11, %xmm8, %xmm1
vfmsub231pd %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm2: %ymm2_vfmsubadd132pd_xmm_xmm_xmm

State for specgen instruction: vfmsubadd231pd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm2_vfmsubadd231pd_xmm_xmm_xmm[127:64], %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:64], %ymm3_vfmsubadd231pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm2_vfmsubadd231pd_xmm_xmm_xmm[63:0], vnfmsub132_double(%ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], %ymm1_vfmsubadd231pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm1_vfmsubadd231pd_xmm_xmm_xmm[127:96]), %ymm3_vfmsubadd231pd_xmm_xmm_xmm[63:0]))

Final state
%ymm2: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsubadd132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[63:0], vnfmsub132_double(%ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:96]), %ymm3_vfmsubadd132pd_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vfmsubadd132pd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfmsubadd132pd_xmm_xmm_xmm

%xmm0: %ymm0_vfmsubadd132pd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vfmsubadd132pd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vfmsubadd132pd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfmsubadd132pd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsubadd132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[63:0], vnfmsub132_double(%ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:96]), %ymm3_vfmsubadd132pd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsubadd132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[63:0], vnfmsub132_double(%ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:96]), %ymm3_vfmsubadd132pd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vfmsubadd132pd %xmm3, %xmm2, %xmm1

.target:
vfmsubadd231pd %xmm3, %xmm1, %xmm2
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1

State for specgen instruction: vfmsubadd132pd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsubadd132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[63:0], vnfmsub132_double(%ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:96]), %ymm3_vfmsubadd132pd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:64], %ymm3_vfmsubadd132pd_xmm_xmm_xmm[127:64]) ∘ vfmsub132_double(%ymm1_vfmsubadd132pd_xmm_xmm_xmm[63:0], vnfmsub132_double(%ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], %ymm2_vfmsubadd132pd_xmm_xmm_xmm[63:0], 0x0₃₂ ∘ %ymm2_vfmsubadd132pd_xmm_xmm_xmm[127:96]), %ymm3_vfmsubadd132pd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (vfmsub132_double(%ymm1[127:64], %ymm2[127:64], %ymm3[127:64]) ∘ vfmsub132_double(%ymm1[63:0], vnfmsub132_double(%ymm2[63:0], %ymm2[63:0], 0x0₃₂ ∘ %ymm2[127:96]), %ymm3[63:0]))

=====================================
Circuits:

%ymm1  : 0x0₁₂₈ ∘ (vfmsub132_double(%ymm1[127:64], %ymm2[127:64], %ymm3[127:64]) ∘ vfmsub132_double(%ymm1[63:0], vnfmsub132_double(%ymm2[63:0], %ymm2[63:0], 0x0₃₂ ∘ %ymm2[127:96]), %ymm3[63:0]))

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv

*/