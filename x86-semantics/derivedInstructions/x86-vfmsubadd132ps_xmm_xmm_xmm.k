// Autogenerated using stratification.
requires "x86-configuration.k"

module VFMSUBADD132PS-XMM-XMM-XMM
  imports X86-CONFIGURATION

  rule <k>
    execinstr (vfmsubadd132ps R1:Xmm, R2:Xmm, R3:Xmm,  .Typedoperands) => .
  ...</k>
    <regstate>
RSMap:Map => updateMap(RSMap,
convToRegKeys(R3) |-> (concatenateMInt(mi(128, 0), concatenateMInt(concatenateMInt(Float2MInt( (  ( MInt2Float(extractMInt(getParentValue(R3, RSMap), 128, 160), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 128, 160), 24, 8) )  +Float  ( negateFloat( ( MInt2Float(concatenateMInt(mi(24, 0), extractMInt(getParentValue(R2, RSMap), 224, 232)), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 128, 160), 24, 8) ) )  -Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 128, 160), 24, 8) )  ) , 32), Float2MInt( (  ( MInt2Float(extractMInt(getParentValue(R3, RSMap), 160, 192), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 160, 192), 24, 8) )  -Float  ( negateFloat( ( MInt2Float(concatenateMInt(mi(24, 0), extractMInt(getParentValue(R2, RSMap), 232, 240)), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 160, 192), 24, 8) ) )  -Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 160, 192), 24, 8) )  ) , 32)), concatenateMInt(Float2MInt( (  ( MInt2Float(extractMInt(getParentValue(R3, RSMap), 192, 224), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 192, 224), 24, 8) )  +Float  ( negateFloat( ( MInt2Float(concatenateMInt(mi(24, 0), extractMInt(getParentValue(R2, RSMap), 240, 248)), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 192, 224), 24, 8) ) )  -Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 192, 224), 24, 8) )  ) , 32), Float2MInt( (  ( MInt2Float(extractMInt(getParentValue(R3, RSMap), 224, 256), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 224, 256), 24, 8) )  -Float  ( negateFloat( ( MInt2Float(concatenateMInt(mi(24, 0), extractMInt(getParentValue(R2, RSMap), 248, 256)), 24, 8)  *Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 224, 256), 24, 8) ) )  -Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 224, 256), 24, 8) )  ) , 32)))) )


)

    </regstate>
endmodule

module VFMSUBADD132PS-XMM-XMM-XMM-SEMANTICS
  imports VFMSUBADD132PS-XMM-XMM-XMM
endmodule
/*
TargetInstr:
vfmsubadd132ps %xmm3, %xmm2, %xmm1
RWSet:
maybe read:{ %xmm1 %xmm2 %xmm3 }
must read:{ %xmm1 %xmm2 %xmm3 }
maybe write:{ %ymm1 }
must write:{ %ymm1 }
maybe undef:{ }
must undef:{ }
required flags:{ fma }

Circuit:
circuit:vfmsubadd231ps %xmm3, %xmm1, %xmm2  #  1     0     5      OPC=vfmsubadd231ps_xmm_xmm_xmm
circuit:callq .move_128_064_xmm2_r8_r9      #  2     0x5   5      OPC=callq_label
circuit:vzeroall                            #  3     0xa   3      OPC=vzeroall
circuit:callq .move_064_128_r8_r9_xmm1      #  4     0xd   5      OPC=callq_label
BVF:
WARNING: No live out values provided, assuming { }
WARNING: No def in values provided; assuming { %mxcsr::rc[0] }
Target

vfmsubadd132ps %xmm3, %xmm2, %xmm1

  maybe read:      { %xmm1 %xmm2 %xmm3 }
  must read:       { %xmm1 %xmm2 %xmm3 }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { fma }

-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm2, %xmm1

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpmovzxbq_ymm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_vpmovzxbq_ymm_xmm
%rdx/%rdx: %rdx_vpmovzxbq_ymm_xmm

%xmm0: %ymm0_vpmovzxbq_ymm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_byte_11_of_ymm1_to_r9b

Final state:
%rax/%rax: %rax_vpmovzxbq_ymm_xmm
%rdx/%rdx: %rdx_vpmovzxbq_ymm_xmm

%xmm0: %ymm0_vpmovzxbq_ymm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_r9b_to_byte_24_of_ymm1

Final state:
%rax/%rax: %rax_vpmovzxbq_ymm_xmm
%rdx/%rdx: %rdx_vpmovzxbq_ymm_xmm

%xmm0: %ymm0_vpmovzxbq_ymm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_byte_6_of_ymm1_to_r9b

Final state:
%rax/%rax: %rax_vpmovzxbq_ymm_xmm
%rdx/%rdx: %rdx_vpmovzxbq_ymm_xmm

%xmm0: %ymm0_vpmovzxbq_ymm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_r9b_to_byte_16_of_ymm1

Final state:
%rax/%rax: %rax_vpmovzxbq_ymm_xmm
%rdx/%rdx: %rdx_vpmovzxbq_ymm_xmm

%xmm0: %ymm0_vpmovzxbq_ymm_xmm[127:0]
%xmm1: (((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[255:136] ∘ ((%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[63:8] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[55:48])[7:0] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vpmovzxbq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxbq_xmm_xmm

%xmm0: %ymm0_vpmovzxbq_xmm_xmm[127:0]
%xmm1: %ymm1_vpmovzxbq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_r8b_to_byte_0_of_ymm1

Final state:
%rax/%rax: %rax_vpmovzxbq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxbq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:8] ∘ %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][7:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for xorq %rdx, %rdx

Final state:
%rdx/%rdx: %rdx_notb_r8 ⊕ %rdx_notb_r8

%cf: false
%pf: !((%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][0:0] = 0x1₁ ⊕ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][1:1] = 0x1₁ ⊕ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][2:2] = 0x1₁ ⊕ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][3:3] = 0x1₁ ⊕ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][4:4] = 0x1₁ ⊕ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][5:5] = 0x1₁ ⊕ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][6:6] = 0x1₁ ⊕ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][7:7] = 0x1₁)
%zf: (%rdx_notb_r8 ⊕ %rdx_notb_r8) = 0x0₆₄
%sf: (%rdx_notb_r8 ⊕ %rdx_notb_r8)[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for adcb %bl, %dl

Final state:
%rdx/%dl: (%rdx_notb_r8 ⊕ %rdx_notb_r8)[63:8] ∘ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0]

%cf: ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[8:8] = 0x1₁
%pf: !(((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][0:0] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][1:1] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][2:2] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][3:3] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][4:4] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][5:5] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][6:6] = 0x1₁ ⊕ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:0][7:7] = 0x1₁)
%af: (0x0₁ ∘ %rbx_notb_r8[7:0][3:0] + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][3:0])[4:4] = 0x1₁
%zf: ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0] = 0x0₈
%sf: ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0][7:7] = 0x1₁
%of: (%rbx_notb_r8[7:0][7:7] = 0x1₁ ↔ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0][7:7] = 0x1₁) ∧ !(%rbx_notb_r8[7:0][7:7] = 0x1₁ ↔ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:7] = 0x1₁)

-------------------------------------
-------------------------------------
Getting base circuit for movslq %ebx, %rax

Final state:
%rax/%rax: sign-extend-64(%rbx_notl_r32[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for movq $0xffffffffffffffff, %rbx

Final state:
%rbx/%rbx: 0xffffffffffffffff₆₄

-------------------------------------
-------------------------------------
Getting base circuit for movswq %bx, %r10

Final state:
%r10/%r10: sign-extend-64(%rbx_xchgl_r32_r32[15:0])

-------------------------------------
-------------------------------------
Getting base circuit for movslq %ebx, %rbx

Final state:
%rbx/%rbx: sign-extend-64(%rbx_xchgl_r32_r32[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_016_ecx_r8w_r9w

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_032_rbx_r10d_r11d

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %rcx

Final state:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_032_r8w_r9w_ebx

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
=====================================
Computing circuit for xchgl %ebx, %ecx

.target:
movswq %bx, %r10
movslq %ebx, %rbx
callq .move_032_016_ecx_r8w_r9w
callq .move_064_032_rbx_r10d_r11d
movq %r10, %rcx
callq .move_016_032_r8w_r9w_ebx
retq 

Initial state:
%rcx/%rcx: %rcx_xorl_r32_r32
%rbx/%rbx: %rbx_xorl_r32_r32

State for specgen instruction: xchgl %ecx, %ebx:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
%rbx/%rbx: 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])

Register        -> %rcx
  translates to => %rbx
Value is               -> 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
  after renaming it is => 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0]

Register        -> %rbx
  translates to => %rcx
Value is               -> 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])
  after renaming it is => 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

Final state
%rcx/%rcx: 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0]

=====================================
-------------------------------------
Getting base circuit for xorq %rcx, %rbx

Final state:
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

%cf: false
%pf: !((0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][0:0] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][1:1] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][2:2] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][3:3] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][4:4] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][5:5] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][6:6] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][7:7] = 0x1₁)
%zf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]) = 0x0₆₄
%sf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .set_szp_for_ebx

Final state:
%rax/%rax: %rax_xorl_r32_r32
%rdx/%rdx: %rdx_xorl_r32_r32

%xmm0: %ymm0_xorl_r32_r32[127:0]
%xmm1: %ymm1_xorl_r32_r32[127:0]

-------------------------------------
=====================================
Computing circuit for xorl %eax, %ebx

.target:
xchgl %ebx, %ecx
xorq %rcx, %rbx
callq .set_szp_for_ebx
retq 

Initial state:
%rbx/%rbx: 0xffffffffffffffff₆₄

%cf: %cf_notl_r32
%pf: %pf_notl_r32
%zf: %zf_notl_r32
%sf: %sf_notl_r32
%of: %of_notl_r32

State for specgen instruction: xorl %ecx, %ebx:
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

%cf: false
%pf: !((0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][0:0] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][1:1] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][2:2] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][3:3] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][4:4] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][5:5] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][6:6] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][7:7] = 0x1₁)
%zf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0] = 0x0₃₂
%sf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][31:31] = 0x1₁
%of: false

Register        -> %rbx
  translates to => %rbx
Value is               -> 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]
  after renaming it is => 0x0₃₂ ∘ %rbx_notl_r32[31:0] ⊕ 0xffffffff₆₄

Final state
%rbx/%rbx: 0x0₃₂ ∘ %rbx_notl_r32[31:0] ⊕ 0xffffffff₆₄

%cf: false
%pf: !((%rbx_notl_r32[0:0] ⊕ 0x1₁) = 0x1₁ ⊕ (%rbx_notl_r32[1:1] ⊕ 0x1₁) = 0x1₁ ⊕ (%rbx_notl_r32[2:2] ⊕ 0x1₁) = 0x1₁ ⊕ (%rbx_notl_r32[3:3] ⊕ 0x1₁) = 0x1₁ ⊕ (%rbx_notl_r32[4:4] ⊕ 0x1₁) = 0x1₁ ⊕ (%rbx_notl_r32[5:5] ⊕ 0x1₁) = 0x1₁ ⊕ (%rbx_notl_r32[6:6] ⊕ 0x1₁) = 0x1₁ ⊕ (%rbx_notl_r32[7:7] ⊕ 0x1₁) = 0x1₁)
%zf: (%rbx_notl_r32[31:0] ⊕ 0xffffffff₃₂) = 0x0₃₂
%sf: (%rbx_notl_r32[31:31] ⊕ 0x1₁) = 0x1₁
%of: false

=====================================
=====================================
Computing circuit for notl %edx

.target:
movslq %ebx, %rax
movq $0xffffffffffffffff, %rbx
xorl %eax, %ebx
retq 

Initial state:
%rdx/%rdx: (%rdx_notb_r8 ⊕ %rdx_notb_r8)[63:8] ∘ ((false ? 0x0₁ ∘ %rbx_notb_r8[7:0] + 0x1₉ : 0x0₁ ∘ %rbx_notb_r8[7:0]) + 0x0₁ ∘ (%rdx_notb_r8 ⊕ %rdx_notb_r8)[7:0])[7:0]

State for specgen instruction: notl %ebx:
%rbx/%rbx: 0x0₃₂ ∘ %rbx_notl_r32[31:0] ⊕ 0xffffffff₆₄

Register        -> %rbx
  translates to => %rdx
Value is               -> 0x0₃₂ ∘ %rbx_notl_r32[31:0] ⊕ 0xffffffff₆₄
  after renaming it is => 0x0₃₂ ∘ (0x0₂₄ ∘ %rbx_notb_r8[7:0]) ⊕ 0xffffffff₆₄

Final state
%rdx/%rdx: 0x0₃₂ ∘ (0x0₂₄ ∘ %rbx_notb_r8[7:0]) ⊕ 0xffffffff₆₄

=====================================
-------------------------------------
Getting base circuit for movq $0x20, %rbx

Final state:
%rbx/%rbx: 0x20₆₄

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_008_cx_r12b_r13b

Final state:
%rax/%rax: %rax_movzwl_r32_r16
%rdx/%rdx: %rdx_movzwl_r32_r16

%xmm0: %ymm0_movzwl_r32_r16[127:0]
%xmm1: %ymm1_movzwl_r32_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_008_016_r12b_r13b_bx

Final state:
%rax/%rax: %rax_movzwl_r32_r16
%rdx/%rdx: %rdx_movzwl_r32_r16

%xmm0: %ymm0_movzwl_r32_r16[127:0]
%xmm1: %ymm1_movzwl_r32_r16[127:0]

-------------------------------------
=====================================
Computing circuit for movzwl %cx, %ebx

.target:
movq $0x20, %rbx
callq .move_016_008_cx_r12b_r13b
callq .move_008_016_r12b_r13b_bx
retq 

Initial state:
%rbx/%rbx: %rbx_movswl_r32_r16

State for specgen instruction: movzwl %cx, %ebx:
%rbx/%rbx: 0x20₆₄[63:16] ∘ ((%r13_movzwl_r32_r16[63:8] ∘ %rcx_movzwl_r32_r16[15:0][15:8])[7:0][7:0] ∘ (%r12_movzwl_r32_r16[63:8] ∘ %rcx_movzwl_r32_r16[15:0][7:0])[7:0][7:0])

Register        -> %rbx
  translates to => %rbx
Value is               -> 0x20₆₄[63:16] ∘ ((%r13_movzwl_r32_r16[63:8] ∘ %rcx_movzwl_r32_r16[15:0][15:8])[7:0][7:0] ∘ (%r12_movzwl_r32_r16[63:8] ∘ %rcx_movzwl_r32_r16[15:0][7:0])[7:0][7:0])
  after renaming it is => 0x0₄₈ ∘ %rcx_movswl_r32_r16[15:0]

Final state
%rbx/%rbx: 0x0₄₈ ∘ %rcx_movswl_r32_r16[15:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_032_rbx_r8d_r9d

Final state:
%rax/%rax: %rax_movswl_r32_r16
%rdx/%rdx: %rdx_movswl_r32_r16

%xmm0: %ymm0_movswl_r32_r16[127:0]
%xmm1: %ymm1_movswl_r32_r16[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for movq $0xffffffffffffffc0, %rbx

Final state:
%rbx/%rbx: 0xffffffffffffffc0₆₄

-------------------------------------
-------------------------------------
Getting base circuit for movswq %cx, %rbx

Final state:
%rbx/%rbx: sign-extend-64(%rcx_movw_r16_r16[15:0])

-------------------------------------
=====================================
Computing circuit for movw %r8w, %bx

.target:
movswq %cx, %rbx
retq 

Initial state:
%rbx/%bx: 0xffffffffffffffc0₆₄

State for specgen instruction: movw %cx, %bx:
%rbx/%bx: sign-extend-64(%rcx_movw_r16_r16[15:0])

Register        -> %bx
  translates to => %bx
Value is               -> sign-extend-64(%rcx_movw_r16_r16[15:0])[15:0]
  after renaming it is => %rcx_movswl_r32_r16[15:0]

Final state
%rbx/%bx: 0xffffffffffffffc0₆₄[63:16] ∘ %rcx_movswl_r32_r16[15:0]

=====================================
-------------------------------------
Getting base circuit for movswq %bx, %r9

Final state:
%r9/%r9: sign-extend-64((0xffffffffffffffc0₆₄[63:16] ∘ %rcx_movswl_r32_r16[15:0])[15:0])

-------------------------------------
-------------------------------------
Getting base circuit for movswq %bx, %r10

Final state:
%r10/%r10: sign-extend-64(%rbx_xchgl_r32_r32[15:0])

-------------------------------------
-------------------------------------
Getting base circuit for movslq %ebx, %rbx

Final state:
%rbx/%rbx: sign-extend-64(%rbx_xchgl_r32_r32[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_016_ecx_r8w_r9w

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_032_rbx_r10d_r11d

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %rcx

Final state:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_032_r8w_r9w_ebx

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
=====================================
Computing circuit for xchgl %ebx, %r9d

.target:
movswq %bx, %r10
movslq %ebx, %rbx
callq .move_032_016_ecx_r8w_r9w
callq .move_064_032_rbx_r10d_r11d
movq %r10, %rcx
callq .move_016_032_r8w_r9w_ebx
retq 

Initial state:
%rbx/%rbx: 0xffffffffffffffc0₆₄[63:16] ∘ %rcx_movswl_r32_r16[15:0]
%r9/%r9: sign-extend-64((0xffffffffffffffc0₆₄[63:16] ∘ %rcx_movswl_r32_r16[15:0])[15:0])

State for specgen instruction: xchgl %ecx, %ebx:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
%rbx/%rbx: 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])

Register        -> %rcx
  translates to => %rbx
Value is               -> 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
  after renaming it is => 0x0₃₂ ∘ sign-extend-64(%rcx_movswl_r32_r16[15:0])[31:0]

Register        -> %rbx
  translates to => %r9
Value is               -> 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])
  after renaming it is => 0x0₃₂ ∘ (0xffff₁₆ ∘ %rcx_movswl_r32_r16[15:0])

Final state
%rbx/%rbx: 0x0₃₂ ∘ sign-extend-64(%rcx_movswl_r32_r16[15:0])[31:0]
%r9/%r9: 0x0₃₂ ∘ (0xffff₁₆ ∘ %rcx_movswl_r32_r16[15:0])

=====================================
=====================================
Computing circuit for movswl %dx, %ebx

.target:
movzwl %cx, %ebx
callq .move_064_032_rbx_r8d_r9d
movq $0xffffffffffffffc0, %rbx
movw %r8w, %bx
movswq %bx, %r9
xchgl %ebx, %r9d
retq 

Initial state:
%rbx/%rbx: %rbx_notb_r8

State for specgen instruction: movswl %cx, %ebx:
%rbx/%rbx: 0x0₃₂ ∘ sign-extend-64(%rcx_movswl_r32_r16[15:0])[31:0]

Register        -> %rbx
  translates to => %rbx
Value is               -> 0x0₃₂ ∘ sign-extend-64(%rcx_movswl_r32_r16[15:0])[31:0]
  after renaming it is => 0x0₃₂ ∘ sign-extend-64(0x0₈ ∘ %rbx_notb_r8[7:0] ⊕ 0xffff₁₆)[31:0]

Final state
%rbx/%rbx: 0x0₃₂ ∘ sign-extend-64(0x0₈ ∘ %rbx_notb_r8[7:0] ⊕ 0xffff₁₆)[31:0]

=====================================
=====================================
Computing circuit for notb %r8b

.target:
xorq %rdx, %rdx
adcb %bl, %dl
notl %edx
movswl %dx, %ebx
retq 

Initial state:
%r8/%r8b: %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0]

State for specgen instruction: notb %bl:
%rbx/%bl: 0x0₃₂ ∘ sign-extend-64(0x0₈ ∘ %rbx_notb_r8[7:0] ⊕ 0xffff₁₆)[31:0]

Register        -> %bl
  translates to => %r8b
Value is               -> (0x0₃₂ ∘ sign-extend-64(0x0₈ ∘ %rbx_notb_r8[7:0] ⊕ 0xffff₁₆)[31:0])[7:0]
  after renaming it is => %ymm2_vpmovzxbq_xmm_xmm[7:0] ⊕ 0xff₈

Final state
%r8/%r8b: %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][63:8] ∘ (%ymm2_vpmovzxbq_xmm_xmm[7:0] ⊕ 0xff₈)

=====================================
-------------------------------------
Getting base circuit for callq .move_016_032_r8w_r9w_edx

Final state:
%rax/%rax: %rax_vpmovzxbq_xmm_xmm
%rdx/%rdx: 0x0₃₂ ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][127:64][15:0][15:0] ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][63:8] ∘ (%ymm2_vpmovzxbq_xmm_xmm[7:0] ⊕ 0xff₈))[15:0][15:0])

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:8] ∘ %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][7:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_008_dx_r8b_r9b

Final state:
%rax/%rax: %rax_vpmovzxbq_xmm_xmm
%rdx/%rdx: 0x0₃₂ ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][127:64][15:0][15:0] ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][63:8] ∘ (%ymm2_vpmovzxbq_xmm_xmm[7:0] ⊕ 0xff₈))[15:0][15:0])

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:8] ∘ %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][7:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_r9b_to_byte_8_of_ymm1

Final state:
%rax/%rax: %rax_vpmovzxbq_xmm_xmm
%rdx/%rdx: 0x0₃₂ ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][127:64][15:0][15:0] ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][63:8] ∘ (%ymm2_vpmovzxbq_xmm_xmm[7:0] ⊕ 0xff₈))[15:0][15:0])

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:8] ∘ %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][7:0])[255:72] ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][127:64][63:8] ∘ (0x0₃₂ ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][127:64][15:0][15:0] ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][63:8] ∘ (%ymm2_vpmovzxbq_xmm_xmm[7:0] ⊕ 0xff₈))[15:0][15:0]))[15:0][15:8])[7:0] ∘ (0x0₂₅₆[255:8] ∘ %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][7:0])[63:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vpmovzxbq %xmm2, %xmm2

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_r8b_to_byte_0_of_ymm1
notb %r8b
callq .move_016_032_r8w_r9w_edx
callq .move_016_008_dx_r8b_r9b
callq .move_r9b_to_byte_8_of_ymm1
retq 

Initial state:
%ymm2: %ymm2_pmovzxbq_xmm_xmm

State for specgen instruction: vpmovzxbq %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:8] ∘ %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][7:0])[255:72] ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][127:64][63:8] ∘ (0x0₃₂ ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][127:64][15:0][15:0] ∘ (%ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][63:8] ∘ (%ymm2_vpmovzxbq_xmm_xmm[7:0] ⊕ 0xff₈))[15:0][15:0]))[15:0][15:8])[7:0] ∘ (0x0₂₅₆[255:8] ∘ %ymm2_vpmovzxbq_xmm_xmm[127:0][63:0][7:0])[63:0]

Final state
%ymm2: 0x0₁₈₄ ∘ %ymm2_pmovzxbq_xmm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_pmovzxbq_xmm_xmm
%rdx/%rdx: %rdx_pmovzxbq_xmm_xmm

%xmm0: %ymm0_pmovzxbq_xmm_xmm[127:0]
%xmm1: %ymm1_pmovzxbq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_movddup_xmm_xmm
%rdx/%rdx: %rdx_movddup_xmm_xmm

%xmm0: %ymm0_movddup_xmm_xmm[127:0]
%xmm1: %ymm1_movddup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for movq %r12, %r13

Final state:
%r13/%r13: %ymm2_movddup_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_movddup_xmm_xmm
%rdx/%rdx: %rdx_movddup_xmm_xmm

%xmm0: %ymm0_movddup_xmm_xmm[127:0]
%xmm1: (%ymm1_movddup_xmm_xmm[255:128] ∘ (%ymm2_movddup_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_movddup_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movddup %xmm8, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
movq %r12, %r13
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm1: %ymm1_pmovzxbq_xmm_xmm[127:0]

State for specgen instruction: movddup %xmm2, %xmm1:
%xmm1: (%ymm1_movddup_xmm_xmm[255:128] ∘ (%ymm2_movddup_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_movddup_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_pmovzxbq_xmm_xmm[255:128] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm8_xmm9

Final state:
%rax/%rax: %rax_vorpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vorpd_ymm_ymm_ymm

%xmm0: %ymm0_vorpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vorpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm2, %xmm2

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm2: %ymm2_por_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ %ymm2_por_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm1, %xmm2, %xmm3

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm3: %ymm3_por_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_por_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for por %xmm2, %xmm8

.target:
vmovapd %xmm2, %xmm2
vorpd %xmm1, %xmm2, %xmm3
movdqa %xmm3, %xmm1
retq 

Initial state:
%xmm8: (%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[127:0]

State for specgen instruction: por %xmm2, %xmm1:
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

Final state
%xmm8: ((%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[127:64] | %ymm2_vorpd_ymm_ymm_ymm[127:64]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[63:0] | %ymm2_vorpd_ymm_ymm_ymm[63:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_vorpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vorpd_ymm_ymm_ymm

%xmm0: %ymm0_vorpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vorpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm1, %xmm2, %xmm3

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm3: %ymm3_orps_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ ((%ymm1_orps_xmm_xmm[127:64] | %ymm2_orps_xmm_xmm[127:64]) ∘ (%ymm1_orps_xmm_xmm[63:0] | %ymm2_orps_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm10_xmm11

Final state:
%rax/%rax: %rax_orps_xmm_xmm
%rdx/%rdx: %rdx_orps_xmm_xmm

%xmm0: %ymm0_orps_xmm_xmm[127:0]
%xmm1: %ymm1_orps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm10, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_orps_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_orps_xmm_xmm[255:128] ∘ ((%ymm1_orps_xmm_xmm[127:64] | %ymm2_orps_xmm_xmm[127:64]) ∘ (%ymm1_orps_xmm_xmm[63:0] | %ymm2_orps_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for orps %xmm2, %xmm1

.target:
vorpd %xmm1, %xmm2, %xmm3
callq .move_256_128_ymm3_xmm10_xmm11
movdqa %xmm10, %xmm1
retq 

Initial state:
%xmm1: %ymm1_orpd_xmm_xmm[127:0]

State for specgen instruction: orps %xmm2, %xmm1:
%xmm1: (%ymm1_orps_xmm_xmm[255:128] ∘ ((%ymm1_orps_xmm_xmm[127:64] | %ymm2_orps_xmm_xmm[127:64]) ∘ (%ymm1_orps_xmm_xmm[63:0] | %ymm2_orps_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: (%ymm1_orpd_xmm_xmm[255:128] ∘ ((%ymm1_orpd_xmm_xmm[127:64] | %ymm2_orpd_xmm_xmm[127:64]) ∘ (%ymm1_orpd_xmm_xmm[63:0] | %ymm2_orpd_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for orpd %xmm11, %xmm9

.target:
orps %xmm2, %xmm1
retq 

Initial state:
%xmm9: (%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[127:0]

State for specgen instruction: orpd %xmm2, %xmm1:
%xmm1: (%ymm1_orpd_xmm_xmm[255:128] ∘ ((%ymm1_orpd_xmm_xmm[127:64] | %ymm2_orpd_xmm_xmm[127:64]) ∘ (%ymm1_orpd_xmm_xmm[63:0] | %ymm2_orpd_xmm_xmm[63:0])))[127:0]

Final state
%xmm9: ((%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[255:192] | %ymm2_vorpd_ymm_ymm_ymm[255:192]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[191:128] | %ymm2_vorpd_ymm_ymm_ymm[191:128])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vorpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vorpd_ymm_ymm_ymm

%xmm0: %ymm0_vorpd_ymm_ymm_ymm[127:0]
%xmm1: (((%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[255:192] | %ymm2_vorpd_ymm_ymm_ymm[255:192]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[191:128] | %ymm2_vorpd_ymm_ymm_ymm[191:128])))[127:0][127:0] ∘ ((%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[127:64] | %ymm2_vorpd_ymm_ymm_ymm[127:64]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[63:0] | %ymm2_vorpd_ymm_ymm_ymm[63:0])))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %ymm2, %ymm2, %ymm15

.target:
callq .move_256_128_ymm3_xmm8_xmm9
por %xmm2, %xmm8
callq .move_256_128_ymm2_xmm10_xmm11
orpd %xmm11, %xmm9
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm15: %ymm15_pmovzxbq_xmm_xmm

State for specgen instruction: vorpd %ymm3, %ymm2, %ymm1:
%ymm1: ((%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[255:192] | %ymm2_vorpd_ymm_ymm_ymm[255:192]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[191:128] | %ymm2_vorpd_ymm_ymm_ymm[191:128])))[127:0][127:0] ∘ ((%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[127:64] | %ymm2_vorpd_ymm_ymm_ymm[127:64]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[63:0] | %ymm2_vorpd_ymm_ymm_ymm[63:0])))[127:0][127:0]

Final state
%ymm15: 0x0₁₂₈ ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm15, %xmm1

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm1: (%ymm1_pmovzxbq_xmm_xmm[255:128] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0])))[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm1: ((%ymm1_pmovzxbq_xmm_xmm[255:128] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0])))[255:128] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0])))[127:0]

=====================================
=====================================
Computing circuit for pmovzxbq %xmm10, %xmm1

.target:
vpmovzxbq %xmm2, %xmm2
callq .move_128_64_xmm2_xmm8_xmm9
movddup %xmm8, %xmm1
vorpd %ymm2, %ymm2, %ymm15
unpckhpd %xmm15, %xmm1
retq 

Initial state:
%xmm1: (((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[255:136] ∘ ((%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[63:8] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[55:48])[7:0] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[127:0])[127:0]

State for specgen instruction: pmovzxbq %xmm2, %xmm1:
%xmm1: ((%ymm1_pmovzxbq_xmm_xmm[255:128] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0])))[255:128] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_pmovzxbq_xmm_xmm[7:0])))[127:0]

Final state
%xmm1: ((((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[255:136] ∘ ((%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[63:8] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[55:48])[7:0] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[127:0])[255:128] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbq_ymm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbq_ymm_xmm[7:0])))[127:0]

=====================================
=====================================
Computing circuit for vpmovzxbq %xmm2, %ymm2

.target:
vbroadcastss %xmm2, %xmm1
callq .move_128_64_xmm2_xmm10_xmm11
callq .move_byte_11_of_ymm1_to_r9b
callq .move_r9b_to_byte_24_of_ymm1
callq .move_byte_6_of_ymm1_to_r9b
callq .move_r9b_to_byte_16_of_ymm1
pmovzxbq %xmm10, %xmm1
retq 

Initial state:
%ymm2: %ymm2_vpmovzxbd_xmm_xmm

State for specgen instruction: vpmovzxbq %xmm2, %ymm1:
%ymm1: (((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[255:136] ∘ ((%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[63:8] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[55:48])[7:0] ∘ ((0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[255:200] ∘ (%r9_vpmovzxbq_ymm_xmm[63:8] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[95:88])[7:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0] ∘ %ymm2_vpmovzxbq_ymm_xmm[31:0]))[191:0])[127:0])[255:128] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbq_ymm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbq_ymm_xmm[7:0]))

Final state
%ymm2: 0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[31:24] ∘ 0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[23:16] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[7:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_vhaddps_ymm_ymm_ymm
%rdx/%rdx: %rdx_vhaddps_ymm_ymm_ymm

%xmm0: %ymm0_vhaddps_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vhaddps_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm1, %xmm5

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm5: %ymm5_haddps_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm5: 0x0₁₂₈ ∘ %ymm1_haddps_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm3, %xmm14

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm14: %ymm14_vhaddps_xmm_xmm_xmm

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm14: 0x0₁₂₈ ∘ (%ymm3_vhaddps_xmm_xmm_xmm[63:0] ∘ %ymm3_vhaddps_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_vhaddps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vhaddps_xmm_xmm_xmm

%xmm0: %ymm0_vhaddps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vhaddps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm3, %xmm2

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm2: %ymm2_vunpckhps_xmm_xmm_xmm[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm2: (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vunpckhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpckhps_xmm_xmm_xmm

%xmm0: %ymm0_vunpckhps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vunpckhps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovddup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: (%ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm1, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0])

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

=====================================
=====================================
Computing circuit for vmovddup %xmm9, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
callq .move_064_128_r12_r13_xmm1
vpbroadcastq %xmm1, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vunpckhps_xmm_xmm_xmm

State for specgen instruction: vmovddup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovq_xmm_xmm
%rdx/%rdx: %rdx_vmovq_xmm_xmm

%xmm0: %ymm0_vmovq_xmm_xmm[127:0]
%xmm1: %ymm1_vmovq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movswq %bx, %r10

Final state:
%r10/%r10: sign-extend-64(%rbx_xchgl_r32_r32[15:0])

-------------------------------------
-------------------------------------
Getting base circuit for movslq %ebx, %rbx

Final state:
%rbx/%rbx: sign-extend-64(%rbx_xchgl_r32_r32[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_016_ecx_r8w_r9w

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_032_rbx_r10d_r11d

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %rcx

Final state:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_032_r8w_r9w_ebx

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
=====================================
Computing circuit for xchgl %ebx, %ecx

.target:
movswq %bx, %r10
movslq %ebx, %rbx
callq .move_032_016_ecx_r8w_r9w
callq .move_064_032_rbx_r10d_r11d
movq %r10, %rcx
callq .move_016_032_r8w_r9w_ebx
retq 

Initial state:
%rcx/%rcx: %rcx_xorl_r32_r32
%rbx/%rbx: %rbx_xorl_r32_r32

State for specgen instruction: xchgl %ecx, %ebx:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
%rbx/%rbx: 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])

Register        -> %rcx
  translates to => %rbx
Value is               -> 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
  after renaming it is => 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0]

Register        -> %rbx
  translates to => %rcx
Value is               -> 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])
  after renaming it is => 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

Final state
%rcx/%rcx: 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0]

=====================================
-------------------------------------
Getting base circuit for xorq %rcx, %rbx

Final state:
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

%cf: false
%pf: !((0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][0:0] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][1:1] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][2:2] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][3:3] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][4:4] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][5:5] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][6:6] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][7:7] = 0x1₁)
%zf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]) = 0x0₆₄
%sf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .set_szp_for_ebx

Final state:
%rax/%rax: %rax_xorl_r32_r32
%rdx/%rdx: %rdx_xorl_r32_r32

%xmm0: %ymm0_xorl_r32_r32[127:0]
%xmm1: %ymm1_xorl_r32_r32[127:0]

-------------------------------------
=====================================
Computing circuit for xorl %r13d, %r13d

.target:
xchgl %ebx, %ecx
xorq %rcx, %rbx
callq .set_szp_for_ebx
retq 

Initial state:
%r13/%r13: %ymm2_vmovq_xmm_xmm[127:0][127:64]

%cf: %cf_vmovq_xmm_xmm
%pf: %pf_vmovq_xmm_xmm
%zf: %zf_vmovq_xmm_xmm
%sf: %sf_vmovq_xmm_xmm
%of: %of_vmovq_xmm_xmm

State for specgen instruction: xorl %ecx, %ebx:
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

%cf: false
%pf: !((0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][0:0] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][1:1] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][2:2] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][3:3] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][4:4] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][5:5] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][6:6] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][7:7] = 0x1₁)
%zf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0] = 0x0₃₂
%sf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][31:31] = 0x1₁
%of: false

Register        -> %rbx
  translates to => %r13
Value is               -> 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]
  after renaming it is => 0x0₆₄

Final state
%r13/%r13: 0x0₆₄

%cf: false
%pf: true
%zf: true
%sf: false
%of: false

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovq_xmm_xmm
%rdx/%rdx: %rdx_vmovq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (0x0₆₄[63:0] ∘ %ymm2_vmovq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %xmm10

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
xorl %r13d, %r13d
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm10: %ymm10_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][95:64])

State for specgen instruction: vmovq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (0x0₆₄[63:0] ∘ %ymm2_vmovq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm10: 0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vunpckhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpckhps_xmm_xmm_xmm

%xmm0: %ymm0_vunpckhps_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vunpckhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpckhps_xmm_xmm_xmm

%xmm0: %ymm0_vunpckhps_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[255:128] ∘ ((%ymm11_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:96]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[127:0][31:0] ∘ ((%ymm9_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:32]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:64]))[127:0][31:0] ∘ ((%ymm8_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][31:0]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vunpckhps %xmm3, %xmm14, %xmm12

.target:
unpckhpd %xmm3, %xmm2
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovddup %xmm9, %xmm1
vmovq %xmm1, %xmm10
callq .move_128_64_xmm2_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm12: %ymm12_vhaddps_xmm_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_vhaddps_xmm_xmm_xmm[127:0][63:0])

State for specgen instruction: vunpckhps %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[255:128] ∘ ((%ymm11_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:96]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[127:0][31:0] ∘ ((%ymm9_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:32]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:64]))[127:0][31:0] ∘ ((%ymm8_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][31:0]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm12: 0x0₁₂₈ ∘ (%ymm3_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm3_vhaddps_xmm_xmm_xmm[63:32] ∘ %ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpbroadcastq_ymm_xmm

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm1, %xmm8

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm8: %ymm8_vpbroadcastq_ymm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for vminpd %ymm2, %ymm2, %ymm1

Final state:
%ymm1: (mincmp_double(%ymm2_vmovdqa_ymm_ymm[255:192], %ymm2_vmovdqa_ymm_ymm[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[255:192] : %ymm2_vmovdqa_ymm_ymm[255:192]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[191:128], %ymm2_vmovdqa_ymm_ymm[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[191:128] : %ymm2_vmovdqa_ymm_ymm[191:128]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[127:64], %ymm2_vmovdqa_ymm_ymm[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[127:64] : %ymm2_vmovdqa_ymm_ymm[127:64]) ∘ (mincmp_double(%ymm2_vmovdqa_ymm_ymm[63:0], %ymm2_vmovdqa_ymm_ymm[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[63:0] : %ymm2_vmovdqa_ymm_ymm[63:0])))

-------------------------------------
=====================================
Computing circuit for vmovdqa %ymm1, %ymm9

.target:
vminpd %ymm2, %ymm2, %ymm1
retq 

Initial state:
%ymm9: %ymm9_vpbroadcastq_ymm_xmm

State for specgen instruction: vmovdqa %ymm2, %ymm1:
%ymm1: (mincmp_double(%ymm2_vmovdqa_ymm_ymm[255:192], %ymm2_vmovdqa_ymm_ymm[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[255:192] : %ymm2_vmovdqa_ymm_ymm[255:192]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[191:128], %ymm2_vmovdqa_ymm_ymm[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[191:128] : %ymm2_vmovdqa_ymm_ymm[191:128]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[127:64], %ymm2_vmovdqa_ymm_ymm[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[127:64] : %ymm2_vmovdqa_ymm_ymm[127:64]) ∘ (mincmp_double(%ymm2_vmovdqa_ymm_ymm[63:0], %ymm2_vmovdqa_ymm_ymm[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[63:0] : %ymm2_vmovdqa_ymm_ymm[63:0])))

Final state
%ymm9: 0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vpbroadcastq_ymm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_ymm_xmm

%xmm0: %ymm0_vpbroadcastq_ymm_xmm[127:0]
%xmm1: ((0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])))[127:0][127:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm3, %ymm6

.target:
vpbroadcastq %xmm2, %xmm1
vmovupd %xmm1, %xmm8
vmovdqa %ymm1, %ymm9
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm6: %ymm6_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vpbroadcastq %xmm2, %ymm1:
%ymm1: (0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])))[127:0][127:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0]))[127:0][127:0]

Final state
%ymm6: %ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm3, %r8

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r8/%r8: %r8_vunpcklpd_xmm_xmm_xmm

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r8
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

Final state
%r8/%r8: %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vunpcklpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpcklpd_xmm_xmm_xmm

%xmm0: %ymm0_vunpcklpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vunpcklpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vunpcklpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpcklpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vunpcklpd_xmm_xmm_xmm[63:0][63:0] ∘ %ymm2_vunpcklpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vunpcklpd %xmm3, %xmm6, %xmm9

.target:
movq %xmm3, %r8
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vunpcklpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vunpcklpd_xmm_xmm_xmm[63:0][63:0] ∘ %ymm2_vunpcklpd_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm3, %xmm2, %xmm1

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vorps_xmm_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vorps %xmm3, %xmm2, %xmm14

.target:
vorpd %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm14: %ymm14_vpor_xmm_xmm_xmm

State for specgen instruction: vorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

Final state
%ymm14: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm14, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpor_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vpor %xmm9, %xmm9, %xmm7

.target:
vorps %xmm3, %xmm2, %xmm14
vmovapd %xmm14, %xmm1
retq 

Initial state:
%ymm7: %ymm7_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vpor %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

Final state
%ymm7: 0x0₁₂₈ ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastsd_ymm_xmm
%rdx/%rdx: %rdx_vbroadcastsd_ymm_xmm

%xmm0: %ymm0_vbroadcastsd_ymm_xmm[127:0]
%xmm1: %ymm1_vbroadcastsd_ymm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm10, %xmm10, %xmm11

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm11: %ymm11_vbroadcastsd_ymm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_vbroadcastsd_ymm_xmm[127:0][127:64])

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for vmaxps %ymm2, %ymm2, %ymm10

Final state:
%ymm10: (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for vmaxpd %ymm10, %ymm2, %ymm1

Final state:
%ymm1: (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[255:192], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:192] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[191:128], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:128] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[127:64], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:64] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64]) ∘ (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[63:0], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:0] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])))

-------------------------------------
=====================================
Computing circuit for vmovdqu %ymm11, %ymm10

.target:
vmaxps %ymm2, %ymm2, %ymm10
vmaxpd %ymm10, %ymm2, %ymm1
retq 

Initial state:
%ymm10: %ymm10_vbroadcastsd_ymm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_vbroadcastsd_ymm_xmm[127:0][63:0])

State for specgen instruction: vmovdqu %ymm2, %ymm1:
%ymm1: (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[255:192], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:192] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[191:128], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:128] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[127:64], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:64] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64]) ∘ (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[63:0], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:0] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])))

Final state
%ymm10: 0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vbroadcastsd_ymm_xmm
%rdx/%rdx: %rdx_vbroadcastsd_ymm_xmm

%xmm0: %ymm0_vbroadcastsd_ymm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0]))[127:0][127:0] ∘ (0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0])))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastsd %xmm1, %ymm9

.target:
callq .move_128_64_xmm2_xmm10_xmm11
vpunpcklqdq %xmm10, %xmm10, %xmm11
vmovdqu %ymm11, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm9: %ymm9_unpcklps_xmm_xmm

State for specgen instruction: vbroadcastsd %xmm2, %ymm1:
%ymm1: (0x0₁₂₈ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0]))[127:0][127:0] ∘ (0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0])))[127:0][127:0]

Final state
%ymm9: %ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0] ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm9, %xmm10

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm10: %ymm10_unpcklps_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm10: 0x0₁₂₈ ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovddup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: (%ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm1, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0])

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

=====================================
=====================================
Computing circuit for vmovddup %xmm2, %xmm15

.target:
callq .move_128_064_xmm2_r12_r13
callq .move_064_128_r12_r13_xmm1
vpbroadcastq %xmm1, %xmm1
retq 

Initial state:
%ymm15: %ymm15_unpcklps_xmm_xmm

State for specgen instruction: vmovddup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

Final state
%ymm15: 0x0₁₂₈ ∘ (%ymm2_unpcklps_xmm_xmm[63:0] ∘ %ymm2_unpcklps_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhps_xmm_xmm
%rdx/%rdx: %rdx_unpckhps_xmm_xmm

%xmm0: %ymm0_unpckhps_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_unpckhps_xmm_xmm
%rdx/%rdx: %rdx_unpckhps_xmm_xmm

%xmm0: %ymm0_unpckhps_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmaxss_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_vmaxss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmaxss_xmm_xmm_xmm

%xmm0: %ymm0_vmaxss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm3

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm3: %ymm3_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm11: %ymm11_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vmaxps %ymm3, %ymm11, %ymm1

Final state:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vmaxps %xmm3, %xmm4, %xmm13

.target:
vmovdqa %xmm3, %xmm3
vmovdqa %xmm2, %xmm11
vmaxps %ymm3, %ymm11, %ymm1
retq 

Initial state:
%ymm13: %ymm13_vmaxss_xmm_xmm_xmm

State for specgen instruction: vmaxps %xmm3, %xmm2, %xmm1:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm13: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[127:96]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[127:96]) ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[95:64]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[95:64]) ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[63:32]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[63:32]) ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0])))))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: %ymm1_movss_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: %ymm0_vpmovzxdq_xmm_xmm[127:0]
%xmm1: %ymm1_vpmovzxdq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_byte_5_of_ymm1_to_r9b

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm2

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_64_128_xmm8_xmm9_xmm1

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpmovzxdq %xmm2, %xmm8

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_byte_5_of_ymm1_to_r9b
callq .move_064_128_r8_r9_xmm2
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
callq .move_64_128_xmm8_xmm9_xmm1
retq 

Initial state:
%ymm8: %ymm8_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][31:0])

State for specgen instruction: vpmovzxdq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movss %xmm13, %xmm1

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
vpmovzxdq %xmm2, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[127:0]

State for specgen instruction: movss %xmm2, %xmm1:
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vmaxss_xmm_xmm_xmm[127:32] ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for vmaxss %xmm13, %xmm13, %xmm0

.target:
vmovupd %xmm2, %xmm1
callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7
vmaxps %xmm3, %xmm4, %xmm13
movss %xmm13, %xmm1
retq 

Initial state:
%ymm0: %ymm0_unpckhps_xmm_xmm

State for specgen instruction: vmaxss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vmaxss_xmm_xmm_xmm[127:32] ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0]))

Final state
%ymm0: 0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm2_unpckhps_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: %ymm0_vrcpps_xmm_xmm[127:0]
%xmm1: %ymm1_vrcpps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vrcpps %ymm1, %ymm10

Final state:
%ymm10: approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vrcpps %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
vrcpps %ymm1, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmovss_xmm_xmm_xmm

State for specgen instruction: vrcpps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm8

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm8: %ymm8_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovss %xmm11, %xmm13, %xmm9

.target:
vrcpps %xmm3, %xmm1
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovdqu %xmm3, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm9: %ymm9_unpckhps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_unpckhps_xmm_xmm[127:0][63:32])

State for specgen instruction: vmovss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm1_unpckhps_xmm_xmm[127:96])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovsldup_xmm_xmm
%rdx/%rdx: %rdx_vmovsldup_xmm_xmm

%xmm0: %ymm0_vmovsldup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovsldup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm2, %xmm9

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm13, %xmm5

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm5: %ymm5_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm5: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm5, %xmm9, %xmm1

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovsldup_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

=====================================
=====================================
Computing circuit for vmovsldup %xmm2, %xmm12

.target:
callq .move_128_64_xmm2_xmm12_xmm13
vbroadcastss %xmm2, %xmm9
vbroadcastss %xmm13, %xmm5
vpunpcklqdq %xmm5, %xmm9, %xmm1
retq 

Initial state:
%ymm12: %ymm12_movsldup_xmm_xmm

State for specgen instruction: vmovsldup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

Final state
%ymm12: 0x0₁₂₈ ∘ (%ymm2_movsldup_xmm_xmm[95:64] ∘ %ymm2_movsldup_xmm_xmm[95:64] ∘ (%ymm2_movsldup_xmm_xmm[31:0] ∘ %ymm2_movsldup_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm12, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_movsldup_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_movsldup_xmm_xmm[255:128] ∘ (%ymm2_movsldup_xmm_xmm[95:64] ∘ %ymm2_movsldup_xmm_xmm[95:64] ∘ (%ymm2_movsldup_xmm_xmm[31:0] ∘ %ymm2_movsldup_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for movsldup %xmm0, %xmm13

.target:
vmovsldup %xmm2, %xmm12
movdqa %xmm12, %xmm1
retq 

Initial state:
%xmm13: (%ymm13_unpckhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:0][127:64]))[127:0]

State for specgen instruction: movsldup %xmm2, %xmm1:
%xmm1: (%ymm1_movsldup_xmm_xmm[255:128] ∘ (%ymm2_movsldup_xmm_xmm[95:64] ∘ %ymm2_movsldup_xmm_xmm[95:64] ∘ (%ymm2_movsldup_xmm_xmm[31:0] ∘ %ymm2_movsldup_xmm_xmm[31:0])))[127:0]

Final state
%xmm13: ((%ymm13_unpckhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:0][127:64]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm2_unpckhps_xmm_xmm[95:64])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: %ymm0_vrcpps_xmm_xmm[127:0]
%xmm1: %ymm1_vrcpps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vrcpps %ymm1, %ymm10

Final state:
%ymm10: approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vrcpps %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
vrcpps %ymm1, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmovss_xmm_xmm_xmm

State for specgen instruction: vrcpps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm8

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm8: %ymm8_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovss %xmm10, %xmm13, %xmm8

.target:
vrcpps %xmm3, %xmm1
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovdqu %xmm3, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm8: %ymm8_unpckhps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_unpckhps_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm1_unpckhps_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_64_128_xmm8_xmm9_xmm1

Final state:
%rax/%rax: %rax_unpckhps_xmm_xmm
%rdx/%rdx: %rdx_unpckhps_xmm_xmm

%xmm0: (0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm2_unpckhps_xmm_xmm[95:64]))[127:0]
%xmm1: (%ymm1_unpckhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm1_unpckhps_xmm_xmm[127:96]))[127:0][63:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm1_unpckhps_xmm_xmm[95:64]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhps %xmm15, %xmm10

.target:
callq .move_128_64_xmm2_xmm12_xmm13
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
vmaxss %xmm13, %xmm13, %xmm0
vmovss %xmm11, %xmm13, %xmm9
movsldup %xmm0, %xmm13
vmovss %xmm10, %xmm13, %xmm8
callq .move_64_128_xmm8_xmm9_xmm1
retq 

Initial state:
%xmm10: (0x0₁₂₈ ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0]))[127:0]

State for specgen instruction: unpckhps %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm1_unpckhps_xmm_xmm[127:96]))[127:0][63:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm1_unpckhps_xmm_xmm[95:64]))[127:0][63:0]))[127:0]

Final state
%xmm10: ((0x0₁₂₈ ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0]))[255:128] ∘ (%ymm2_unpcklps_xmm_xmm[63:32] ∘ %ymm1_unpcklps_xmm_xmm[63:32] ∘ (%ymm2_unpcklps_xmm_xmm[31:0] ∘ %ymm1_unpcklps_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_movapd_xmm_xmm
%rdx/%rdx: %rdx_movapd_xmm_xmm

%xmm0: %ymm0_movapd_xmm_xmm[127:0]
%xmm1: %ymm1_movapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_movapd_xmm_xmm
%rdx/%rdx: %rdx_movapd_xmm_xmm

%xmm0: %ymm0_movapd_xmm_xmm[127:0]
%xmm1: (%ymm1_movapd_xmm_xmm[255:128] ∘ (%ymm2_movapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movapd %xmm10, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm1: %ymm1_unpcklps_xmm_xmm[127:0]

State for specgen instruction: movapd %xmm2, %xmm1:
%xmm1: (%ymm1_movapd_xmm_xmm[255:128] ∘ (%ymm2_movapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_unpcklps_xmm_xmm[255:128] ∘ (%ymm2_unpcklps_xmm_xmm[63:32] ∘ %ymm1_unpcklps_xmm_xmm[63:32] ∘ (%ymm2_unpcklps_xmm_xmm[31:0] ∘ %ymm1_unpcklps_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for unpcklps %xmm7, %xmm2

.target:
vbroadcastsd %xmm1, %ymm9
vmovdqa %xmm9, %xmm10
vmovddup %xmm2, %xmm15
unpckhps %xmm15, %xmm10
movapd %xmm10, %xmm1
retq 

Initial state:
%xmm2: %ymm2_vpunpckldq_xmm_xmm_xmm[127:0]

State for specgen instruction: unpcklps %xmm2, %xmm1:
%xmm1: (%ymm1_unpcklps_xmm_xmm[255:128] ∘ (%ymm2_unpcklps_xmm_xmm[63:32] ∘ %ymm1_unpcklps_xmm_xmm[63:32] ∘ (%ymm2_unpcklps_xmm_xmm[31:0] ∘ %ymm1_unpcklps_xmm_xmm[31:0])))[127:0]

Final state
%xmm2: (%ymm2_vpunpckldq_xmm_xmm_xmm[255:128] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm1, %xmm3

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm3: %ymm3_mulpd_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: %ymm0_vmovaps_xmm_xmm[127:0]
%xmm1: %ymm1_vmovaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovaps %xmm2, %xmm8

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm8: %ymm8_mulpd_xmm_xmm

State for specgen instruction: vmovaps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vmulpd %ymm8, %ymm3, %ymm6

Final state:
%ymm6: mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[255:192], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[255:192]) ∘ (mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[191:128], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[191:128]) ∘ (mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[127:64], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[127:64]) ∘ mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[63:0], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[63:0])))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm6, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_mulpd_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_mulpd_xmm_xmm[255:128] ∘ (mul_double(%ymm1_mulpd_xmm_xmm[127:64], %ymm2_mulpd_xmm_xmm[127:64]) ∘ mul_double(%ymm1_mulpd_xmm_xmm[63:0], %ymm2_mulpd_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for mulpd %xmm3, %xmm2

.target:
vmovapd %xmm1, %xmm3
vmovaps %xmm2, %xmm8
vmulpd %ymm8, %ymm3, %ymm6
movdqa %xmm6, %xmm1
retq 

Initial state:
%xmm2: %ymm2_vmulpd_xmm_xmm_xmm[127:0]

State for specgen instruction: mulpd %xmm2, %xmm1:
%xmm1: (%ymm1_mulpd_xmm_xmm[255:128] ∘ (mul_double(%ymm1_mulpd_xmm_xmm[127:64], %ymm2_mulpd_xmm_xmm[127:64]) ∘ mul_double(%ymm1_mulpd_xmm_xmm[63:0], %ymm2_mulpd_xmm_xmm[63:0])))[127:0]

Final state
%xmm2: (%ymm2_vmulpd_xmm_xmm_xmm[255:128] ∘ (mul_double(%ymm2_vmulpd_xmm_xmm_xmm[127:64], %ymm3_vmulpd_xmm_xmm_xmm[127:64]) ∘ mul_double(%ymm2_vmulpd_xmm_xmm_xmm[63:0], %ymm3_vmulpd_xmm_xmm_xmm[63:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmulpd_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (mul_double(%ymm2_vmulpd_xmm_xmm_xmm[127:64], %ymm3_vmulpd_xmm_xmm_xmm[127:64]) ∘ mul_double(%ymm2_vmulpd_xmm_xmm_xmm[63:0], %ymm3_vmulpd_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vmulpd %xmm6, %xmm2, %xmm12

.target:
mulpd %xmm3, %xmm2
vmovdqu %xmm2, %xmm1
retq 

Initial state:
%ymm12: %ymm12_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vmulpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (mul_double(%ymm2_vmulpd_xmm_xmm_xmm[127:64], %ymm3_vmulpd_xmm_xmm_xmm[127:64]) ∘ mul_double(%ymm2_vmulpd_xmm_xmm_xmm[63:0], %ymm3_vmulpd_xmm_xmm_xmm[63:0]))

Final state
%ymm12: 0x0₁₂₈ ∘ (mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]) ∘ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r12_r13

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r13, %r9

Final state:
%r9/%r9: %ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r12, %r8

Final state:
%r8/%r8: %ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vxorps %xmm12, %xmm2, %xmm1

.target:
callq .move_128_064_xmm3_r12_r13
callq .move_128_064_xmm2_r8_r9
vzeroall 
xorq %r13, %r9
xorq %r12, %r8
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vxorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_movdqu_xmm_xmm
%rdx/%rdx: %rdx_movdqu_xmm_xmm

%xmm0: %ymm0_movdqu_xmm_xmm[127:0]
%xmm1: %ymm1_movdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_64_128_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movdqu_xmm_xmm
%rdx/%rdx: %rdx_movdqu_xmm_xmm

%xmm0: %ymm0_movdqu_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqu_xmm_xmm[255:128] ∘ ((%ymm11_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ (%ymm10_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqu %xmm2, %xmm1

.target:
callq .move_128_64_xmm2_xmm10_xmm11
callq .move_64_128_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))))[127:0]

State for specgen instruction: movdqu %xmm2, %xmm1:
%xmm1: (%ymm1_movdqu_xmm_xmm[255:128] ∘ ((%ymm11_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ (%ymm10_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))))[255:128] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for vpunpckldq %xmm13, %xmm2, %xmm1

.target:
vpbroadcastq %xmm3, %ymm6
vunpcklpd %xmm3, %xmm6, %xmm9
vpor %xmm9, %xmm9, %xmm7
unpcklps %xmm7, %xmm2
vmulpd %xmm6, %xmm2, %xmm12
vxorps %xmm12, %xmm2, %xmm1
movdqu %xmm2, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vhaddps_xmm_xmm_xmm

State for specgen instruction: vpunpckldq %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))))[255:128] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0]))

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vunpckhpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm10_xmm11

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vunpckhpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm3, %xmm8

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm8: (%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm8: ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm11, %xmm9

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm9: (%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm9: ((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: (((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0][127:0] ∘ ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vunpckhpd %ymm3, %ymm2, %ymm1

.target:
callq .move_256_128_ymm2_xmm8_xmm9
callq .move_256_128_ymm3_xmm10_xmm11
unpckhpd %xmm3, %xmm8
unpckhpd %xmm11, %xmm9
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm1: %ymm1_vpunpckhqdq_ymm_ymm_ymm

State for specgen instruction: vunpckhpd %ymm3, %ymm2, %ymm1:
%ymm1: ((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0][127:0] ∘ ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0][127:0]

Final state
%ymm1: %ymm3_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ (%ymm3_vpunpckhqdq_ymm_ymm_ymm[127:64] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[127:64])

=====================================
=====================================
Computing circuit for vpunpckhqdq %ymm12, %ymm1, %ymm11

.target:
vunpckhpd %ymm3, %ymm2, %ymm1
retq 

Initial state:
%ymm11: %ymm11_vhaddps_xmm_xmm_xmm

State for specgen instruction: vpunpckhqdq %ymm3, %ymm2, %ymm1:
%ymm1: %ymm3_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ (%ymm3_vpunpckhqdq_ymm_ymm_ymm[127:64] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[127:64])

Final state
%ymm11: 0x0₆₄ ∘ 0x0₆₄ ∘ (%ymm3_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm3_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_movlhps_xmm_xmm
%rdx/%rdx: %rdx_movlhps_xmm_xmm

%xmm0: %ymm0_movlhps_xmm_xmm[127:0]
%xmm1: %ymm1_movlhps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm11: %ymm11_movlhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movlhps_xmm_xmm[127:0][127:64])

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_movlhps_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_64_128_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movlhps_xmm_xmm
%rdx/%rdx: %rdx_movlhps_xmm_xmm

%xmm0: %ymm0_movlhps_xmm_xmm[127:0]
%xmm1: (%ymm1_movlhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ %ymm2_movlhps_xmm_xmm[127:0])[127:0][63:0] ∘ (%ymm10_movlhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movlhps_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movlhps %xmm12, %xmm1

.target:
callq .move_128_64_xmm1_xmm10_xmm11
vmovdqa %xmm2, %xmm11
callq .move_64_128_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[127:0]

State for specgen instruction: movlhps %xmm2, %xmm1:
%xmm1: (%ymm1_movlhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ %ymm2_movlhps_xmm_xmm[127:0])[127:0][63:0] ∘ (%ymm10_movlhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movlhps_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm10

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm10: %ymm10_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm10: 0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm11: %ymm11_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vaddps %ymm10, %ymm11, %ymm3

Final state:
%ymm3: add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[255:224]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[223:192]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[191:160]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[159:128]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[127:96]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[95:64]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[63:32]) ∘ add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[127:96], %ymm3_vaddps_xmm_xmm_xmm[127:96]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[95:64], %ymm3_vaddps_xmm_xmm_xmm[95:64]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[63:32], %ymm3_vaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vaddps_xmm_xmm_xmm[31:0], %ymm3_vaddps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vaddps %xmm2, %xmm1, %xmm2

.target:
vmovdqu %xmm3, %xmm10
vmovups %xmm2, %xmm11
vaddps %ymm10, %ymm11, %ymm3
vmovdqa %xmm3, %xmm1
retq 

Initial state:
%ymm2: %ymm2_addps_xmm_xmm

State for specgen instruction: vaddps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[127:96], %ymm3_vaddps_xmm_xmm_xmm[127:96]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[95:64], %ymm3_vaddps_xmm_xmm_xmm[95:64]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[63:32], %ymm3_vaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vaddps_xmm_xmm_xmm[31:0], %ymm3_vaddps_xmm_xmm_xmm[31:0]))))

Final state
%ymm2: 0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_addps_xmm_xmm
%rdx/%rdx: %rdx_addps_xmm_xmm

%xmm0: %ymm0_addps_xmm_xmm[127:0]
%xmm1: %ymm1_addps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_addps_xmm_xmm
%rdx/%rdx: %rdx_addps_xmm_xmm

%xmm0: %ymm0_addps_xmm_xmm[127:0]
%xmm1: (%ymm1_addps_xmm_xmm[255:128] ∘ ((%ymm7_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for addps %xmm11, %xmm1

.target:
vaddps %xmm2, %xmm1, %xmm2
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[127:0]

State for specgen instruction: addps %xmm2, %xmm1:
%xmm1: (%ymm1_addps_xmm_xmm[255:128] ∘ ((%ymm7_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (add_single(%ymm3_vhaddps_xmm_xmm_xmm[95:64], %ymm3_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm3_vhaddps_xmm_xmm_xmm[31:0], %ymm3_vhaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[95:64], %ymm2_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[31:0], %ymm2_vhaddps_xmm_xmm_xmm[63:32])))[127:0]

=====================================
=====================================
Computing circuit for vhaddps %xmm2, %xmm5, %xmm5

.target:
vpbroadcastq %xmm3, %xmm14
callq .move_128_64_xmm2_xmm12_xmm13
vunpckhps %xmm3, %xmm14, %xmm12
vpunpckldq %xmm13, %xmm2, %xmm1
vpunpckhqdq %ymm12, %ymm1, %ymm11
movlhps %xmm12, %xmm1
addps %xmm11, %xmm1
retq 

Initial state:
%ymm5: 0x0₁₂₈ ∘ %ymm1_haddps_xmm_xmm[127:0]

State for specgen instruction: vhaddps %xmm3, %xmm2, %xmm1:
%ymm1: ((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (add_single(%ymm3_vhaddps_xmm_xmm_xmm[95:64], %ymm3_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm3_vhaddps_xmm_xmm_xmm[31:0], %ymm3_vhaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[95:64], %ymm2_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[31:0], %ymm2_vhaddps_xmm_xmm_xmm[63:32]))

Final state
%ymm5: 0x0₁₂₈ ∘ (add_single(%ymm2_haddps_xmm_xmm[95:64], %ymm2_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm2_haddps_xmm_xmm[31:0], %ymm2_haddps_xmm_xmm[63:32]) ∘ add_single(%ymm1_haddps_xmm_xmm[95:64], %ymm1_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm1_haddps_xmm_xmm[31:0], %ymm1_haddps_xmm_xmm[63:32]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movups_xmm_xmm
%rdx/%rdx: %rdx_movups_xmm_xmm

%xmm0: %ymm0_movups_xmm_xmm[127:0]
%xmm1: %ymm1_movups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movups_xmm_xmm
%rdx/%rdx: %rdx_movups_xmm_xmm

%xmm0: %ymm0_movups_xmm_xmm[127:0]
%xmm1: (%ymm1_movups_xmm_xmm[255:128] ∘ ((%ymm7_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movups %xmm5, %xmm1

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: %ymm1_haddps_xmm_xmm[127:0]

State for specgen instruction: movups %xmm2, %xmm1:
%xmm1: (%ymm1_movups_xmm_xmm[255:128] ∘ ((%ymm7_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1_haddps_xmm_xmm[255:128] ∘ (add_single(%ymm2_haddps_xmm_xmm[95:64], %ymm2_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm2_haddps_xmm_xmm[31:0], %ymm2_haddps_xmm_xmm[63:32]) ∘ add_single(%ymm1_haddps_xmm_xmm[95:64], %ymm1_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm1_haddps_xmm_xmm[31:0], %ymm1_haddps_xmm_xmm[63:32])))[127:0]

=====================================
=====================================
Computing circuit for haddps %xmm3, %xmm10

.target:
vmovupd %xmm1, %xmm5
vhaddps %xmm2, %xmm5, %xmm5
movups %xmm5, %xmm1
retq 

Initial state:
%xmm10: (%ymm10_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[127:0])[127:0]

State for specgen instruction: haddps %xmm2, %xmm1:
%xmm1: (%ymm1_haddps_xmm_xmm[255:128] ∘ (add_single(%ymm2_haddps_xmm_xmm[95:64], %ymm2_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm2_haddps_xmm_xmm[31:0], %ymm2_haddps_xmm_xmm[63:32]) ∘ add_single(%ymm1_haddps_xmm_xmm[95:64], %ymm1_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm1_haddps_xmm_xmm[31:0], %ymm1_haddps_xmm_xmm[63:32])))[127:0]

Final state
%xmm10: ((%ymm10_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[127:0])[255:128] ∘ (add_single(%ymm3_vhaddps_ymm_ymm_ymm[95:64], %ymm3_vhaddps_ymm_ymm_ymm[127:96]) ∘ add_single(%ymm3_vhaddps_ymm_ymm_ymm[31:0], %ymm3_vhaddps_ymm_ymm_ymm[63:32]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[95:64], %ymm2_vhaddps_ymm_ymm_ymm[127:96]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[31:0], %ymm2_vhaddps_ymm_ymm_ymm[63:32])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm8_xmm9

Final state:
%rax/%rax: %rax_vhaddps_ymm_ymm_ymm
%rdx/%rdx: %rdx_vhaddps_ymm_ymm_ymm

%xmm0: %ymm0_vhaddps_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vhaddps_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm1, %xmm5

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm5: %ymm5_haddps_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm5: 0x0₁₂₈ ∘ %ymm1_haddps_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm3, %xmm14

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm14: %ymm14_vhaddps_xmm_xmm_xmm

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm14: 0x0₁₂₈ ∘ (%ymm3_vhaddps_xmm_xmm_xmm[63:0] ∘ %ymm3_vhaddps_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_vhaddps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vhaddps_xmm_xmm_xmm

%xmm0: %ymm0_vhaddps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vhaddps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm3, %xmm2

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm2: %ymm2_vunpckhps_xmm_xmm_xmm[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm2: (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vunpckhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpckhps_xmm_xmm_xmm

%xmm0: %ymm0_vunpckhps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vunpckhps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovddup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: (%ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm1, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0])

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

=====================================
=====================================
Computing circuit for vmovddup %xmm9, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
callq .move_064_128_r12_r13_xmm1
vpbroadcastq %xmm1, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vunpckhps_xmm_xmm_xmm

State for specgen instruction: vmovddup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovq_xmm_xmm
%rdx/%rdx: %rdx_vmovq_xmm_xmm

%xmm0: %ymm0_vmovq_xmm_xmm[127:0]
%xmm1: %ymm1_vmovq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movswq %bx, %r10

Final state:
%r10/%r10: sign-extend-64(%rbx_xchgl_r32_r32[15:0])

-------------------------------------
-------------------------------------
Getting base circuit for movslq %ebx, %rbx

Final state:
%rbx/%rbx: sign-extend-64(%rbx_xchgl_r32_r32[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_016_ecx_r8w_r9w

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_032_rbx_r10d_r11d

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %rcx

Final state:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_016_032_r8w_r9w_ebx

Final state:
%rax/%rax: %rax_xchgl_r32_r32
%rdx/%rdx: %rdx_xchgl_r32_r32

%xmm0: %ymm0_xchgl_r32_r32[127:0]
%xmm1: %ymm1_xchgl_r32_r32[127:0]

-------------------------------------
=====================================
Computing circuit for xchgl %ebx, %ecx

.target:
movswq %bx, %r10
movslq %ebx, %rbx
callq .move_032_016_ecx_r8w_r9w
callq .move_064_032_rbx_r10d_r11d
movq %r10, %rcx
callq .move_016_032_r8w_r9w_ebx
retq 

Initial state:
%rcx/%rcx: %rcx_xorl_r32_r32
%rbx/%rbx: %rbx_xorl_r32_r32

State for specgen instruction: xchgl %ecx, %ebx:
%rcx/%rcx: 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
%rbx/%rbx: 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])

Register        -> %rcx
  translates to => %rbx
Value is               -> 0x0₃₂ ∘ sign-extend-64(%rbx_xchgl_r32_r32[31:0])[31:0]
  after renaming it is => 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0]

Register        -> %rbx
  translates to => %rcx
Value is               -> 0x0₃₂ ∘ ((%r9_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][31:16])[15:0][15:0] ∘ (%r8_xchgl_r32_r32[63:16] ∘ %rcx_xchgl_r32_r32[31:0][15:0])[15:0][15:0])
  after renaming it is => 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

Final state
%rcx/%rcx: 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0]

=====================================
-------------------------------------
Getting base circuit for xorq %rcx, %rbx

Final state:
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

%cf: false
%pf: !((0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][0:0] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][1:1] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][2:2] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][3:3] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][4:4] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][5:5] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][6:6] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[7:0][7:7] = 0x1₁)
%zf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]) = 0x0₆₄
%sf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .set_szp_for_ebx

Final state:
%rax/%rax: %rax_xorl_r32_r32
%rdx/%rdx: %rdx_xorl_r32_r32

%xmm0: %ymm0_xorl_r32_r32[127:0]
%xmm1: %ymm1_xorl_r32_r32[127:0]

-------------------------------------
=====================================
Computing circuit for xorl %r13d, %r13d

.target:
xchgl %ebx, %ecx
xorq %rcx, %rbx
callq .set_szp_for_ebx
retq 

Initial state:
%r13/%r13: %ymm2_vmovq_xmm_xmm[127:0][127:64]

%cf: %cf_vmovq_xmm_xmm
%pf: %pf_vmovq_xmm_xmm
%zf: %zf_vmovq_xmm_xmm
%sf: %sf_vmovq_xmm_xmm
%of: %of_vmovq_xmm_xmm

State for specgen instruction: xorl %ecx, %ebx:
%rbx/%rbx: 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]

%cf: false
%pf: !((0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][0:0] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][1:1] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][2:2] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][3:3] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][4:4] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][5:5] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][6:6] = 0x1₁ ⊕ (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][7:0][7:7] = 0x1₁)
%zf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0] = 0x0₃₂
%sf: (0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0])[31:0][31:31] = 0x1₁
%of: false

Register        -> %rbx
  translates to => %r13
Value is               -> 0x0₃₂ ∘ %rcx_xorl_r32_r32[31:0] ⊕ 0x0₃₂ ∘ %rbx_xorl_r32_r32[31:0]
  after renaming it is => 0x0₆₄

Final state
%r13/%r13: 0x0₆₄

%cf: false
%pf: true
%zf: true
%sf: false
%of: false

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovq_xmm_xmm
%rdx/%rdx: %rdx_vmovq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (0x0₆₄[63:0] ∘ %ymm2_vmovq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %xmm10

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
xorl %r13d, %r13d
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm10: %ymm10_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][95:64])

State for specgen instruction: vmovq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (0x0₆₄[63:0] ∘ %ymm2_vmovq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm10: 0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vunpckhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpckhps_xmm_xmm_xmm

%xmm0: %ymm0_vunpckhps_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vunpckhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpckhps_xmm_xmm_xmm

%xmm0: %ymm0_vunpckhps_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[255:128] ∘ ((%ymm11_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:96]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[127:0][31:0] ∘ ((%ymm9_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:32]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:64]))[127:0][31:0] ∘ ((%ymm8_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][31:0]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vunpckhps %xmm3, %xmm14, %xmm12

.target:
unpckhpd %xmm3, %xmm2
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovddup %xmm9, %xmm1
vmovq %xmm1, %xmm10
callq .move_128_64_xmm2_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm12: %ymm12_vhaddps_xmm_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_vhaddps_xmm_xmm_xmm[127:0][63:0])

State for specgen instruction: vunpckhps %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96] ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[255:128] ∘ ((%ymm11_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:96]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ (0x0₃₂ ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:96])))[127:0][31:0] ∘ ((%ymm9_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:32]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][127:64]))[127:0][31:0] ∘ ((%ymm8_vunpckhps_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][31:0]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_vunpckhps_xmm_xmm_xmm[255:128] ∘ (%ymm3_vunpckhps_xmm_xmm_xmm[127:64] ∘ %ymm2_vunpckhps_xmm_xmm_xmm[127:64]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm12: 0x0₁₂₈ ∘ (%ymm3_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm3_vhaddps_xmm_xmm_xmm[63:32] ∘ %ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpbroadcastq_ymm_xmm

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm1, %xmm8

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm8: %ymm8_vpbroadcastq_ymm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for vminpd %ymm2, %ymm2, %ymm1

Final state:
%ymm1: (mincmp_double(%ymm2_vmovdqa_ymm_ymm[255:192], %ymm2_vmovdqa_ymm_ymm[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[255:192] : %ymm2_vmovdqa_ymm_ymm[255:192]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[191:128], %ymm2_vmovdqa_ymm_ymm[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[191:128] : %ymm2_vmovdqa_ymm_ymm[191:128]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[127:64], %ymm2_vmovdqa_ymm_ymm[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[127:64] : %ymm2_vmovdqa_ymm_ymm[127:64]) ∘ (mincmp_double(%ymm2_vmovdqa_ymm_ymm[63:0], %ymm2_vmovdqa_ymm_ymm[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[63:0] : %ymm2_vmovdqa_ymm_ymm[63:0])))

-------------------------------------
=====================================
Computing circuit for vmovdqa %ymm1, %ymm9

.target:
vminpd %ymm2, %ymm2, %ymm1
retq 

Initial state:
%ymm9: %ymm9_vpbroadcastq_ymm_xmm

State for specgen instruction: vmovdqa %ymm2, %ymm1:
%ymm1: (mincmp_double(%ymm2_vmovdqa_ymm_ymm[255:192], %ymm2_vmovdqa_ymm_ymm[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[255:192] : %ymm2_vmovdqa_ymm_ymm[255:192]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[191:128], %ymm2_vmovdqa_ymm_ymm[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[191:128] : %ymm2_vmovdqa_ymm_ymm[191:128]) ∘ ((mincmp_double(%ymm2_vmovdqa_ymm_ymm[127:64], %ymm2_vmovdqa_ymm_ymm[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[127:64] : %ymm2_vmovdqa_ymm_ymm[127:64]) ∘ (mincmp_double(%ymm2_vmovdqa_ymm_ymm[63:0], %ymm2_vmovdqa_ymm_ymm[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqa_ymm_ymm[63:0] : %ymm2_vmovdqa_ymm_ymm[63:0])))

Final state
%ymm9: 0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vpbroadcastq_ymm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_ymm_xmm

%xmm0: %ymm0_vpbroadcastq_ymm_xmm[127:0]
%xmm1: ((0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])))[127:0][127:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm3, %ymm6

.target:
vpbroadcastq %xmm2, %xmm1
vmovupd %xmm1, %xmm8
vmovdqa %ymm1, %ymm9
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm6: %ymm6_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vpbroadcastq %xmm2, %ymm1:
%ymm1: (0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0])))[127:0][127:0] ∘ (0x0₁₂₈ ∘ (%ymm2_vpbroadcastq_ymm_xmm[63:0] ∘ %ymm2_vpbroadcastq_ymm_xmm[63:0]))[127:0][127:0]

Final state
%ymm6: %ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm3, %r8

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r8/%r8: %r8_vunpcklpd_xmm_xmm_xmm

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r8
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

Final state
%r8/%r8: %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vunpcklpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpcklpd_xmm_xmm_xmm

%xmm0: %ymm0_vunpcklpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vunpcklpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vunpcklpd_xmm_xmm_xmm[63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vunpcklpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vunpcklpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vunpcklpd_xmm_xmm_xmm[63:0][63:0] ∘ %ymm2_vunpcklpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vunpcklpd %xmm3, %xmm6, %xmm9

.target:
movq %xmm3, %r8
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vunpcklpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vunpcklpd_xmm_xmm_xmm[63:0][63:0] ∘ %ymm2_vunpcklpd_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm3, %xmm2, %xmm1

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vorps_xmm_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vorps %xmm3, %xmm2, %xmm14

.target:
vorpd %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm14: %ymm14_vpor_xmm_xmm_xmm

State for specgen instruction: vorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

Final state
%ymm14: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm14, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpor_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vpor %xmm9, %xmm9, %xmm7

.target:
vorps %xmm3, %xmm2, %xmm14
vmovapd %xmm14, %xmm1
retq 

Initial state:
%ymm7: %ymm7_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vpor %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

Final state
%ymm7: 0x0₁₂₈ ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:0] ∘ %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastsd_ymm_xmm
%rdx/%rdx: %rdx_vbroadcastsd_ymm_xmm

%xmm0: %ymm0_vbroadcastsd_ymm_xmm[127:0]
%xmm1: %ymm1_vbroadcastsd_ymm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm10, %xmm10, %xmm11

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm11: %ymm11_vbroadcastsd_ymm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_vbroadcastsd_ymm_xmm[127:0][127:64])

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for vmaxps %ymm2, %ymm2, %ymm10

Final state:
%ymm10: (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for vmaxpd %ymm10, %ymm2, %ymm1

Final state:
%ymm1: (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[255:192], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:192] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[191:128], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:128] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[127:64], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:64] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64]) ∘ (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[63:0], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:0] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])))

-------------------------------------
=====================================
Computing circuit for vmovdqu %ymm11, %ymm10

.target:
vmaxps %ymm2, %ymm2, %ymm10
vmaxpd %ymm10, %ymm2, %ymm1
retq 

Initial state:
%ymm10: %ymm10_vbroadcastsd_ymm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_vbroadcastsd_ymm_xmm[127:0][63:0])

State for specgen instruction: vmovdqu %ymm2, %ymm1:
%ymm1: (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[255:192], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:192] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[255:192]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[191:128], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:128] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[191:128]) ∘ ((maxcmp_double(%ymm2_vmovdqu_ymm_ymm[127:64], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:64] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[127:64]) ∘ (maxcmp_double(%ymm2_vmovdqu_ymm_ymm[63:0], ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:0] : ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[255:224], %ymm2_vmovdqu_ymm_ymm[255:224])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[255:224] : %ymm2_vmovdqu_ymm_ymm[255:224]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[223:192], %ymm2_vmovdqu_ymm_ymm[223:192])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[223:192] : %ymm2_vmovdqu_ymm_ymm[223:192]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[191:160], %ymm2_vmovdqu_ymm_ymm[191:160])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[191:160] : %ymm2_vmovdqu_ymm_ymm[191:160]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[159:128], %ymm2_vmovdqu_ymm_ymm[159:128])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[159:128] : %ymm2_vmovdqu_ymm_ymm[159:128]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[127:96], %ymm2_vmovdqu_ymm_ymm[127:96])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[127:96] : %ymm2_vmovdqu_ymm_ymm[127:96]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[95:64], %ymm2_vmovdqu_ymm_ymm[95:64])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[95:64] : %ymm2_vmovdqu_ymm_ymm[95:64]) ∘ ((maxcmp_single(%ymm2_vmovdqu_ymm_ymm[63:32], %ymm2_vmovdqu_ymm_ymm[63:32])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[63:32] : %ymm2_vmovdqu_ymm_ymm[63:32]) ∘ (maxcmp_single(%ymm2_vmovdqu_ymm_ymm[31:0], %ymm2_vmovdqu_ymm_ymm[31:0])[0:0] = 0x1₁ ? %ymm2_vmovdqu_ymm_ymm[31:0] : %ymm2_vmovdqu_ymm_ymm[31:0]))))))))[63:0])))

Final state
%ymm10: 0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vbroadcastsd_ymm_xmm
%rdx/%rdx: %rdx_vbroadcastsd_ymm_xmm

%xmm0: %ymm0_vbroadcastsd_ymm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0]))[127:0][127:0] ∘ (0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0])))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastsd %xmm1, %ymm9

.target:
callq .move_128_64_xmm2_xmm10_xmm11
vpunpcklqdq %xmm10, %xmm10, %xmm11
vmovdqu %ymm11, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm9: %ymm9_unpcklps_xmm_xmm

State for specgen instruction: vbroadcastsd %xmm2, %ymm1:
%ymm1: (0x0₁₂₈ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0]))[127:0][127:0] ∘ (0x0₆₄ ∘ (0x0₆₄ ∘ (%ymm2_vbroadcastsd_ymm_xmm[63:0] ∘ %ymm2_vbroadcastsd_ymm_xmm[63:0])))[127:0][127:0]

Final state
%ymm9: %ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0] ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm9, %xmm10

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm10: %ymm10_unpcklps_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm10: 0x0₁₂₈ ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovddup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovddup_xmm_xmm
%rdx/%rdx: %rdx_vmovddup_xmm_xmm

%xmm0: %ymm0_vmovddup_xmm_xmm[127:0]
%xmm1: (%ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: %ymm0_vpbroadcastq_xmm_xmm[127:0]
%xmm1: %ymm1_vpbroadcastq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r10, %r11

Final state:
%r11/%r11: %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpbroadcastq_xmm_xmm
%rdx/%rdx: %rdx_vpbroadcastq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpbroadcastq %xmm1, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r10, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovddup_xmm_xmm[255:128] ∘ (%ymm2_vmovddup_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovddup_xmm_xmm[127:0][63:0][63:0])

State for specgen instruction: vpbroadcastq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpbroadcastq_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

=====================================
=====================================
Computing circuit for vmovddup %xmm2, %xmm15

.target:
callq .move_128_064_xmm2_r12_r13
callq .move_064_128_r12_r13_xmm1
vpbroadcastq %xmm1, %xmm1
retq 

Initial state:
%ymm15: %ymm15_unpcklps_xmm_xmm

State for specgen instruction: vmovddup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovddup_xmm_xmm[63:0] ∘ %ymm2_vmovddup_xmm_xmm[63:0])

Final state
%ymm15: 0x0₁₂₈ ∘ (%ymm2_unpcklps_xmm_xmm[63:0] ∘ %ymm2_unpcklps_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhps_xmm_xmm
%rdx/%rdx: %rdx_unpckhps_xmm_xmm

%xmm0: %ymm0_unpckhps_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_unpckhps_xmm_xmm
%rdx/%rdx: %rdx_unpckhps_xmm_xmm

%xmm0: %ymm0_unpckhps_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmaxss_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_vmaxss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmaxss_xmm_xmm_xmm

%xmm0: %ymm0_vmaxss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm3

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm3: %ymm3_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm11: %ymm11_vmaxps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vmaxps %ymm3, %ymm11, %ymm1

Final state:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vmaxps %xmm3, %xmm4, %xmm13

.target:
vmovdqa %xmm3, %xmm3
vmovdqa %xmm2, %xmm11
vmaxps %ymm3, %ymm11, %ymm1
retq 

Initial state:
%ymm13: %ymm13_vmaxss_xmm_xmm_xmm

State for specgen instruction: vmaxps %xmm3, %xmm2, %xmm1:
%ymm1: (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[63:32]) ∘ (maxcmp_single((0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vmaxps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vmaxps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm13: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[127:96]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[127:96]) ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[95:64]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[95:64]) ∘ ((maxcmp_single(0x0₃₂, %ymm3_vmaxss_xmm_xmm_xmm[63:32]) = 0x1₁ ? 0x0₃₂ : %ymm3_vmaxss_xmm_xmm_xmm[63:32]) ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0])))))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: %ymm1_movss_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: %ymm0_vpmovzxdq_xmm_xmm[127:0]
%xmm1: %ymm1_vpmovzxdq_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_byte_5_of_ymm1_to_r9b

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm2

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_64_128_xmm8_xmm9_xmm1

Final state:
%rax/%rax: %rax_vpmovzxdq_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxdq_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpmovzxdq %xmm2, %xmm8

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_byte_5_of_ymm1_to_r9b
callq .move_064_128_r8_r9_xmm2
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
callq .move_64_128_xmm8_xmm9_xmm1
retq 

Initial state:
%ymm8: %ymm8_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][31:0])

State for specgen instruction: vpmovzxdq %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][63:32]))[127:0][63:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₉₆ ∘ (0x0₂₅₆[255:128] ∘ ((%ymm2_vpmovzxdq_xmm_xmm[127:0][127:64][63:8] ∘ 0x0₂₅₆[47:40])[63:0] ∘ %ymm2_vpmovzxdq_xmm_xmm[127:0][63:0][63:0]))[127:0][31:0]))[127:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movss_xmm_xmm
%rdx/%rdx: %rdx_movss_xmm_xmm

%xmm0: %ymm0_movss_xmm_xmm[127:0]
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movss %xmm13, %xmm1

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
vpmovzxdq %xmm2, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[127:0]

State for specgen instruction: movss %xmm2, %xmm1:
%xmm1: (%ymm1_movss_xmm_xmm[255:128] ∘ ((%ymm11_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_movss_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movss_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[63:32] ∘ (0x0₃₂ ∘ %ymm2_movss_xmm_xmm[31:0])))[127:0][31:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vmaxss_xmm_xmm_xmm[127:32] ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for vmaxss %xmm13, %xmm13, %xmm0

.target:
vmovupd %xmm2, %xmm1
callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7
vmaxps %xmm3, %xmm4, %xmm13
movss %xmm13, %xmm1
retq 

Initial state:
%ymm0: %ymm0_unpckhps_xmm_xmm

State for specgen instruction: vmaxss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ %ymm2_vmaxss_xmm_xmm_xmm[127:0])[255:128] ∘ (%ymm2_vmaxss_xmm_xmm_xmm[127:32] ∘ (maxcmp_single(%ymm2_vmaxss_xmm_xmm_xmm[31:0], %ymm3_vmaxss_xmm_xmm_xmm[31:0]) = 0x1₁ ? %ymm2_vmaxss_xmm_xmm_xmm[31:0] : %ymm3_vmaxss_xmm_xmm_xmm[31:0]))

Final state
%ymm0: 0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm2_unpckhps_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: %ymm0_vrcpps_xmm_xmm[127:0]
%xmm1: %ymm1_vrcpps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vrcpps %ymm1, %ymm10

Final state:
%ymm10: approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vrcpps %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
vrcpps %ymm1, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmovss_xmm_xmm_xmm

State for specgen instruction: vrcpps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm8

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm8: %ymm8_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovss %xmm11, %xmm13, %xmm9

.target:
vrcpps %xmm3, %xmm1
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovdqu %xmm3, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm9: %ymm9_unpckhps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_unpckhps_xmm_xmm[127:0][63:32])

State for specgen instruction: vmovss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm1_unpckhps_xmm_xmm[127:96])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovsldup_xmm_xmm
%rdx/%rdx: %rdx_vmovsldup_xmm_xmm

%xmm0: %ymm0_vmovsldup_xmm_xmm[127:0]
%xmm1: %ymm1_vmovsldup_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm2, %xmm9

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: %ymm1_movd_r32_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r10_r11

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_movd_r32_xmm
%rdx/%rdx: %rdx_movd_r32_xmm

%xmm0: %ymm0_movd_r32_xmm[127:0]
%xmm1: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for movd %xmm2, %r8d

.target:
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
callq .move_128_256_xmm8_xmm9_ymm1
callq .move_128_064_xmm1_r10_r11
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r8/%r8: %r8_vbroadcastss_xmm_xmm

State for specgen instruction: movd %xmm1, %ebx:
%rbx/%rbx: ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]

Register        -> %rbx
  translates to => %r8
Value is               -> ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][127:64][31:0][31:0] ∘ ((%ymm9_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][63:32]))[127:0][127:0] ∘ (%ymm8_movd_r32_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_movd_r32_xmm[127:0][31:0]))[127:0][127:0])[127:0][63:0][31:0][31:0]
  after renaming it is => 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

Final state
%r8/%r8: 0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0]

=====================================
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movslq %r8d, %r9

Final state:
%r9/%r9: sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm8_xmm9

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vbroadcastss_xmm_xmm
%rdx/%rdx: %rdx_vbroadcastss_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vbroadcastss %xmm13, %xmm5

.target:
movd %xmm2, %r8d
vzeroall 
movslq %r8d, %r9
callq .move_064_128_r8_r9_xmm1
callq .move_128_64_xmm1_xmm10_xmm11
callq .move_128_64_xmm1_xmm8_xmm9
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm5: %ymm5_vmovsldup_xmm_xmm

State for specgen instruction: vbroadcastss %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[255:128] ∘ ((0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][127:64]))[127:0][31:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₆₄ ∘ (0x0₂₅₆[255:128] ∘ (sign-extend-64((0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[31:0])[63:0] ∘ (0x0₃₂ ∘ %ymm2_vbroadcastss_xmm_xmm[31:0])[63:0]))[127:0][63:0]))[127:0][31:0])

Final state
%ymm5: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm5, %xmm9, %xmm1

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vmovsldup_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

=====================================
=====================================
Computing circuit for vmovsldup %xmm2, %xmm12

.target:
callq .move_128_64_xmm2_xmm12_xmm13
vbroadcastss %xmm2, %xmm9
vbroadcastss %xmm13, %xmm5
vpunpcklqdq %xmm5, %xmm9, %xmm1
retq 

Initial state:
%ymm12: %ymm12_movsldup_xmm_xmm

State for specgen instruction: vmovsldup %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vmovsldup_xmm_xmm[95:64] ∘ %ymm2_vmovsldup_xmm_xmm[95:64] ∘ (%ymm2_vmovsldup_xmm_xmm[31:0] ∘ %ymm2_vmovsldup_xmm_xmm[31:0]))

Final state
%ymm12: 0x0₁₂₈ ∘ (%ymm2_movsldup_xmm_xmm[95:64] ∘ %ymm2_movsldup_xmm_xmm[95:64] ∘ (%ymm2_movsldup_xmm_xmm[31:0] ∘ %ymm2_movsldup_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm12, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_movsldup_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_movsldup_xmm_xmm[255:128] ∘ (%ymm2_movsldup_xmm_xmm[95:64] ∘ %ymm2_movsldup_xmm_xmm[95:64] ∘ (%ymm2_movsldup_xmm_xmm[31:0] ∘ %ymm2_movsldup_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for movsldup %xmm0, %xmm13

.target:
vmovsldup %xmm2, %xmm12
movdqa %xmm12, %xmm1
retq 

Initial state:
%xmm13: (%ymm13_unpckhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:0][127:64]))[127:0]

State for specgen instruction: movsldup %xmm2, %xmm1:
%xmm1: (%ymm1_movsldup_xmm_xmm[255:128] ∘ (%ymm2_movsldup_xmm_xmm[95:64] ∘ %ymm2_movsldup_xmm_xmm[95:64] ∘ (%ymm2_movsldup_xmm_xmm[31:0] ∘ %ymm2_movsldup_xmm_xmm[31:0])))[127:0]

Final state
%xmm13: ((%ymm13_unpckhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:0][127:64]))[255:128] ∘ (0x0₆₄ ∘ (%ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm2_unpckhps_xmm_xmm[95:64])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: %ymm0_vrcpps_xmm_xmm[127:0]
%xmm1: %ymm1_vrcpps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vrcpps %ymm1, %ymm10

Final state:
%ymm10: approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vrcpps %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
vrcpps %ymm1, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmovss_xmm_xmm_xmm

State for specgen instruction: vrcpps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm8

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm8: %ymm8_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovss %xmm10, %xmm13, %xmm8

.target:
vrcpps %xmm3, %xmm1
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovdqu %xmm3, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm8: %ymm8_unpckhps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm1_unpckhps_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0])

Final state
%ymm8: 0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm1_unpckhps_xmm_xmm[95:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_64_128_xmm8_xmm9_xmm1

Final state:
%rax/%rax: %rax_unpckhps_xmm_xmm
%rdx/%rdx: %rdx_unpckhps_xmm_xmm

%xmm0: (0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm2_unpckhps_xmm_xmm[95:64]))[127:0]
%xmm1: (%ymm1_unpckhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm1_unpckhps_xmm_xmm[127:96]))[127:0][63:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm1_unpckhps_xmm_xmm[95:64]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhps %xmm15, %xmm10

.target:
callq .move_128_64_xmm2_xmm12_xmm13
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11
vmaxss %xmm13, %xmm13, %xmm0
vmovss %xmm11, %xmm13, %xmm9
movsldup %xmm0, %xmm13
vmovss %xmm10, %xmm13, %xmm8
callq .move_64_128_xmm8_xmm9_xmm1
retq 

Initial state:
%xmm10: (0x0₁₂₈ ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0]))[127:0]

State for specgen instruction: unpckhps %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[127:96] ∘ %ymm1_unpckhps_xmm_xmm[127:96]))[127:0][63:0] ∘ (0x0₁₂₈ ∘ (0x0₆₄ ∘ %ymm2_unpckhps_xmm_xmm[95:64] ∘ %ymm1_unpckhps_xmm_xmm[95:64]))[127:0][63:0]))[127:0]

Final state
%xmm10: ((0x0₁₂₈ ∘ (%ymm1_unpcklps_xmm_xmm[63:0] ∘ %ymm1_unpcklps_xmm_xmm[63:0]))[255:128] ∘ (%ymm2_unpcklps_xmm_xmm[63:32] ∘ %ymm1_unpcklps_xmm_xmm[63:32] ∘ (%ymm2_unpcklps_xmm_xmm[31:0] ∘ %ymm1_unpcklps_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_movapd_xmm_xmm
%rdx/%rdx: %rdx_movapd_xmm_xmm

%xmm0: %ymm0_movapd_xmm_xmm[127:0]
%xmm1: %ymm1_movapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_movapd_xmm_xmm
%rdx/%rdx: %rdx_movapd_xmm_xmm

%xmm0: %ymm0_movapd_xmm_xmm[127:0]
%xmm1: (%ymm1_movapd_xmm_xmm[255:128] ∘ (%ymm2_movapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movapd %xmm10, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm1: %ymm1_unpcklps_xmm_xmm[127:0]

State for specgen instruction: movapd %xmm2, %xmm1:
%xmm1: (%ymm1_movapd_xmm_xmm[255:128] ∘ (%ymm2_movapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_unpcklps_xmm_xmm[255:128] ∘ (%ymm2_unpcklps_xmm_xmm[63:32] ∘ %ymm1_unpcklps_xmm_xmm[63:32] ∘ (%ymm2_unpcklps_xmm_xmm[31:0] ∘ %ymm1_unpcklps_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for unpcklps %xmm7, %xmm2

.target:
vbroadcastsd %xmm1, %ymm9
vmovdqa %xmm9, %xmm10
vmovddup %xmm2, %xmm15
unpckhps %xmm15, %xmm10
movapd %xmm10, %xmm1
retq 

Initial state:
%xmm2: %ymm2_vpunpckldq_xmm_xmm_xmm[127:0]

State for specgen instruction: unpcklps %xmm2, %xmm1:
%xmm1: (%ymm1_unpcklps_xmm_xmm[255:128] ∘ (%ymm2_unpcklps_xmm_xmm[63:32] ∘ %ymm1_unpcklps_xmm_xmm[63:32] ∘ (%ymm2_unpcklps_xmm_xmm[31:0] ∘ %ymm1_unpcklps_xmm_xmm[31:0])))[127:0]

Final state
%xmm2: (%ymm2_vpunpckldq_xmm_xmm_xmm[255:128] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm1, %xmm3

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm3: %ymm3_mulpd_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: %ymm0_vmovaps_xmm_xmm[127:0]
%xmm1: %ymm1_vmovaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovaps %xmm2, %xmm8

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm8: %ymm8_mulpd_xmm_xmm

State for specgen instruction: vmovaps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vmulpd %ymm8, %ymm3, %ymm6

Final state:
%ymm6: mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[255:192], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[255:192]) ∘ (mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[191:128], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[191:128]) ∘ (mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[127:64], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[127:64]) ∘ mul_double((0x0₁₂₈ ∘ %ymm1_mulpd_xmm_xmm[127:0])[63:0], (0x0₁₂₈ ∘ %ymm2_mulpd_xmm_xmm[127:0])[63:0])))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm6, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_mulpd_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_mulpd_xmm_xmm[255:128] ∘ (mul_double(%ymm1_mulpd_xmm_xmm[127:64], %ymm2_mulpd_xmm_xmm[127:64]) ∘ mul_double(%ymm1_mulpd_xmm_xmm[63:0], %ymm2_mulpd_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for mulpd %xmm3, %xmm2

.target:
vmovapd %xmm1, %xmm3
vmovaps %xmm2, %xmm8
vmulpd %ymm8, %ymm3, %ymm6
movdqa %xmm6, %xmm1
retq 

Initial state:
%xmm2: %ymm2_vmulpd_xmm_xmm_xmm[127:0]

State for specgen instruction: mulpd %xmm2, %xmm1:
%xmm1: (%ymm1_mulpd_xmm_xmm[255:128] ∘ (mul_double(%ymm1_mulpd_xmm_xmm[127:64], %ymm2_mulpd_xmm_xmm[127:64]) ∘ mul_double(%ymm1_mulpd_xmm_xmm[63:0], %ymm2_mulpd_xmm_xmm[63:0])))[127:0]

Final state
%xmm2: (%ymm2_vmulpd_xmm_xmm_xmm[255:128] ∘ (mul_double(%ymm2_vmulpd_xmm_xmm_xmm[127:64], %ymm3_vmulpd_xmm_xmm_xmm[127:64]) ∘ mul_double(%ymm2_vmulpd_xmm_xmm_xmm[63:0], %ymm3_vmulpd_xmm_xmm_xmm[63:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmulpd_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (mul_double(%ymm2_vmulpd_xmm_xmm_xmm[127:64], %ymm3_vmulpd_xmm_xmm_xmm[127:64]) ∘ mul_double(%ymm2_vmulpd_xmm_xmm_xmm[63:0], %ymm3_vmulpd_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vmulpd %xmm6, %xmm2, %xmm12

.target:
mulpd %xmm3, %xmm2
vmovdqu %xmm2, %xmm1
retq 

Initial state:
%ymm12: %ymm12_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vmulpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (mul_double(%ymm2_vmulpd_xmm_xmm_xmm[127:64], %ymm3_vmulpd_xmm_xmm_xmm[127:64]) ∘ mul_double(%ymm2_vmulpd_xmm_xmm_xmm[63:0], %ymm3_vmulpd_xmm_xmm_xmm[63:0]))

Final state
%ymm12: 0x0₁₂₈ ∘ (mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]) ∘ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r12_r13

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r13, %r9

Final state:
%r9/%r9: %ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r12, %r8

Final state:
%r8/%r8: %ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vxorps %xmm12, %xmm2, %xmm1

.target:
callq .move_128_064_xmm3_r12_r13
callq .move_128_064_xmm2_r8_r9
vzeroall 
xorq %r13, %r9
xorq %r12, %r8
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpunpckldq_xmm_xmm_xmm

State for specgen instruction: vxorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_movdqu_xmm_xmm
%rdx/%rdx: %rdx_movdqu_xmm_xmm

%xmm0: %ymm0_movdqu_xmm_xmm[127:0]
%xmm1: %ymm1_movdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_64_128_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movdqu_xmm_xmm
%rdx/%rdx: %rdx_movdqu_xmm_xmm

%xmm0: %ymm0_movdqu_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqu_xmm_xmm[255:128] ∘ ((%ymm11_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ (%ymm10_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqu %xmm2, %xmm1

.target:
callq .move_128_64_xmm2_xmm10_xmm11
callq .move_64_128_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))))[127:0]

State for specgen instruction: movdqu %xmm2, %xmm1:
%xmm1: (%ymm1_movdqu_xmm_xmm[255:128] ∘ ((%ymm11_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ (%ymm10_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))))[255:128] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0])))[127:0]

=====================================
=====================================
Computing circuit for vpunpckldq %xmm13, %xmm2, %xmm1

.target:
vpbroadcastq %xmm3, %ymm6
vunpcklpd %xmm3, %xmm6, %xmm9
vpor %xmm9, %xmm9, %xmm7
unpcklps %xmm7, %xmm2
vmulpd %xmm6, %xmm2, %xmm12
vxorps %xmm12, %xmm2, %xmm1
movdqu %xmm2, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vhaddps_xmm_xmm_xmm

State for specgen instruction: vpunpckldq %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ ((%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0])) ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0] ⊕ mul_double(%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0], %ymm3_vpunpckldq_xmm_xmm_xmm[63:0]))))[255:128] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[63:32] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[63:32] ∘ (%ymm3_vpunpckldq_xmm_xmm_xmm[31:0] ∘ %ymm2_vpunpckldq_xmm_xmm_xmm[31:0]))

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vunpckhpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm10_xmm11

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vunpckhpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm3, %xmm8

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm8: (%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm8: ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm11, %xmm9

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm9: (%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm9: ((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: (((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0][127:0] ∘ ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vunpckhpd %ymm3, %ymm2, %ymm1

.target:
callq .move_256_128_ymm2_xmm8_xmm9
callq .move_256_128_ymm3_xmm10_xmm11
unpckhpd %xmm3, %xmm8
unpckhpd %xmm11, %xmm9
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm1: %ymm1_vpunpckhqdq_ymm_ymm_ymm

State for specgen instruction: vunpckhpd %ymm3, %ymm2, %ymm1:
%ymm1: ((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0][127:0] ∘ ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0][127:0]

Final state
%ymm1: %ymm3_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ (%ymm3_vpunpckhqdq_ymm_ymm_ymm[127:64] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[127:64])

=====================================
=====================================
Computing circuit for vpunpckhqdq %ymm12, %ymm1, %ymm11

.target:
vunpckhpd %ymm3, %ymm2, %ymm1
retq 

Initial state:
%ymm11: %ymm11_vhaddps_xmm_xmm_xmm

State for specgen instruction: vpunpckhqdq %ymm3, %ymm2, %ymm1:
%ymm1: %ymm3_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[255:192] ∘ (%ymm3_vpunpckhqdq_ymm_ymm_ymm[127:64] ∘ %ymm2_vpunpckhqdq_ymm_ymm_ymm[127:64])

Final state
%ymm11: 0x0₆₄ ∘ 0x0₆₄ ∘ (%ymm3_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm3_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm10_xmm11

Final state:
%rax/%rax: %rax_movlhps_xmm_xmm
%rdx/%rdx: %rdx_movlhps_xmm_xmm

%xmm0: %ymm0_movlhps_xmm_xmm[127:0]
%xmm1: %ymm1_movlhps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm11: %ymm11_movlhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movlhps_xmm_xmm[127:0][127:64])

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_movlhps_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_64_128_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movlhps_xmm_xmm
%rdx/%rdx: %rdx_movlhps_xmm_xmm

%xmm0: %ymm0_movlhps_xmm_xmm[127:0]
%xmm1: (%ymm1_movlhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ %ymm2_movlhps_xmm_xmm[127:0])[127:0][63:0] ∘ (%ymm10_movlhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movlhps_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movlhps %xmm12, %xmm1

.target:
callq .move_128_64_xmm1_xmm10_xmm11
vmovdqa %xmm2, %xmm11
callq .move_64_128_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[127:0]

State for specgen instruction: movlhps %xmm2, %xmm1:
%xmm1: (%ymm1_movlhps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ %ymm2_movlhps_xmm_xmm[127:0])[127:0][63:0] ∘ (%ymm10_movlhps_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm1_movlhps_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm10

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm10: %ymm10_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm10: 0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm11: %ymm11_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vaddps %ymm10, %ymm11, %ymm3

Final state:
%ymm3: add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[255:224]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[223:192]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[191:160]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[159:128]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[127:96]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[95:64]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[63:32]) ∘ add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[127:96], %ymm3_vaddps_xmm_xmm_xmm[127:96]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[95:64], %ymm3_vaddps_xmm_xmm_xmm[95:64]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[63:32], %ymm3_vaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vaddps_xmm_xmm_xmm[31:0], %ymm3_vaddps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vaddps %xmm2, %xmm1, %xmm2

.target:
vmovdqu %xmm3, %xmm10
vmovups %xmm2, %xmm11
vaddps %ymm10, %ymm11, %ymm3
vmovdqa %xmm3, %xmm1
retq 

Initial state:
%ymm2: %ymm2_addps_xmm_xmm

State for specgen instruction: vaddps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[127:96], %ymm3_vaddps_xmm_xmm_xmm[127:96]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[95:64], %ymm3_vaddps_xmm_xmm_xmm[95:64]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[63:32], %ymm3_vaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vaddps_xmm_xmm_xmm[31:0], %ymm3_vaddps_xmm_xmm_xmm[31:0]))))

Final state
%ymm2: 0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_addps_xmm_xmm
%rdx/%rdx: %rdx_addps_xmm_xmm

%xmm0: %ymm0_addps_xmm_xmm[127:0]
%xmm1: %ymm1_addps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_addps_xmm_xmm
%rdx/%rdx: %rdx_addps_xmm_xmm

%xmm0: %ymm0_addps_xmm_xmm[127:0]
%xmm1: (%ymm1_addps_xmm_xmm[255:128] ∘ ((%ymm7_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for addps %xmm11, %xmm1

.target:
vaddps %xmm2, %xmm1, %xmm2
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: ((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[127:0]

State for specgen instruction: addps %xmm2, %xmm1:
%xmm1: (%ymm1_addps_xmm_xmm[255:128] ∘ ((%ymm7_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_addps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addps_xmm_xmm[127:96], %ymm2_addps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addps_xmm_xmm[95:64], %ymm2_addps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addps_xmm_xmm[63:32], %ymm2_addps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addps_xmm_xmm[31:0], %ymm2_addps_xmm_xmm[31:0])))))[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (add_single(%ymm3_vhaddps_xmm_xmm_xmm[95:64], %ymm3_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm3_vhaddps_xmm_xmm_xmm[31:0], %ymm3_vhaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[95:64], %ymm2_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[31:0], %ymm2_vhaddps_xmm_xmm_xmm[63:32])))[127:0]

=====================================
=====================================
Computing circuit for vhaddps %xmm2, %xmm5, %xmm5

.target:
vpbroadcastq %xmm3, %xmm14
callq .move_128_64_xmm2_xmm12_xmm13
vunpckhps %xmm3, %xmm14, %xmm12
vpunpckldq %xmm13, %xmm2, %xmm1
vpunpckhqdq %ymm12, %ymm1, %ymm11
movlhps %xmm12, %xmm1
addps %xmm11, %xmm1
retq 

Initial state:
%ymm5: 0x0₁₂₈ ∘ %ymm1_haddps_xmm_xmm[127:0]

State for specgen instruction: vhaddps %xmm3, %xmm2, %xmm1:
%ymm1: ((0x0₁₂₈ ∘ (%ymm2_vhaddps_xmm_xmm_xmm[127:96] ∘ %ymm2_vhaddps_xmm_xmm_xmm[63:32] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (%ymm3_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm3_vhaddps_xmm_xmm_xmm[31:0] ∘ (%ymm2_vhaddps_xmm_xmm_xmm[95:64] ∘ %ymm2_vhaddps_xmm_xmm_xmm[31:0])))[255:128] ∘ (add_single(%ymm3_vhaddps_xmm_xmm_xmm[95:64], %ymm3_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm3_vhaddps_xmm_xmm_xmm[31:0], %ymm3_vhaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[95:64], %ymm2_vhaddps_xmm_xmm_xmm[127:96]) ∘ add_single(%ymm2_vhaddps_xmm_xmm_xmm[31:0], %ymm2_vhaddps_xmm_xmm_xmm[63:32]))

Final state
%ymm5: 0x0₁₂₈ ∘ (add_single(%ymm2_haddps_xmm_xmm[95:64], %ymm2_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm2_haddps_xmm_xmm[31:0], %ymm2_haddps_xmm_xmm[63:32]) ∘ add_single(%ymm1_haddps_xmm_xmm[95:64], %ymm1_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm1_haddps_xmm_xmm[31:0], %ymm1_haddps_xmm_xmm[63:32]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movups_xmm_xmm
%rdx/%rdx: %rdx_movups_xmm_xmm

%xmm0: %ymm0_movups_xmm_xmm[127:0]
%xmm1: %ymm1_movups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movups_xmm_xmm
%rdx/%rdx: %rdx_movups_xmm_xmm

%xmm0: %ymm0_movups_xmm_xmm[127:0]
%xmm1: (%ymm1_movups_xmm_xmm[255:128] ∘ ((%ymm7_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movups %xmm5, %xmm1

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: %ymm1_haddps_xmm_xmm[127:0]

State for specgen instruction: movups %xmm2, %xmm1:
%xmm1: (%ymm1_movups_xmm_xmm[255:128] ∘ ((%ymm7_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movups_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movups_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1_haddps_xmm_xmm[255:128] ∘ (add_single(%ymm2_haddps_xmm_xmm[95:64], %ymm2_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm2_haddps_xmm_xmm[31:0], %ymm2_haddps_xmm_xmm[63:32]) ∘ add_single(%ymm1_haddps_xmm_xmm[95:64], %ymm1_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm1_haddps_xmm_xmm[31:0], %ymm1_haddps_xmm_xmm[63:32])))[127:0]

=====================================
=====================================
Computing circuit for haddps %xmm9, %xmm11

.target:
vmovupd %xmm1, %xmm5
vhaddps %xmm2, %xmm5, %xmm5
movups %xmm5, %xmm1
retq 

Initial state:
%xmm11: (%ymm11_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[255:128])[127:0]

State for specgen instruction: haddps %xmm2, %xmm1:
%xmm1: (%ymm1_haddps_xmm_xmm[255:128] ∘ (add_single(%ymm2_haddps_xmm_xmm[95:64], %ymm2_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm2_haddps_xmm_xmm[31:0], %ymm2_haddps_xmm_xmm[63:32]) ∘ add_single(%ymm1_haddps_xmm_xmm[95:64], %ymm1_haddps_xmm_xmm[127:96]) ∘ add_single(%ymm1_haddps_xmm_xmm[31:0], %ymm1_haddps_xmm_xmm[63:32])))[127:0]

Final state
%xmm11: ((%ymm11_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[255:128])[255:128] ∘ (add_single(%ymm3_vhaddps_ymm_ymm_ymm[223:192], %ymm3_vhaddps_ymm_ymm_ymm[255:224]) ∘ add_single(%ymm3_vhaddps_ymm_ymm_ymm[159:128], %ymm3_vhaddps_ymm_ymm_ymm[191:160]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[223:192], %ymm2_vhaddps_ymm_ymm_ymm[255:224]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[159:128], %ymm2_vhaddps_ymm_ymm_ymm[191:160])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vhaddps_ymm_ymm_ymm
%rdx/%rdx: %rdx_vhaddps_ymm_ymm_ymm

%xmm0: %ymm0_vhaddps_ymm_ymm_ymm[127:0]
%xmm1: (((%ymm11_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[255:128])[255:128] ∘ (add_single(%ymm3_vhaddps_ymm_ymm_ymm[223:192], %ymm3_vhaddps_ymm_ymm_ymm[255:224]) ∘ add_single(%ymm3_vhaddps_ymm_ymm_ymm[159:128], %ymm3_vhaddps_ymm_ymm_ymm[191:160]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[223:192], %ymm2_vhaddps_ymm_ymm_ymm[255:224]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[159:128], %ymm2_vhaddps_ymm_ymm_ymm[191:160])))[127:0][127:0] ∘ ((%ymm10_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[127:0])[255:128] ∘ (add_single(%ymm3_vhaddps_ymm_ymm_ymm[95:64], %ymm3_vhaddps_ymm_ymm_ymm[127:96]) ∘ add_single(%ymm3_vhaddps_ymm_ymm_ymm[31:0], %ymm3_vhaddps_ymm_ymm_ymm[63:32]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[95:64], %ymm2_vhaddps_ymm_ymm_ymm[127:96]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[31:0], %ymm2_vhaddps_ymm_ymm_ymm[63:32])))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vhaddps %ymm2, %ymm2, %ymm2

.target:
callq .move_256_128_ymm2_xmm10_xmm11
haddps %xmm3, %xmm10
callq .move_256_128_ymm3_xmm8_xmm9
haddps %xmm9, %xmm11
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm2: 0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[31:24] ∘ 0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[23:16] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[15:8] ∘ (0x0₅₆ ∘ %ymm2_vpmovzxbd_xmm_xmm[7:0]))

State for specgen instruction: vhaddps %ymm3, %ymm2, %ymm1:
%ymm1: ((%ymm11_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[255:128])[255:128] ∘ (add_single(%ymm3_vhaddps_ymm_ymm_ymm[223:192], %ymm3_vhaddps_ymm_ymm_ymm[255:224]) ∘ add_single(%ymm3_vhaddps_ymm_ymm_ymm[159:128], %ymm3_vhaddps_ymm_ymm_ymm[191:160]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[223:192], %ymm2_vhaddps_ymm_ymm_ymm[255:224]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[159:128], %ymm2_vhaddps_ymm_ymm_ymm[191:160])))[127:0][127:0] ∘ ((%ymm10_vhaddps_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddps_ymm_ymm_ymm[127:0])[255:128] ∘ (add_single(%ymm3_vhaddps_ymm_ymm_ymm[95:64], %ymm3_vhaddps_ymm_ymm_ymm[127:96]) ∘ add_single(%ymm3_vhaddps_ymm_ymm_ymm[31:0], %ymm3_vhaddps_ymm_ymm_ymm[63:32]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[95:64], %ymm2_vhaddps_ymm_ymm_ymm[127:96]) ∘ add_single(%ymm2_vhaddps_ymm_ymm_ymm[31:0], %ymm2_vhaddps_ymm_ymm_ymm[63:32])))[127:0][127:0]

Final state
%ymm2: 0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[31:24] ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[23:16]) ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[31:24]) ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[23:16]) ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[15:8] ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[7:0]) ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[15:8]) ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[7:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vpmovzxbd_xmm_xmm
%rdx/%rdx: %rdx_vpmovzxbd_xmm_xmm

%xmm0: %ymm0_vpmovzxbd_xmm_xmm[127:0]
%xmm1: %ymm1_vpmovzxbd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm3, %xmm2, %xmm2

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm2: %ymm2_vmovlhps_xmm_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovlhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovlhps_xmm_xmm_xmm

%xmm0: %ymm0_vmovlhps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vmovlhps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovlhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovlhps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovlhps %xmm9, %xmm2, %xmm1

.target:
vpunpcklqdq %xmm3, %xmm2, %xmm2
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpmovzxbd_xmm_xmm

State for specgen instruction: vmovlhps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[31:24] ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[23:16]) ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[15:8] ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[7:0])))

=====================================
=====================================
Computing circuit for vpmovzxbd %xmm1, %xmm8

.target:
vpmovzxbq %xmm2, %ymm2
vhaddps %ymm2, %ymm2, %ymm2
callq .move_256_128_ymm2_xmm8_xmm9
vmovlhps %xmm9, %xmm2, %xmm1
retq 

Initial state:
%ymm8: %ymm8_vfmsubadd231ps_xmm_xmm_xmm

State for specgen instruction: vpmovzxbd %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[31:24] ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[23:16]) ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[15:8] ∘ (0x0₂₄ ∘ %ymm2_vpmovzxbd_xmm_xmm[7:0])))

Final state
%ymm8: 0x0₁₂₈ ∘ (0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24] ∘ (0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16]) ∘ (0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8] ∘ (0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm1, %xmm13

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm13: %ymm13_vfnmsub231ps_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm13: 0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm6

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm6: %ymm6_vfnmsub231ps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm6: 0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfnmsub231ps_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vfnmsub132ps %ymm1, %ymm13, %ymm6

Final state:
%ymm6: vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[255:224]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[223:192]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[191:160]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[159:128]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[127:96]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[95:64]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[63:32]) ∘ vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm6, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: 0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0]

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:96]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm1_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm3_vfnmsub231ps_xmm_xmm_xmm[95:64]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm1_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm3_vfnmsub231ps_xmm_xmm_xmm[63:32]) ∘ vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm1_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm3_vfnmsub231ps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vfnmsub231ps %xmm1, %xmm8, %xmm1

.target:
vmovupd %xmm1, %xmm13
vmovdqu %xmm2, %xmm6
vmovapd %xmm3, %xmm1
vfnmsub132ps %ymm1, %ymm13, %ymm6
vmovapd %xmm6, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmsubadd231ps_xmm_xmm_xmm

State for specgen instruction: vfnmsub231ps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:96]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm1_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm3_vfnmsub231ps_xmm_xmm_xmm[95:64]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm1_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm3_vfnmsub231ps_xmm_xmm_xmm[63:32]) ∘ vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm1_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm3_vfnmsub231ps_xmm_xmm_xmm[31:0]))))

Final state
%ymm1: 0x0₁₂₈ ∘ (vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96]) ∘ (vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64]) ∘ (vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32]) ∘ vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm3, %xmm2, %xmm1

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vorps_xmm_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vorps %xmm3, %xmm2, %xmm14

.target:
vorpd %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm14: %ymm14_vpor_xmm_xmm_xmm

State for specgen instruction: vorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

Final state
%ymm14: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm14, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpor_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vpor %xmm3, %xmm3, %xmm10

.target:
vorps %xmm3, %xmm2, %xmm14
vmovapd %xmm14, %xmm1
retq 

Initial state:
%ymm10: %ymm10_vfmaddsub213ps_xmm_xmm_xmm

State for specgen instruction: vpor %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

Final state
%ymm10: 0x0₁₂₈ ∘ %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vpxor_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpxor_xmm_xmm_xmm

%xmm0: %ymm0_vpxor_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpxor_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpxor_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r8_r9

Final state:
%rax/%rax: %rax_vpxor_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpxor_xmm_xmm_xmm

%xmm0: %ymm0_vpxor_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r8, %r12

Final state:
%r12/%r12: %ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0]

%cf: false
%pf: !((%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0]) = 0x0₆₄
%sf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r9, %r13

Final state:
%r13/%r13: %ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64]

%cf: false
%pf: !((%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64]) = 0x0₆₄
%sf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vpxor_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpxor_xmm_xmm_xmm

%xmm0: %ymm0_vpxor_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[255:128] ∘ ((%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[63:0] ∘ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpxor %xmm2, %xmm3, %xmm2

.target:
callq .move_128_064_xmm2_r12_r13
vmovdqa %xmm3, %xmm1
callq .move_128_064_xmm1_r8_r9
xorq %r8, %r12
xorq %r9, %r13
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm2: %ymm2_vxorpd_xmm_xmm_xmm

State for specgen instruction: vpxor %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[255:128] ∘ ((%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[63:0] ∘ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ ((%ymm3_vxorpd_xmm_xmm_xmm[127:64] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vxorpd_xmm_xmm_xmm[63:0] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vxorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorpd_xmm_xmm_xmm

%xmm0: %ymm0_vxorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vxorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ ((%ymm3_vxorpd_xmm_xmm_xmm[127:64] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vxorpd_xmm_xmm_xmm[63:0] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ ((%ymm3_vxorpd_xmm_xmm_xmm[127:64] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vxorpd_xmm_xmm_xmm[63:0] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vxorpd %xmm3, %xmm10, %xmm3

.target:
vpxor %xmm2, %xmm3, %xmm2
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm3: %ymm3_vfmaddsub213ps_xmm_xmm_xmm

State for specgen instruction: vxorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ ((%ymm3_vxorpd_xmm_xmm_xmm[127:64] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vxorpd_xmm_xmm_xmm[63:0] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[63:0])))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ ((%ymm3_vxorpd_xmm_xmm_xmm[127:64] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vxorpd_xmm_xmm_xmm[63:0] ⊕ %ymm2_vxorpd_xmm_xmm_xmm[63:0])))[127:0][63:0][63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄)

=====================================
-------------------------------------
Getting base circuit for vfnmsub132ps %ymm3, %ymm3, %ymm3

Final state:
%ymm3: vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32]) ∘ vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm10

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm10: %ymm10_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm10: 0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm2, %xmm11

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm11: %ymm11_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm11: 0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vaddps %ymm10, %ymm11, %ymm3

Final state:
%ymm3: add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[255:224]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[223:192]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[191:160]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[159:128]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[127:96]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[95:64]) ∘ (add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[63:32]) ∘ add_single((0x0₁₂₈ ∘ %ymm2_vaddps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vaddps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vaddps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[127:96], %ymm3_vaddps_xmm_xmm_xmm[127:96]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[95:64], %ymm3_vaddps_xmm_xmm_xmm[95:64]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[63:32], %ymm3_vaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vaddps_xmm_xmm_xmm[31:0], %ymm3_vaddps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vaddps %xmm2, %xmm1, %xmm3

.target:
vmovdqu %xmm3, %xmm10
vmovups %xmm2, %xmm11
vaddps %ymm10, %ymm11, %ymm3
vmovdqa %xmm3, %xmm1
retq 

Initial state:
%ymm3: %ymm3_addsubps_xmm_xmm

State for specgen instruction: vaddps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[127:96], %ymm3_vaddps_xmm_xmm_xmm[127:96]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[95:64], %ymm3_vaddps_xmm_xmm_xmm[95:64]) ∘ (add_single(%ymm2_vaddps_xmm_xmm_xmm[63:32], %ymm3_vaddps_xmm_xmm_xmm[63:32]) ∘ add_single(%ymm2_vaddps_xmm_xmm_xmm[31:0], %ymm3_vaddps_xmm_xmm_xmm[31:0]))))

Final state
%ymm3: 0x0₁₂₈ ∘ (add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm3_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_addsubps_xmm_xmm
%rdx/%rdx: %rdx_addsubps_xmm_xmm

%xmm0: %ymm0_addsubps_xmm_xmm[127:0]
%xmm1: %ymm1_addsubps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vminps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm14

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm14: %ymm14_vminps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm14: 0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vminps %ymm1, %ymm14, %ymm1

Final state:
%ymm1: (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32]) ∘ (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vminps %xmm2, %xmm2, %xmm5

.target:
vmovdqa %xmm3, %xmm1
vmovdqu %xmm2, %xmm14
vminps %ymm1, %ymm14, %ymm1
retq 

Initial state:
%ymm5: %ymm5_vsubps_xmm_xmm_xmm

State for specgen instruction: vminps %xmm3, %xmm2, %xmm1:
%ymm1: (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32]) ∘ (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm5: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm2_vsubps_xmm_xmm_xmm[127:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: %ymm0_vmovaps_xmm_xmm[127:0]
%xmm1: %ymm1_vmovaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovaps_xmm_xmm
%rdx/%rdx: %rdx_vmovaps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovaps %xmm2, %xmm10

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm10: %ymm10_subps_xmm_xmm

State for specgen instruction: vmovaps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovaps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovaps_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm10: 0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vminps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm14

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm14: %ymm14_vminps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm14: 0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vminps %ymm1, %ymm14, %ymm1

Final state:
%ymm1: (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32]) ∘ (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vminps %xmm1, %xmm1, %xmm4

.target:
vmovdqa %xmm3, %xmm1
vmovdqu %xmm2, %xmm14
vminps %ymm1, %ymm14, %ymm1
retq 

Initial state:
%ymm4: %ymm4_subps_xmm_xmm

State for specgen instruction: vminps %xmm3, %xmm2, %xmm1:
%ymm1: (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[255:224] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[255:224]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[223:192] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[223:192]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[191:160] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[191:160]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[159:128] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[159:128]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[127:96] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[127:96]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[95:64] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[95:64]) ∘ ((mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[63:32] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[63:32]) ∘ (mincmp_single((0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])[0:0] = 0x1₁ ? (0x0₁₂₈ ∘ %ymm2_vminps_xmm_xmm_xmm[127:0])[31:0] : (0x0₁₂₈ ∘ %ymm3_vminps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm4: 0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0])))

=====================================
-------------------------------------
Getting base circuit for vsubps %ymm10, %ymm4, %ymm2

Final state:
%ymm2: sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[255:224], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[255:224]) ∘ (sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[223:192], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[223:192]) ∘ (sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[191:160], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[191:160]) ∘ (sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[159:128], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[159:128]) ∘ (sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[127:96], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[127:96]) ∘ (sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[95:64], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[95:64]) ∘ (sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[63:32], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[63:32]) ∘ sub_single((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm1_subps_xmm_xmm[127:0]))))[31:0], (0x0₁₂₈ ∘ %ymm2_subps_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_movdqu_xmm_xmm
%rdx/%rdx: %rdx_movdqu_xmm_xmm

%xmm0: %ymm0_movdqu_xmm_xmm[127:0]
%xmm1: %ymm1_movdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_64_128_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_movdqu_xmm_xmm
%rdx/%rdx: %rdx_movdqu_xmm_xmm

%xmm0: %ymm0_movdqu_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqu_xmm_xmm[255:128] ∘ ((%ymm11_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ (%ymm10_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqu %xmm2, %xmm1

.target:
callq .move_128_64_xmm2_xmm10_xmm11
callq .move_64_128_xmm10_xmm11_xmm1
retq 

Initial state:
%xmm1: %ymm1_subps_xmm_xmm[127:0]

State for specgen instruction: movdqu %xmm2, %xmm1:
%xmm1: (%ymm1_movdqu_xmm_xmm[255:128] ∘ ((%ymm11_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][127:64]))[127:0][63:0] ∘ (%ymm10_movdqu_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ %ymm2_movdqu_xmm_xmm[127:0][63:0]))[127:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_subps_xmm_xmm[255:128] ∘ (sub_single(%ymm1_subps_xmm_xmm[127:96], %ymm2_subps_xmm_xmm[127:96]) ∘ (sub_single(%ymm1_subps_xmm_xmm[95:64], %ymm2_subps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_subps_xmm_xmm[63:32], %ymm2_subps_xmm_xmm[63:32]) ∘ sub_single(%ymm1_subps_xmm_xmm[31:0], %ymm2_subps_xmm_xmm[31:0])))))[127:0]

=====================================
=====================================
Computing circuit for subps %xmm3, %xmm5

.target:
vmovaps %xmm2, %xmm10
vminps %xmm1, %xmm1, %xmm4
vsubps %ymm10, %ymm4, %ymm2
movdqu %xmm2, %xmm1
retq 

Initial state:
%xmm5: (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm2_vsubps_xmm_xmm_xmm[127:0]))))[127:0]

State for specgen instruction: subps %xmm2, %xmm1:
%xmm1: (%ymm1_subps_xmm_xmm[255:128] ∘ (sub_single(%ymm1_subps_xmm_xmm[127:96], %ymm2_subps_xmm_xmm[127:96]) ∘ (sub_single(%ymm1_subps_xmm_xmm[95:64], %ymm2_subps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_subps_xmm_xmm[63:32], %ymm2_subps_xmm_xmm[63:32]) ∘ sub_single(%ymm1_subps_xmm_xmm[31:0], %ymm2_subps_xmm_xmm[31:0])))))[127:0]

Final state
%xmm5: ((0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ (0x0₃₂ ∘ %ymm2_vsubps_xmm_xmm_xmm[127:0]))))[255:128] ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[127:96], %ymm3_vsubps_xmm_xmm_xmm[127:96]) ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[95:64], %ymm3_vsubps_xmm_xmm_xmm[95:64]) ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[63:32], %ymm3_vsubps_xmm_xmm_xmm[63:32]) ∘ sub_single(%ymm2_vsubps_xmm_xmm_xmm[31:0], %ymm3_vsubps_xmm_xmm_xmm[31:0])))))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm5, %xmm1

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm1: %ymm1_vsubps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[127:96], %ymm3_vsubps_xmm_xmm_xmm[127:96]) ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[95:64], %ymm3_vsubps_xmm_xmm_xmm[95:64]) ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[63:32], %ymm3_vsubps_xmm_xmm_xmm[63:32]) ∘ sub_single(%ymm2_vsubps_xmm_xmm_xmm[31:0], %ymm3_vsubps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vsubps %xmm2, %xmm1, %xmm4

.target:
vminps %xmm2, %xmm2, %xmm5
subps %xmm3, %xmm5
vmovdqu %xmm5, %xmm1
retq 

Initial state:
%ymm4: %ymm4_addsubps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0])))))[127:0][31:0])

State for specgen instruction: vsubps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[127:96], %ymm3_vsubps_xmm_xmm_xmm[127:96]) ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[95:64], %ymm3_vsubps_xmm_xmm_xmm[95:64]) ∘ (sub_single(%ymm2_vsubps_xmm_xmm_xmm[63:32], %ymm3_vsubps_xmm_xmm_xmm[63:32]) ∘ sub_single(%ymm2_vsubps_xmm_xmm_xmm[31:0], %ymm3_vsubps_xmm_xmm_xmm[31:0]))))

Final state
%ymm4: 0x0₁₂₈ ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vunpckhpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm10_xmm11

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vunpckhpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm3, %xmm8

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm8: (%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm8: ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_64_xmm1_xmm12_xmm13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: %ymm1_unpckhpd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm1_r10d_r11d_r12d_r13d

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_064_r10d_r11d_rbx

Final state:
%rax/%rax: %rax_vmovq_r64_xmm
%rdx/%rdx: %rdx_vmovq_r64_xmm

%xmm0: %ymm0_vmovq_r64_xmm[127:0]
%xmm1: %ymm1_vmovq_r64_xmm[127:0]

-------------------------------------
=====================================
Computing circuit for vmovq %xmm1, %r10

.target:
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d
callq .move_032_064_r10d_r11d_rbx
retq 

Initial state:
%r10/%r10: %r10_movq_r64_xmm

State for specgen instruction: vmovq %xmm1, %rbx:
%rbx/%rbx: (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]

Register        -> %rbx
  translates to => %r10
Value is               -> (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][63:32])[31:0][31:0] ∘ (0x0₃₂ ∘ %ymm1_vmovq_r64_xmm[127:0][31:0])[31:0][31:0]
  after renaming it is => %ymm1_movq_r64_xmm[63:0]

Final state
%r10/%r10: %ymm1_movq_r64_xmm[63:0]

=====================================
-------------------------------------
Getting base circuit for movq %r10, %rbx

Final state:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

-------------------------------------
=====================================
Computing circuit for movq %xmm13, %r12

.target:
vmovq %xmm1, %r10
movq %r10, %rbx
retq 

Initial state:
%r12/%r12: %ymm2_unpckhpd_xmm_xmm[127:0][63:0]

State for specgen instruction: movq %xmm1, %rbx:
%rbx/%rbx: %ymm1_movq_r64_xmm[63:0]

Register        -> %rbx
  translates to => %r12
Value is               -> %ymm1_movq_r64_xmm[63:0]
  after renaming it is => %ymm1_unpckhpd_xmm_xmm[127:64]

Final state
%r12/%r12: %ymm1_unpckhpd_xmm_xmm[127:64]

=====================================
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_unpckhpd_xmm_xmm
%rdx/%rdx: %rdx_unpckhpd_xmm_xmm

%xmm0: %ymm0_unpckhpd_xmm_xmm[127:0]
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for unpckhpd %xmm11, %xmm9

.target:
callq .move_128_64_xmm1_xmm12_xmm13
callq .move_128_064_xmm2_r12_r13
movq %xmm13, %r12
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm9: (%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[127:0]

State for specgen instruction: unpckhpd %xmm2, %xmm1:
%xmm1: (%ymm1_unpckhpd_xmm_xmm[255:128] ∘ (%ymm2_unpckhpd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm1_unpckhpd_xmm_xmm[127:64][63:0]))[127:0]

Final state
%xmm9: ((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vunpckhpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vunpckhpd_ymm_ymm_ymm

%xmm0: %ymm0_vunpckhpd_ymm_ymm_ymm[127:0]
%xmm1: (((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0][127:0] ∘ ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vunpckhpd %ymm4, %ymm4, %ymm6

.target:
callq .move_256_128_ymm2_xmm8_xmm9
callq .move_256_128_ymm3_xmm10_xmm11
unpckhpd %xmm3, %xmm8
unpckhpd %xmm11, %xmm9
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm6: %ymm6_addsubps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0])))))[127:0][95:64])

State for specgen instruction: vunpckhpd %ymm3, %ymm2, %ymm1:
%ymm1: ((%ymm9_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:128])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[255:192] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[255:192]))[127:0][127:0] ∘ ((%ymm8_vunpckhpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:0])[255:128] ∘ (%ymm3_vunpckhpd_ymm_ymm_ymm[127:64] ∘ %ymm2_vunpckhpd_ymm_ymm_ymm[127:64]))[127:0][127:0]

Final state
%ymm6: 0x0₆₄ ∘ 0x0₆₄ ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm3_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_addsubps_xmm_xmm
%rdx/%rdx: %rdx_addsubps_xmm_xmm

%xmm0: %ymm0_addsubps_xmm_xmm[127:0]
%xmm1: %ymm1_addsubps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: %ymm0_vrcpps_xmm_xmm[127:0]
%xmm1: %ymm1_vrcpps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vrcpps %ymm1, %ymm10

Final state:
%ymm10: approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm10_xmm11_ymm1

Final state:
%rax/%rax: %rax_vrcpps_xmm_xmm
%rdx/%rdx: %rdx_vrcpps_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vrcpps %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
vrcpps %ymm1, %ymm10
callq .move_128_256_xmm10_xmm11_ymm1
retq 

Initial state:
%ymm1: %ymm1_vmovss_xmm_xmm_xmm

State for specgen instruction: vrcpps %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[127:0][127:0] ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[255:224]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[223:192]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[191:160]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[159:128]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[127:96]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[95:64]) ∘ (approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[63:32]) ∘ approx_reciprocal_single((0x0₂₅₆[255:128] ∘ (%ymm2_vrcpps_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vrcpps_xmm_xmm[127:0][63:0][63:0]))[31:0]))))))))[127:0][127:0]

Final state
%ymm1: 0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm3, %xmm8

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm8: %ymm8_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][31:0])

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm8: 0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1

Final state:
%rax/%rax: %rax_vmovss_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovss_xmm_xmm_xmm

%xmm0: %ymm0_vmovss_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovss %xmm11, %xmm10, %xmm7

.target:
vrcpps %xmm3, %xmm1
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11
vmovdqu %xmm3, %xmm8
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1
retq 

Initial state:
%ymm7: %ymm7_addsubps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0])))))[127:0][127:96])

State for specgen instruction: vmovss %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[127:96]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[95:64]) ∘ (approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[63:32]) ∘ approx_reciprocal_single(%ymm3_vmovss_xmm_xmm_xmm[31:0])))))[255:128] ∘ ((%ymm11_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm10_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm9_vmovss_xmm_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_vmovss_xmm_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ %ymm3_vmovss_xmm_xmm_xmm[127:0])[127:0][31:0])

Final state
%ymm7: 0x0₁₂₈ ∘ (0x0₉₆ ∘ add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]))

=====================================
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_addsubps_xmm_xmm
%rdx/%rdx: %rdx_addsubps_xmm_xmm

%xmm0: %ymm0_addsubps_xmm_xmm[127:0]
%xmm1: (%ymm1_addsubps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ (0x0₉₆ ∘ add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96])))[127:0][31:0] ∘ (0x0₆₄ ∘ 0x0₆₄ ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]))))[127:0][31:0] ∘ (%ymm5_addsubps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0])))))[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0])))))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for addsubps %xmm10, %xmm3

.target:
vaddps %xmm2, %xmm1, %xmm3
callq .move_128_032_xmm3_xmm4_xmm5_xmm6_xmm7
vsubps %xmm2, %xmm1, %xmm4
vunpckhpd %ymm4, %ymm4, %ymm6
callq .move_128_032_xmm3_xmm8_xmm9_xmm10_xmm11
vmovss %xmm11, %xmm10, %xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm3: (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32]) ∘ vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0]))))))))[127:0]

State for specgen instruction: addsubps %xmm2, %xmm1:
%xmm1: (%ymm1_addsubps_xmm_xmm[255:128] ∘ ((0x0₁₂₈ ∘ (0x0₉₆ ∘ add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96])))[127:0][31:0] ∘ (0x0₆₄ ∘ 0x0₆₄ ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]))))[127:0][31:0] ∘ (%ymm5_addsubps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ (0x0₁₂₈ ∘ (add_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (add_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ add_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0])))))[127:0][63:32]))[127:0][31:0] ∘ (0x0₁₂₈ ∘ (sub_single(%ymm1_addsubps_xmm_xmm[127:96], %ymm2_addsubps_xmm_xmm[127:96]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[95:64], %ymm2_addsubps_xmm_xmm[95:64]) ∘ (sub_single(%ymm1_addsubps_xmm_xmm[63:32], %ymm2_addsubps_xmm_xmm[63:32]) ∘ sub_single(%ymm1_addsubps_xmm_xmm[31:0], %ymm2_addsubps_xmm_xmm[31:0])))))[127:0][31:0]))[127:0]

Final state
%xmm3: ((vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[255:224]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[223:192]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[191:160]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[159:128]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[127:96]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[95:64]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[63:32]) ∘ vfnmsub132_single((0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0], (0x0₁₂₈ ∘ (0x0₆₄ ∘ 0x0₆₄))[31:0]))))))))[255:128] ∘ (add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]) ∘ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r12_r13

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r13, %r9

Final state:
%r9/%r9: %ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r12, %r8

Final state:
%r8/%r8: %ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vxorps %xmm2, %xmm1, %xmm6

.target:
callq .move_128_064_xmm3_r12_r13
callq .move_128_064_xmm2_r8_r9
vzeroall 
xorq %r13, %r9
xorq %r12, %r8
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm6: %ymm6_xorps_xmm_xmm

State for specgen instruction: vxorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm6: 0x0₁₂₈ ∘ ((%ymm1_xorps_xmm_xmm[127:64] ⊕ %ymm2_xorps_xmm_xmm[127:64]) ∘ (%ymm1_xorps_xmm_xmm[63:0] ⊕ %ymm2_xorps_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm6, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_xorps_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_xorps_xmm_xmm[255:128] ∘ ((%ymm1_xorps_xmm_xmm[127:64] ⊕ %ymm2_xorps_xmm_xmm[127:64]) ∘ (%ymm1_xorps_xmm_xmm[63:0] ⊕ %ymm2_xorps_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for xorps %xmm2, %xmm1

.target:
vxorps %xmm2, %xmm1, %xmm6
movdqa %xmm6, %xmm1
retq 

Initial state:
%xmm1: %ymm1_pxor_xmm_xmm[127:0]

State for specgen instruction: xorps %xmm2, %xmm1:
%xmm1: (%ymm1_xorps_xmm_xmm[255:128] ∘ ((%ymm1_xorps_xmm_xmm[127:64] ⊕ %ymm2_xorps_xmm_xmm[127:64]) ∘ (%ymm1_xorps_xmm_xmm[63:0] ⊕ %ymm2_xorps_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: (%ymm1_pxor_xmm_xmm[255:128] ∘ ((%ymm1_pxor_xmm_xmm[127:64] ⊕ %ymm2_pxor_xmm_xmm[127:64]) ∘ (%ymm1_pxor_xmm_xmm[63:0] ⊕ %ymm2_pxor_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for pxor %xmm9, %xmm9

.target:
xorps %xmm2, %xmm1
retq 

Initial state:
%xmm9: %ymm9_vfmadd213ps_xmm_xmm_xmm[127:0]

State for specgen instruction: pxor %xmm2, %xmm1:
%xmm1: (%ymm1_pxor_xmm_xmm[255:128] ∘ ((%ymm1_pxor_xmm_xmm[127:64] ⊕ %ymm2_pxor_xmm_xmm[127:64]) ∘ (%ymm1_pxor_xmm_xmm[63:0] ⊕ %ymm2_pxor_xmm_xmm[63:0])))[127:0]

Final state
%xmm9: (%ymm9_vfmadd213ps_xmm_xmm_xmm[255:128] ∘ (0x0₆₄ ∘ 0x0₆₄))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vandps_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vandps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vpxor_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpxor_xmm_xmm_xmm

%xmm0: %ymm0_vpxor_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpxor_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpxor_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm1_r8_r9

Final state:
%rax/%rax: %rax_vpxor_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpxor_xmm_xmm_xmm

%xmm0: %ymm0_vpxor_xmm_xmm_xmm[127:0]
%xmm1: (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r8, %r12

Final state:
%r12/%r12: %ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0]

%cf: false
%pf: !((%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0]) = 0x0₆₄
%sf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r9, %r13

Final state:
%r13/%r13: %ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64]

%cf: false
%pf: !((%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64]) = 0x0₆₄
%sf: (%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vpxor_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpxor_xmm_xmm_xmm

%xmm0: %ymm0_vpxor_xmm_xmm_xmm[127:0]
%xmm1: ((0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[255:128] ∘ ((%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[63:0] ∘ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpxor %xmm1, %xmm2, %xmm0

.target:
callq .move_128_064_xmm2_r12_r13
vmovdqa %xmm3, %xmm1
callq .move_128_064_xmm1_r8_r9
xorq %r8, %r12
xorq %r9, %r13
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm0: %ymm0_pand_xmm_xmm

State for specgen instruction: vpxor %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[255:128] ∘ ((%ymm2_vpxor_xmm_xmm_xmm[127:0][127:64] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][127:64])[63:0] ∘ (%ymm2_vpxor_xmm_xmm_xmm[127:0][63:0] ⊕ (0x0₁₂₈ ∘ %ymm3_vpxor_xmm_xmm_xmm[127:0])[127:0][63:0])[63:0])

Final state
%ymm0: 0x0₁₂₈ ∘ ((%ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64]) ∘ (%ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm2, %xmm2

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm2: %ymm2_por_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ %ymm2_por_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm1, %xmm2, %xmm3

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm3: %ymm3_por_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_por_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for por %xmm1, %xmm2

.target:
vmovapd %xmm2, %xmm2
vorpd %xmm1, %xmm2, %xmm3
movdqa %xmm3, %xmm1
retq 

Initial state:
%xmm2: %ymm2_pandn_xmm_xmm[127:0]

State for specgen instruction: por %xmm2, %xmm1:
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

Final state
%xmm2: (%ymm2_pandn_xmm_xmm[255:128] ∘ ((%ymm2_pandn_xmm_xmm[127:64] | %ymm1_pandn_xmm_xmm[127:64]) ∘ (%ymm2_pandn_xmm_xmm[63:0] | %ymm1_pandn_xmm_xmm[63:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r12_r13

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: %ymm0_vxorps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vxorps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r13, %r9

Final state:
%r9/%r9: %ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for xorq %r12, %r8

Final state:
%r8/%r8: %ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vxorps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vxorps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vxorps %xmm2, %xmm1, %xmm6

.target:
callq .move_128_064_xmm3_r12_r13
callq .move_128_064_xmm2_r8_r9
vzeroall 
xorq %r13, %r9
xorq %r12, %r8
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm6: %ymm6_xorps_xmm_xmm

State for specgen instruction: vxorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm2_vxorps_xmm_xmm_xmm[127:0][127:64] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm2_vxorps_xmm_xmm_xmm[127:0][63:0] ⊕ %ymm3_vxorps_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm6: 0x0₁₂₈ ∘ ((%ymm1_xorps_xmm_xmm[127:64] ⊕ %ymm2_xorps_xmm_xmm[127:64]) ∘ (%ymm1_xorps_xmm_xmm[63:0] ⊕ %ymm2_xorps_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm6, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_xorps_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_xorps_xmm_xmm[255:128] ∘ ((%ymm1_xorps_xmm_xmm[127:64] ⊕ %ymm2_xorps_xmm_xmm[127:64]) ∘ (%ymm1_xorps_xmm_xmm[63:0] ⊕ %ymm2_xorps_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for xorps %xmm2, %xmm1

.target:
vxorps %xmm2, %xmm1, %xmm6
movdqa %xmm6, %xmm1
retq 

Initial state:
%xmm1: %ymm1_pxor_xmm_xmm[127:0]

State for specgen instruction: xorps %xmm2, %xmm1:
%xmm1: (%ymm1_xorps_xmm_xmm[255:128] ∘ ((%ymm1_xorps_xmm_xmm[127:64] ⊕ %ymm2_xorps_xmm_xmm[127:64]) ∘ (%ymm1_xorps_xmm_xmm[63:0] ⊕ %ymm2_xorps_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: (%ymm1_pxor_xmm_xmm[255:128] ∘ ((%ymm1_pxor_xmm_xmm[127:64] ⊕ %ymm2_pxor_xmm_xmm[127:64]) ∘ (%ymm1_pxor_xmm_xmm[63:0] ⊕ %ymm2_pxor_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for pxor %xmm2, %xmm1

.target:
xorps %xmm2, %xmm1
retq 

Initial state:
%xmm1: %ymm1_pandn_xmm_xmm[127:0]

State for specgen instruction: pxor %xmm2, %xmm1:
%xmm1: (%ymm1_pxor_xmm_xmm[255:128] ∘ ((%ymm1_pxor_xmm_xmm[127:64] ⊕ %ymm2_pxor_xmm_xmm[127:64]) ∘ (%ymm1_pxor_xmm_xmm[63:0] ⊕ %ymm2_pxor_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: (%ymm1_pandn_xmm_xmm[255:128] ∘ ((%ymm1_pandn_xmm_xmm[127:64] ⊕ (%ymm2_pandn_xmm_xmm[127:64] | %ymm1_pandn_xmm_xmm[127:64])) ∘ (%ymm1_pandn_xmm_xmm[63:0] ⊕ (%ymm2_pandn_xmm_xmm[63:0] | %ymm1_pandn_xmm_xmm[63:0]))))[127:0]

=====================================
=====================================
Computing circuit for pandn %xmm2, %xmm0

.target:
por %xmm1, %xmm2
pxor %xmm2, %xmm1
retq 

Initial state:
%xmm0: (0x0₁₂₈ ∘ ((%ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64]) ∘ (%ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0])))[127:0]

State for specgen instruction: pandn %xmm2, %xmm1:
%xmm1: (%ymm1_pandn_xmm_xmm[255:128] ∘ ((%ymm1_pandn_xmm_xmm[127:64] ⊕ (%ymm2_pandn_xmm_xmm[127:64] | %ymm1_pandn_xmm_xmm[127:64])) ∘ (%ymm1_pandn_xmm_xmm[63:0] ⊕ (%ymm2_pandn_xmm_xmm[63:0] | %ymm1_pandn_xmm_xmm[63:0]))))[127:0]

Final state
%xmm0: ((0x0₁₂₈ ∘ ((%ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64]) ∘ (%ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0])))[255:128] ∘ ((%ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64] ⊕ (%ymm2_pand_xmm_xmm[127:64] | %ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64])) ∘ (%ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0] ⊕ (%ymm2_pand_xmm_xmm[63:0] | %ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0]))))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm0, %xmm2

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm2: %ymm2_pand_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ ((%ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64] ⊕ (%ymm2_pand_xmm_xmm[127:64] | %ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64])) ∘ (%ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0] ⊕ (%ymm2_pand_xmm_xmm[63:0] | %ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_pand_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_pand_xmm_xmm[255:128] ∘ ((%ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64] ⊕ (%ymm2_pand_xmm_xmm[127:64] | %ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64])) ∘ (%ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0] ⊕ (%ymm2_pand_xmm_xmm[63:0] | %ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0]))))[127:0]

=====================================
=====================================
Computing circuit for pand %xmm1, %xmm2

.target:
vpxor %xmm1, %xmm2, %xmm0
pandn %xmm2, %xmm0
vmovapd %xmm0, %xmm2
movdqa %xmm2, %xmm1
retq 

Initial state:
%xmm2: %ymm2_vandps_xmm_xmm_xmm[127:0]

State for specgen instruction: pand %xmm2, %xmm1:
%xmm1: (%ymm1_pand_xmm_xmm[255:128] ∘ ((%ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64] ⊕ (%ymm2_pand_xmm_xmm[127:64] | %ymm2_pand_xmm_xmm[127:64] ⊕ %ymm1_pand_xmm_xmm[127:64])) ∘ (%ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0] ⊕ (%ymm2_pand_xmm_xmm[63:0] | %ymm2_pand_xmm_xmm[63:0] ⊕ %ymm1_pand_xmm_xmm[63:0]))))[127:0]

Final state
%xmm2: (%ymm2_vandps_xmm_xmm_xmm[255:128] ∘ ((%ymm3_vandps_xmm_xmm_xmm[127:64] ⊕ %ymm2_vandps_xmm_xmm_xmm[127:64] ⊕ (%ymm3_vandps_xmm_xmm_xmm[127:64] | %ymm3_vandps_xmm_xmm_xmm[127:64] ⊕ %ymm2_vandps_xmm_xmm_xmm[127:64])) ∘ (%ymm3_vandps_xmm_xmm_xmm[63:0] ⊕ %ymm2_vandps_xmm_xmm_xmm[63:0] ⊕ (%ymm3_vandps_xmm_xmm_xmm[63:0] | %ymm3_vandps_xmm_xmm_xmm[63:0] ⊕ %ymm2_vandps_xmm_xmm_xmm[63:0]))))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: %ymm0_vmovups_xmm_xmm[127:0]
%xmm1: %ymm1_vmovups_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovups_xmm_xmm
%rdx/%rdx: %rdx_vmovups_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovups %xmm2, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: 0x0₁₂₈ ∘ %ymm3_vandps_xmm_xmm_xmm[127:0]

State for specgen instruction: vmovups %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovups_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovups_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vandps_xmm_xmm_xmm[127:64] ⊕ %ymm2_vandps_xmm_xmm_xmm[127:64] ⊕ (%ymm3_vandps_xmm_xmm_xmm[127:64] | %ymm3_vandps_xmm_xmm_xmm[127:64] ⊕ %ymm2_vandps_xmm_xmm_xmm[127:64])) ∘ (%ymm3_vandps_xmm_xmm_xmm[63:0] ⊕ %ymm2_vandps_xmm_xmm_xmm[63:0] ⊕ (%ymm3_vandps_xmm_xmm_xmm[63:0] | %ymm3_vandps_xmm_xmm_xmm[63:0] ⊕ %ymm2_vandps_xmm_xmm_xmm[63:0])))

=====================================
=====================================
Computing circuit for vandps %xmm9, %xmm3, %xmm10

.target:
vmovupd %xmm3, %xmm1
pand %xmm1, %xmm2
vmovups %xmm2, %xmm1
retq 

Initial state:
%ymm10: %ymm10_vfmadd213ps_xmm_xmm_xmm

State for specgen instruction: vandps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vandps_xmm_xmm_xmm[127:64] ⊕ %ymm2_vandps_xmm_xmm_xmm[127:64] ⊕ (%ymm3_vandps_xmm_xmm_xmm[127:64] | %ymm3_vandps_xmm_xmm_xmm[127:64] ⊕ %ymm2_vandps_xmm_xmm_xmm[127:64])) ∘ (%ymm3_vandps_xmm_xmm_xmm[63:0] ⊕ %ymm2_vandps_xmm_xmm_xmm[63:0] ⊕ (%ymm3_vandps_xmm_xmm_xmm[63:0] | %ymm3_vandps_xmm_xmm_xmm[63:0] ⊕ %ymm2_vandps_xmm_xmm_xmm[63:0])))

Final state
%ymm10: 0x0₁₂₈ ∘ ((0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:64] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:64])) ∘ (0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:0] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:0])))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm1, %xmm13

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm13: %ymm13_vfnmsub231ps_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm13: 0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: %ymm0_vmovdqu_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqu_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm3

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: 0x0₂₅₆[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_256_xmm12_xmm13_ymm1

Final state:
%rax/%rax: %rax_vmovdqu_xmm_xmm
%rdx/%rdx: %rdx_vmovdqu_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: ((0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqu %xmm2, %xmm6

.target:
callq .move_128_064_xmm2_r10_r11
vzeroall 
callq .move_064_128_r10_r11_xmm3
callq .move_256_128_ymm3_xmm12_xmm13
callq .move_128_256_xmm12_xmm13_ymm1
retq 

Initial state:
%ymm6: %ymm6_vfnmsub231ps_xmm_xmm_xmm

State for specgen instruction: vmovdqu %xmm2, %xmm1:
%ymm1: (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[255:128])[127:0][127:0] ∘ (0x0₂₅₆[255:128] ∘ (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqu_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqu_xmm_xmm[127:0][63:0][63:0]))[127:0])[127:0][127:0]

Final state
%ymm6: 0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfnmsub231ps_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vfnmsub132ps %ymm1, %ymm13, %ymm6

Final state:
%ymm6: vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[255:224]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[223:192]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[191:160]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[159:128]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[127:96]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[95:64]) ∘ (vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[63:32]) ∘ vfnmsub132_single((0x0₁₂₈ ∘ %ymm2_vfnmsub231ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm6, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: 0x0₁₂₈ ∘ %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:0]

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:96]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm1_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm3_vfnmsub231ps_xmm_xmm_xmm[95:64]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm1_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm3_vfnmsub231ps_xmm_xmm_xmm[63:32]) ∘ vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm1_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm3_vfnmsub231ps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vfnmsub231ps %xmm3, %xmm10, %xmm3

.target:
vmovupd %xmm1, %xmm13
vmovdqu %xmm2, %xmm6
vmovapd %xmm3, %xmm1
vfnmsub132ps %ymm1, %ymm13, %ymm6
vmovapd %xmm6, %xmm1
retq 

Initial state:
%ymm3: %ymm3_vfmadd213ps_xmm_xmm_xmm

State for specgen instruction: vfnmsub231ps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm1_vfnmsub231ps_xmm_xmm_xmm[127:96], %ymm3_vfnmsub231ps_xmm_xmm_xmm[127:96]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm1_vfnmsub231ps_xmm_xmm_xmm[95:64], %ymm3_vfnmsub231ps_xmm_xmm_xmm[95:64]) ∘ (vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm1_vfnmsub231ps_xmm_xmm_xmm[63:32], %ymm3_vfnmsub231ps_xmm_xmm_xmm[63:32]) ∘ vfnmsub132_single(%ymm2_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm1_vfnmsub231ps_xmm_xmm_xmm[31:0], %ymm3_vfnmsub231ps_xmm_xmm_xmm[31:0]))))

Final state
%ymm3: 0x0₁₂₈ ∘ (vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]), %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96], %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]) ∘ (vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]), %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64], %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]) ∘ (vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]), %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32], %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]) ∘ vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]), %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0], %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm1, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmsub132ps_xmm_xmm_xmm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm3, %xmm0

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm0: %ymm0_vfmsub132ps_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm0: 0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: %ymm0_vmovupd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovupd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovupd_xmm_xmm
%rdx/%rdx: %rdx_vmovupd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovupd %xmm2, %xmm4

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm4: %ymm4_vfmsub132ps_xmm_xmm_xmm

State for specgen instruction: vmovupd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovupd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovupd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm4: 0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for vfmsub132ps %ymm0, %ymm4, %ymm1

Final state:
%ymm1: vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[255:224]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[223:192]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[191:160]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[159:128]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[127:96]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[95:64]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[63:32]) ∘ vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[31:0])))))))

-------------------------------------
=====================================
Computing circuit for vfmsub132ps %xmm1, %xmm3, %xmm2

.target:
vmovdqa %xmm1, %xmm1
vmovupd %xmm3, %xmm0
vmovupd %xmm2, %xmm4
vfmsub132ps %ymm0, %ymm4, %ymm1
retq 

Initial state:
%ymm2: %ymm2_vfmadd213ps_xmm_xmm_xmm

State for specgen instruction: vfmsub132ps %xmm3, %xmm2, %xmm1:
%ymm1: vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[255:224], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[255:224]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[223:192], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[223:192]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[191:160], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[191:160]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[159:128], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[159:128]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[127:96], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[127:96]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[95:64], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[95:64]) ∘ (vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[63:32], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[63:32]) ∘ vfmsub132_single((0x0₁₂₈ ∘ %ymm1_vfmsub132ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm2_vfmsub132ps_xmm_xmm_xmm[127:0])[31:0], (0x0₁₂₈ ∘ %ymm3_vfmsub132ps_xmm_xmm_xmm[127:0])[31:0])))))))

Final state
%ymm2: vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ (vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]), %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96], %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]), %ymm1_vfmadd213ps_xmm_xmm_xmm[127:96]) ∘ (vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]), %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64], %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]), %ymm1_vfmadd213ps_xmm_xmm_xmm[95:64]) ∘ (vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]), %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32], %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]), %ymm1_vfmadd213ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]), %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0], %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]), %ymm1_vfmadd213ps_xmm_xmm_xmm[31:0])))))))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm8_xmm9

Final state:
%rax/%rax: %rax_vorpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vorpd_ymm_ymm_ymm

%xmm0: %ymm0_vorpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vorpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm2, %xmm2

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm2: %ymm2_por_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ %ymm2_por_xmm_xmm[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm1, %xmm2, %xmm3

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm3: %ymm3_por_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm3, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_por_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for por %xmm2, %xmm8

.target:
vmovapd %xmm2, %xmm2
vorpd %xmm1, %xmm2, %xmm3
movdqa %xmm3, %xmm1
retq 

Initial state:
%xmm8: (%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[127:0]

State for specgen instruction: por %xmm2, %xmm1:
%xmm1: (%ymm1_por_xmm_xmm[255:128] ∘ ((%ymm1_por_xmm_xmm[127:64] | %ymm2_por_xmm_xmm[127:64]) ∘ (%ymm1_por_xmm_xmm[63:0] | %ymm2_por_xmm_xmm[63:0])))[127:0]

Final state
%xmm8: ((%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[127:64] | %ymm2_vorpd_ymm_ymm_ymm[127:64]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[63:0] | %ymm2_vorpd_ymm_ymm_ymm[63:0])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm10_xmm11

Final state:
%rax/%rax: %rax_vorpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vorpd_ymm_ymm_ymm

%xmm0: %ymm0_vorpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vorpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm1, %xmm2, %xmm3

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm3: %ymm3_orps_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm3: 0x0₁₂₈ ∘ ((%ymm1_orps_xmm_xmm[127:64] | %ymm2_orps_xmm_xmm[127:64]) ∘ (%ymm1_orps_xmm_xmm[63:0] | %ymm2_orps_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm10_xmm11

Final state:
%rax/%rax: %rax_orps_xmm_xmm
%rdx/%rdx: %rdx_orps_xmm_xmm

%xmm0: %ymm0_orps_xmm_xmm[127:0]
%xmm1: %ymm1_orps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: %ymm1_movdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_movdqa_xmm_xmm
%rdx/%rdx: %rdx_movdqa_xmm_xmm

%xmm0: %ymm0_movdqa_xmm_xmm[127:0]
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movdqa %xmm10, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%xmm1: %ymm1_orps_xmm_xmm[127:0]

State for specgen instruction: movdqa %xmm2, %xmm1:
%xmm1: (%ymm1_movdqa_xmm_xmm[255:128] ∘ (%ymm2_movdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_orps_xmm_xmm[255:128] ∘ ((%ymm1_orps_xmm_xmm[127:64] | %ymm2_orps_xmm_xmm[127:64]) ∘ (%ymm1_orps_xmm_xmm[63:0] | %ymm2_orps_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for orps %xmm2, %xmm1

.target:
vorpd %xmm1, %xmm2, %xmm3
callq .move_256_128_ymm3_xmm10_xmm11
movdqa %xmm10, %xmm1
retq 

Initial state:
%xmm1: %ymm1_orpd_xmm_xmm[127:0]

State for specgen instruction: orps %xmm2, %xmm1:
%xmm1: (%ymm1_orps_xmm_xmm[255:128] ∘ ((%ymm1_orps_xmm_xmm[127:64] | %ymm2_orps_xmm_xmm[127:64]) ∘ (%ymm1_orps_xmm_xmm[63:0] | %ymm2_orps_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: (%ymm1_orpd_xmm_xmm[255:128] ∘ ((%ymm1_orpd_xmm_xmm[127:64] | %ymm2_orpd_xmm_xmm[127:64]) ∘ (%ymm1_orpd_xmm_xmm[63:0] | %ymm2_orpd_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for orpd %xmm11, %xmm9

.target:
orps %xmm2, %xmm1
retq 

Initial state:
%xmm9: (%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[127:0]

State for specgen instruction: orpd %xmm2, %xmm1:
%xmm1: (%ymm1_orpd_xmm_xmm[255:128] ∘ ((%ymm1_orpd_xmm_xmm[127:64] | %ymm2_orpd_xmm_xmm[127:64]) ∘ (%ymm1_orpd_xmm_xmm[63:0] | %ymm2_orpd_xmm_xmm[63:0])))[127:0]

Final state
%xmm9: ((%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[255:192] | %ymm2_vorpd_ymm_ymm_ymm[255:192]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[191:128] | %ymm2_vorpd_ymm_ymm_ymm[191:128])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vorpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vorpd_ymm_ymm_ymm

%xmm0: %ymm0_vorpd_ymm_ymm_ymm[127:0]
%xmm1: (((%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[255:192] | %ymm2_vorpd_ymm_ymm_ymm[255:192]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[191:128] | %ymm2_vorpd_ymm_ymm_ymm[191:128])))[127:0][127:0] ∘ ((%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[127:64] | %ymm2_vorpd_ymm_ymm_ymm[127:64]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[63:0] | %ymm2_vorpd_ymm_ymm_ymm[63:0])))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %ymm2, %ymm10, %ymm1

.target:
callq .move_256_128_ymm3_xmm8_xmm9
por %xmm2, %xmm8
callq .move_256_128_ymm2_xmm10_xmm11
orpd %xmm11, %xmm9
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm1: %ymm1_vfmadd213ps_xmm_xmm_xmm

State for specgen instruction: vorpd %ymm3, %ymm2, %ymm1:
%ymm1: ((%ymm9_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[255:128])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[255:192] | %ymm2_vorpd_ymm_ymm_ymm[255:192]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[191:128] | %ymm2_vorpd_ymm_ymm_ymm[191:128])))[127:0][127:0] ∘ ((%ymm8_vorpd_ymm_ymm_ymm[255:128] ∘ %ymm3_vorpd_ymm_ymm_ymm[127:0])[255:128] ∘ ((%ymm3_vorpd_ymm_ymm_ymm[127:64] | %ymm2_vorpd_ymm_ymm_ymm[127:64]) ∘ (%ymm3_vorpd_ymm_ymm_ymm[63:0] | %ymm2_vorpd_ymm_ymm_ymm[63:0])))[127:0][127:0]

Final state
%ymm1: (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ ((vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]), %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96], %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]), %ymm1_vfmadd213ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]), %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64], %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]), %ymm1_vfmadd213ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:64] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:64])) ∘ (vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]), %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32], %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]), %ymm1_vfmadd213ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]), %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0], %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]), %ymm1_vfmadd213ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:0] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:0])))

=====================================
=====================================
Computing circuit for vfmadd213ps %xmm3, %xmm2, %xmm1

.target:
pxor %xmm9, %xmm9
vandps %xmm9, %xmm3, %xmm10
vfnmsub231ps %xmm3, %xmm10, %xmm3
vfmsub132ps %xmm1, %xmm3, %xmm2
vorpd %ymm2, %ymm10, %ymm1
retq 

Initial state:
%ymm1: %ymm1_vfmaddsub213ps_xmm_xmm_xmm

State for specgen instruction: vfmadd213ps %xmm3, %xmm2, %xmm1:
%ymm1: (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ ((vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]), %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96], %ymm3_vfmadd213ps_xmm_xmm_xmm[127:96]), %ymm1_vfmadd213ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]), %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64], %ymm3_vfmadd213ps_xmm_xmm_xmm[95:64]), %ymm1_vfmadd213ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:64] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[127:64])) ∘ (vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]), %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32], %ymm3_vfmadd213ps_xmm_xmm_xmm[63:32]), %ymm1_vfmadd213ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmadd213ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]), %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0], %ymm3_vfmadd213ps_xmm_xmm_xmm[31:0]), %ymm1_vfmadd213ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:0] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm3_vfmadd213ps_xmm_xmm_xmm[63:0])))

Final state
%ymm1: (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ ((vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]))) ∘ (vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vfmaddsub213ps %xmm1, %xmm2, %xmm3

.target:
vpor %xmm3, %xmm3, %xmm10
vxorpd %xmm3, %xmm10, %xmm3
vfnmsub132ps %ymm3, %ymm3, %ymm3
addsubps %xmm10, %xmm3
vfmadd213ps %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm3: %ymm3_vfmaddsub231ps_xmm_xmm_xmm

State for specgen instruction: vfmaddsub213ps %xmm3, %xmm2, %xmm1:
%ymm1: (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ ((vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[95:64]))) ∘ (vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmaddsub213ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0])), %ymm1_vfmaddsub213ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm3_vfmaddsub213ps_xmm_xmm_xmm[31:0]))))

Final state
%ymm3: (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ (vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) ∘ vfmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂) | 0x0₆₄) ∘ ((vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]))) ∘ (vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: %ymm0_vorpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vorpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for orq %r9, %r11

Final state:
%r11/%r11: %ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for orq %r8, %r10

Final state:
%r10/%r10: %ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]

%cf: false
%pf: !((%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][0:0] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][1:1] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][2:2] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][3:3] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][4:4] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][5:5] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][6:6] = 0x1₁ ⊕ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[7:0][7:7] = 0x1₁)
%zf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0]) = 0x0₆₄
%sf: (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:63] = 0x1₁
%of: false

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vorpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vorpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vorpd %xmm3, %xmm2, %xmm1

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
orq %r9, %r11
orq %r8, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm1: %ymm1_vorps_xmm_xmm_xmm

State for specgen instruction: vorpd %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((%ymm3_vorpd_xmm_xmm_xmm[127:0][127:64] | %ymm2_vorpd_xmm_xmm_xmm[127:0][127:64])[63:0] ∘ (%ymm3_vorpd_xmm_xmm_xmm[127:0][63:0] | %ymm2_vorpd_xmm_xmm_xmm[127:0][63:0])[63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vorps %xmm3, %xmm2, %xmm14

.target:
vorpd %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm14: %ymm14_vpor_xmm_xmm_xmm

State for specgen instruction: vorps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vorps_xmm_xmm_xmm[127:64] | %ymm2_vorps_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vorps_xmm_xmm_xmm[63:0] | %ymm2_vorps_xmm_xmm_xmm[63:0]))

Final state
%ymm14: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: %ymm0_vmovapd_xmm_xmm[127:0]
%xmm1: %ymm1_vmovapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovapd_xmm_xmm
%rdx/%rdx: %rdx_vmovapd_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovapd %xmm14, %xmm1

.target:
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vpor_xmm_xmm_xmm

State for specgen instruction: vmovapd %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovapd_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

=====================================
=====================================
Computing circuit for vpor %xmm3, %xmm3, %xmm1

.target:
vorps %xmm3, %xmm2, %xmm14
vmovapd %xmm14, %xmm1
retq 

Initial state:
%ymm1: %ymm1_vfmaddsub231ps_xmm_xmm_xmm

State for specgen instruction: vpor %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((%ymm3_vpor_xmm_xmm_xmm[127:64] | %ymm2_vpor_xmm_xmm_xmm[127:64]) ∘ (%ymm3_vpor_xmm_xmm_xmm[63:0] | %ymm2_vpor_xmm_xmm_xmm[63:0]))

Final state
%ymm1: 0x0₁₂₈ ∘ ((vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]))) ∘ (vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]))))

=====================================
=====================================
Computing circuit for vfmaddsub231ps %xmm3, %xmm2, %xmm1

.target:
vfmaddsub213ps %xmm1, %xmm2, %xmm3
vpor %xmm3, %xmm3, %xmm1
retq 

Initial state:
%ymm1: 0x0₁₂₈ ∘ (vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96]) ∘ (vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64]) ∘ (vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32]) ∘ vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0]))))

State for specgen instruction: vfmaddsub231ps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[127:96]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[95:64]))) ∘ (vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmaddsub231ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0])), %ymm3_vfmaddsub231ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[63:32]) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), %ymm1_vfmaddsub231ps_xmm_xmm_xmm[31:0]))))

Final state
%ymm1: 0x0₁₂₈ ∘ ((vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])))) ∘ (vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])))))

=====================================
=====================================
Computing circuit for vfmsubadd231ps %xmm3, %xmm1, %xmm2

.target:
vpmovzxbd %xmm1, %xmm8
vfnmsub231ps %xmm1, %xmm8, %xmm1
vfmaddsub231ps %xmm3, %xmm2, %xmm1
retq 

Initial state:
%ymm2: %ymm2_vfmsubadd132ps_xmm_xmm_xmm

State for specgen instruction: vfmsubadd231ps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₁₂₈ ∘ ((vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:24], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[23:16], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[95:64])))) ∘ (vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm2_vfmsubadd231ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0]))), %ymm3_vfmsubadd231ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[15:8], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm1_vfmsubadd231ps_xmm_xmm_xmm[7:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0], %ymm1_vfmsubadd231ps_xmm_xmm_xmm[31:0])))))

Final state
%ymm2: 0x0₁₂₈ ∘ ((vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])))) ∘ (vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])))))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vfmsubadd132ps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfmsubadd132ps_xmm_xmm_xmm

%xmm0: %ymm0_vfmsubadd132ps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vfmsubadd132ps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vfmsubadd132ps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vfmsubadd132ps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ ((vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])))) ∘ (vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))))))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ ((vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])))) ∘ (vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))))))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vfmsubadd132ps %xmm3, %xmm2, %xmm1

.target:
vfmsubadd231ps %xmm3, %xmm1, %xmm2
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1

State for specgen instruction: vfmsubadd132ps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ ((vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])))) ∘ (vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))))))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ ((vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[127:96]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:24], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[23:16], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[95:64])))) ∘ (vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[63:32]) ∘ vfmsub132_single(%ymm1_vfmsubadd132ps_xmm_xmm_xmm[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))), %ymm3_vfmsubadd132ps_xmm_xmm_xmm[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[15:8], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2_vfmsubadd132ps_xmm_xmm_xmm[7:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0], %ymm2_vfmsubadd132ps_xmm_xmm_xmm[31:0]))))))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ ((vfmsub132_single(%ymm1[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96]))), %ymm3[127:96]) ∘ vfmsub132_single(%ymm1[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64]))), %ymm3[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])))) ∘ (vfmsub132_single(%ymm1[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32]))), %ymm3[63:32]) ∘ vfmsub132_single(%ymm1[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0]))), %ymm3[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])))))

=====================================
Circuits:

%ymm1  : 0x0₁₂₈ ∘ ((vfmsub132_single(%ymm1[127:96], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96]))), %ymm3[127:96]) ∘ vfmsub132_single(%ymm1[95:64], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64]))), %ymm3[95:64]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[31:24], %ymm2[127:96], %ymm2[127:96])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[23:16], %ymm2[95:64], %ymm2[95:64])))) ∘ (vfmsub132_single(%ymm1[63:32], vfnmsub132_single(0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32]))), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])), add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32]))), %ymm3[63:32]) ∘ vfmsub132_single(%ymm1[31:0], vfnmsub132_single(0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])) ⊕ (0x0₃₂ | 0x0₃₂ ⊕ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0]))), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])), sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0]))), %ymm3[31:0]) | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])) ⊕ (0x0₆₄ | 0x0₆₄ ⊕ add_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[15:8], %ymm2[63:32], %ymm2[63:32])) ∘ sub_single(vfnmsub132_single(0x0₃₂, 0x0₃₂, 0x0₃₂), vfnmsub132_single(0x0₂₄ ∘ %ymm2[7:0], %ymm2[31:0], %ymm2[31:0])))))

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv

*/