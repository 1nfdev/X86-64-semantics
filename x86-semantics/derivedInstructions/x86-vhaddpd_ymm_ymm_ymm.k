// Autogenerated using stratification.
requires "x86-configuration.k"

module VHADDPD-YMM-YMM-YMM
  imports X86-CONFIGURATION

  rule <k>
    execinstr (vhaddpd R1:Ymm, R2:Ymm, R3:Ymm,  .Typedoperands) => .
  ...</k>
    <regstate>
RSMap:Map => updateMap(RSMap,
convToRegKeys(R3) |-> (concatenateMInt(concatenateMInt(Float2MInt( ( MInt2Float(extractMInt(getParentValue(R1, RSMap), 0, 64), 53, 11)  +Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 64, 128), 53, 11) ) , 64), Float2MInt( ( MInt2Float(extractMInt(getParentValue(R2, RSMap), 0, 64), 53, 11)  +Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 64, 128), 53, 11) ) , 64)), concatenateMInt(Float2MInt( ( MInt2Float(extractMInt(getParentValue(R1, RSMap), 128, 192), 53, 11)  +Float  MInt2Float(extractMInt(getParentValue(R1, RSMap), 192, 256), 53, 11) ) , 64), Float2MInt( ( MInt2Float(extractMInt(getParentValue(R2, RSMap), 128, 192), 53, 11)  +Float  MInt2Float(extractMInt(getParentValue(R2, RSMap), 192, 256), 53, 11) ) , 64))) )


)

    </regstate>
endmodule

module VHADDPD-YMM-YMM-YMM-SEMANTICS
  imports VHADDPD-YMM-YMM-YMM
endmodule
/*
TargetInstr:
vhaddpd %ymm3, %ymm2, %ymm1
RWSet:
maybe read:{ %ymm2 %ymm3 }
must read:{ %ymm2 %ymm3 }
maybe write:{ %ymm1 }
must write:{ %ymm1 }
maybe undef:{ }
must undef:{ }
required flags:{ avx }

Circuit:
circuit:callq .move_256_128_ymm3_xmm12_xmm13  #  1     0     5      OPC=callq_label
circuit:vmovdqa %xmm13, %xmm0                 #  2     0x5   5      OPC=vmovdqa_xmm_xmm
circuit:callq .move_256_128_ymm2_xmm8_xmm9    #  3     0xa   5      OPC=callq_label
circuit:haddpd %xmm0, %xmm9                   #  4     0xf   5      OPC=haddpd_xmm_xmm
circuit:vhaddpd %xmm3, %xmm2, %xmm8           #  5     0x14  4      OPC=vhaddpd_xmm_xmm_xmm
circuit:callq .move_128_256_xmm8_xmm9_ymm1    #  6     0x18  5      OPC=callq_label
BVF:
WARNING: No live out values provided, assuming { }
WARNING: No def in values provided; assuming { %mxcsr::rc[0] }
Target

vhaddpd %ymm3, %ymm2, %ymm1

  maybe read:      { %ymm2 %ymm3 }
  must read:       { %ymm2 %ymm3 }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { avx }

-------------------------------------
Getting base circuit for callq .move_256_128_ymm3_xmm12_xmm13

Final state:
%rax/%rax: %rax_vhaddpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vhaddpd_ymm_ymm_ymm

%xmm0: %ymm0_vhaddpd_ymm_ymm_ymm[127:0]
%xmm1: %ymm1_vhaddpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: %ymm0_vmovdqa_xmm_xmm[127:0]
%xmm1: %ymm1_vmovdqa_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_vmovdqa_xmm_xmm
%rdx/%rdx: %rdx_vmovdqa_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovdqa %xmm13, %xmm0

.target:
callq .move_128_064_xmm2_r12_r13
vzeroall 
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%ymm0: %ymm0_vhaddpd_ymm_ymm_ymm

State for specgen instruction: vmovdqa %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm2_vmovdqa_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vmovdqa_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm0: 0x0₁₂₈ ∘ %ymm3_vhaddpd_ymm_ymm_ymm[255:128]

=====================================
-------------------------------------
Getting base circuit for callq .move_256_128_ymm2_xmm8_xmm9

Final state:
%rax/%rax: %rax_vhaddpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vhaddpd_ymm_ymm_ymm

%xmm0: (0x0₁₂₈ ∘ %ymm3_vhaddpd_ymm_ymm_ymm[255:128])[127:0]
%xmm1: %ymm1_vhaddpd_ymm_ymm_ymm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm3, %xmm2, %xmm2

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm2: %ymm2_vmovlhps_xmm_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovlhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovlhps_xmm_xmm_xmm

%xmm0: %ymm0_vmovlhps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vmovlhps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovlhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovlhps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovlhps %xmm3, %xmm2, %xmm1

.target:
vpunpcklqdq %xmm3, %xmm2, %xmm2
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vhaddpd_xmm_xmm_xmm

State for specgen instruction: vmovlhps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vpunpckhqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpckhqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpckhqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpckhqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vpunpckhqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpckhqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpckhqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpckhqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r9, %r10

Final state:
%r10/%r10: %ymm2_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpckhqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpckhqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpckhqdq %xmm3, %xmm2, %xmm9

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
movq %r9, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vhaddpd_xmm_xmm_xmm

State for specgen instruction: vpunpckhqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[127:64] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[127:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: %ymm0_vaddpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vaddpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: %ymm0_vaddpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vaddpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm3

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vaddpd %ymm3, %ymm1, %ymm1

Final state:
%ymm1: add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64]) ∘ add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0])))

-------------------------------------
=====================================
Computing circuit for vaddpd %xmm2, %xmm1, %xmm9

.target:
callq .move_128_064_xmm2_r10_r11
callq .move_128_064_xmm3_r8_r9
vzeroall 
callq .move_064_128_r10_r11_xmm1
callq .move_064_128_r8_r9_xmm3
vaddpd %ymm3, %ymm1, %ymm1
retq 

Initial state:
%ymm9: %ymm9_addpd_xmm_xmm

State for specgen instruction: vaddpd %xmm3, %xmm2, %xmm1:
%ymm1: add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64]) ∘ add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0])))

Final state
%ymm9: 0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0])))

=====================================
-------------------------------------
Getting base circuit for vminpd %ymm9, %ymm9, %ymm12

Final state:
%ymm12: (mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192]) ∘ ((mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128]) ∘ ((mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64]) ∘ (mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0])))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: %ymm1_movaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movaps %xmm12, %xmm1

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: %ymm1_addpd_xmm_xmm[127:0]

State for specgen instruction: movaps %xmm2, %xmm1:
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1_addpd_xmm_xmm[255:128] ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for addpd %xmm9, %xmm1

.target:
vaddpd %xmm2, %xmm1, %xmm9
vminpd %ymm9, %ymm9, %ymm12
movaps %xmm12, %xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0]))[127:0]

State for specgen instruction: addpd %xmm2, %xmm1:
%xmm1: (%ymm1_addpd_xmm_xmm[255:128] ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0]))[255:128] ∘ (add_double(%ymm3_vhaddpd_xmm_xmm_xmm[63:0], %ymm3_vhaddpd_xmm_xmm_xmm[127:64]) ∘ add_double(%ymm2_vhaddpd_xmm_xmm_xmm[63:0], %ymm2_vhaddpd_xmm_xmm_xmm[127:64])))[127:0]

=====================================
=====================================
Computing circuit for vhaddpd %xmm2, %xmm1, %xmm7

.target:
vmovlhps %xmm3, %xmm2, %xmm1
vpunpckhqdq %xmm3, %xmm2, %xmm9
addpd %xmm9, %xmm1
retq 

Initial state:
%ymm7: %ymm7_haddpd_xmm_xmm

State for specgen instruction: vhaddpd %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0]))[255:128] ∘ (add_double(%ymm3_vhaddpd_xmm_xmm_xmm[63:0], %ymm3_vhaddpd_xmm_xmm_xmm[127:64]) ∘ add_double(%ymm2_vhaddpd_xmm_xmm_xmm[63:0], %ymm2_vhaddpd_xmm_xmm_xmm[127:64]))

Final state
%ymm7: 0x0₁₂₈ ∘ (add_double(%ymm2_haddpd_xmm_xmm[63:0], %ymm2_haddpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_haddpd_xmm_xmm[63:0], %ymm1_haddpd_xmm_xmm[127:64]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r12_r13

Final state:
%rax/%rax: %rax_movapd_xmm_xmm
%rdx/%rdx: %rdx_movapd_xmm_xmm

%xmm0: %ymm0_movapd_xmm_xmm[127:0]
%xmm1: %ymm1_movapd_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r12_r13_xmm1

Final state:
%rax/%rax: %rax_movapd_xmm_xmm
%rdx/%rdx: %rdx_movapd_xmm_xmm

%xmm0: %ymm0_movapd_xmm_xmm[127:0]
%xmm1: (%ymm1_movapd_xmm_xmm[255:128] ∘ (%ymm2_movapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movapd %xmm7, %xmm1

.target:
callq .move_128_064_xmm2_r12_r13
callq .move_064_128_r12_r13_xmm1
retq 

Initial state:
%xmm1: %ymm1_haddpd_xmm_xmm[127:0]

State for specgen instruction: movapd %xmm2, %xmm1:
%xmm1: (%ymm1_movapd_xmm_xmm[255:128] ∘ (%ymm2_movapd_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_movapd_xmm_xmm[127:0][63:0][63:0]))[127:0]

Final state
%xmm1: (%ymm1_haddpd_xmm_xmm[255:128] ∘ (add_double(%ymm2_haddpd_xmm_xmm[63:0], %ymm2_haddpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_haddpd_xmm_xmm[63:0], %ymm1_haddpd_xmm_xmm[127:64])))[127:0]

=====================================
=====================================
Computing circuit for haddpd %xmm0, %xmm9

.target:
vhaddpd %xmm2, %xmm1, %xmm7
movapd %xmm7, %xmm1
retq 

Initial state:
%xmm9: (%ymm9_vhaddpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddpd_ymm_ymm_ymm[255:128])[127:0]

State for specgen instruction: haddpd %xmm2, %xmm1:
%xmm1: (%ymm1_haddpd_xmm_xmm[255:128] ∘ (add_double(%ymm2_haddpd_xmm_xmm[63:0], %ymm2_haddpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_haddpd_xmm_xmm[63:0], %ymm1_haddpd_xmm_xmm[127:64])))[127:0]

Final state
%xmm9: ((%ymm9_vhaddpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddpd_ymm_ymm_ymm[255:128])[255:128] ∘ (add_double(%ymm3_vhaddpd_ymm_ymm_ymm[191:128], %ymm3_vhaddpd_ymm_ymm_ymm[255:192]) ∘ add_double(%ymm2_vhaddpd_ymm_ymm_ymm[191:128], %ymm2_vhaddpd_ymm_ymm_ymm[255:192])))[127:0]

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpcklqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpcklqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r8, %r11

Final state:
%r11/%r11: %ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpcklqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpcklqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpcklqdq %xmm3, %xmm2, %xmm2

.target:
callq .move_128_064_xmm3_r8_r9
callq .move_128_064_xmm2_r10_r11
vzeroall 
movq %r8, %r11
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm2: %ymm2_vmovlhps_xmm_xmm_xmm

State for specgen instruction: vpunpcklqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0] ∘ %ymm2_vpunpcklqdq_xmm_xmm_xmm[127:0][63:0][63:0])

Final state
%ymm2: 0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vmovlhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovlhps_xmm_xmm_xmm

%xmm0: %ymm0_vmovlhps_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vmovlhps_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm1

Final state:
%rax/%rax: %rax_vmovlhps_xmm_xmm_xmm
%rdx/%rdx: %rdx_vmovlhps_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][63:0][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vmovlhps %xmm3, %xmm2, %xmm1

.target:
vpunpcklqdq %xmm3, %xmm2, %xmm2
callq .move_128_064_xmm2_r8_r9
vzeroall 
callq .move_064_128_r8_r9_xmm1
retq 

Initial state:
%ymm1: %ymm1_vhaddpd_xmm_xmm_xmm

State for specgen instruction: vmovlhps %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ ((0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][127:64][63:0] ∘ (0x0₁₂₈ ∘ (%ymm3_vmovlhps_xmm_xmm_xmm[63:0] ∘ %ymm2_vmovlhps_xmm_xmm_xmm[63:0]))[127:0][63:0][63:0])

Final state
%ymm1: 0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r10_r11

Final state:
%rax/%rax: %rax_vpunpckhqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpckhqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpckhqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpckhqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r8_r9

Final state:
%rax/%rax: %rax_vpunpckhqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpckhqdq_xmm_xmm_xmm

%xmm0: %ymm0_vpunpckhqdq_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vpunpckhqdq_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for movq %r9, %r10

Final state:
%r10/%r10: %ymm2_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vpunpckhqdq_xmm_xmm_xmm
%rdx/%rdx: %rdx_vpunpckhqdq_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm3_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for vpunpckhqdq %xmm3, %xmm2, %xmm9

.target:
callq .move_128_064_xmm3_r10_r11
callq .move_128_064_xmm2_r8_r9
vzeroall 
movq %r9, %r10
callq .move_064_128_r10_r11_xmm1
retq 

Initial state:
%ymm9: %ymm9_vhaddpd_xmm_xmm_xmm

State for specgen instruction: vpunpckhqdq %xmm3, %xmm2, %xmm1:
%ymm1: 0x0₂₅₆[255:128] ∘ (%ymm3_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vpunpckhqdq_xmm_xmm_xmm[127:0][127:64][63:0])

Final state
%ymm9: 0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[127:64] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[127:64])

=====================================
-------------------------------------
Getting base circuit for callq .move_128_064_xmm2_r10_r11

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: %ymm0_vaddpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vaddpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_064_xmm3_r8_r9

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: %ymm0_vaddpd_xmm_xmm_xmm[127:0]
%xmm1: %ymm1_vaddpd_xmm_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vzeroall 

Final state:
%ymm0: 0x0₂₅₆
%ymm1: 0x0₂₅₆
%ymm2: 0x0₂₅₆
%ymm3: 0x0₂₅₆
%ymm4: 0x0₂₅₆
%ymm5: 0x0₂₅₆
%ymm6: 0x0₂₅₆
%ymm7: 0x0₂₅₆
%ymm8: 0x0₂₅₆
%ymm9: 0x0₂₅₆
%ymm10: 0x0₂₅₆
%ymm11: 0x0₂₅₆
%ymm12: 0x0₂₅₆
%ymm13: 0x0₂₅₆
%ymm14: 0x0₂₅₆
%ymm15: 0x0₂₅₆

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r10_r11_xmm1

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_064_128_r8_r9_xmm3

Final state:
%rax/%rax: %rax_vaddpd_xmm_xmm_xmm
%rdx/%rdx: %rdx_vaddpd_xmm_xmm_xmm

%xmm0: 0x0₂₅₆[127:0]
%xmm1: (0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for vaddpd %ymm3, %ymm1, %ymm1

Final state:
%ymm1: add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64]) ∘ add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0])))

-------------------------------------
=====================================
Computing circuit for vaddpd %xmm2, %xmm1, %xmm9

.target:
callq .move_128_064_xmm2_r10_r11
callq .move_128_064_xmm3_r8_r9
vzeroall 
callq .move_064_128_r10_r11_xmm1
callq .move_064_128_r8_r9_xmm3
vaddpd %ymm3, %ymm1, %ymm1
retq 

Initial state:
%ymm9: %ymm9_addpd_xmm_xmm

State for specgen instruction: vaddpd %xmm3, %xmm2, %xmm1:
%ymm1: add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[255:192]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[191:128]) ∘ (add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[127:64]) ∘ add_double((0x0₂₅₆[255:128] ∘ (%ymm2_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm2_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0], (0x0₂₅₆[255:128] ∘ (%ymm3_vaddpd_xmm_xmm_xmm[127:0][127:64][63:0] ∘ %ymm3_vaddpd_xmm_xmm_xmm[127:0][63:0][63:0]))[63:0])))

Final state
%ymm9: 0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0])))

=====================================
-------------------------------------
Getting base circuit for vminpd %ymm9, %ymm9, %ymm12

Final state:
%ymm12: (mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[255:192]) ∘ ((mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[191:128]) ∘ ((mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[127:64]) ∘ (mincmp_double((0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0], (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0])[0:0] = 0x1₁ ? (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0] : (0x0₆₄ ∘ (0x0₆₄ ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0]))))[63:0])))

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: %ymm1_movaps_xmm_xmm[127:0]

-------------------------------------
-------------------------------------
Getting base circuit for callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1

Final state:
%rax/%rax: %rax_movaps_xmm_xmm
%rdx/%rdx: %rdx_movaps_xmm_xmm

%xmm0: %ymm0_movaps_xmm_xmm[127:0]
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

-------------------------------------
=====================================
Computing circuit for movaps %xmm12, %xmm1

.target:
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1
retq 

Initial state:
%xmm1: %ymm1_addpd_xmm_xmm[127:0]

State for specgen instruction: movaps %xmm2, %xmm1:
%xmm1: (%ymm1_movaps_xmm_xmm[255:128] ∘ ((%ymm7_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][127:96]))[127:0][31:0] ∘ (%ymm6_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][95:64]))[127:0][31:0] ∘ (%ymm5_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][63:32]))[127:0][31:0] ∘ (%ymm4_movaps_xmm_xmm[255:128] ∘ (0x0₉₆ ∘ %ymm2_movaps_xmm_xmm[127:0][31:0]))[127:0][31:0]))[127:0]

Final state
%xmm1: (%ymm1_addpd_xmm_xmm[255:128] ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0])))[127:0]

=====================================
=====================================
Computing circuit for addpd %xmm9, %xmm1

.target:
vaddpd %xmm2, %xmm1, %xmm9
vminpd %ymm9, %ymm9, %ymm12
movaps %xmm12, %xmm1
retq 

Initial state:
%xmm1: (0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0]))[127:0]

State for specgen instruction: addpd %xmm2, %xmm1:
%xmm1: (%ymm1_addpd_xmm_xmm[255:128] ∘ (add_double(%ymm1_addpd_xmm_xmm[127:64], %ymm2_addpd_xmm_xmm[127:64]) ∘ add_double(%ymm1_addpd_xmm_xmm[63:0], %ymm2_addpd_xmm_xmm[63:0])))[127:0]

Final state
%xmm1: ((0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0]))[255:128] ∘ (add_double(%ymm3_vhaddpd_xmm_xmm_xmm[63:0], %ymm3_vhaddpd_xmm_xmm_xmm[127:64]) ∘ add_double(%ymm2_vhaddpd_xmm_xmm_xmm[63:0], %ymm2_vhaddpd_xmm_xmm_xmm[127:64])))[127:0]

=====================================
=====================================
Computing circuit for vhaddpd %xmm3, %xmm2, %xmm8

.target:
vmovlhps %xmm3, %xmm2, %xmm1
vpunpckhqdq %xmm3, %xmm2, %xmm9
addpd %xmm9, %xmm1
retq 

Initial state:
%ymm8: %ymm8_vhaddpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddpd_ymm_ymm_ymm[127:0]

State for specgen instruction: vhaddpd %xmm3, %xmm2, %xmm1:
%ymm1: (0x0₁₂₈ ∘ (%ymm3_vhaddpd_xmm_xmm_xmm[63:0] ∘ %ymm2_vhaddpd_xmm_xmm_xmm[63:0]))[255:128] ∘ (add_double(%ymm3_vhaddpd_xmm_xmm_xmm[63:0], %ymm3_vhaddpd_xmm_xmm_xmm[127:64]) ∘ add_double(%ymm2_vhaddpd_xmm_xmm_xmm[63:0], %ymm2_vhaddpd_xmm_xmm_xmm[127:64]))

Final state
%ymm8: 0x0₁₂₈ ∘ (add_double(%ymm3_vhaddpd_ymm_ymm_ymm[63:0], %ymm3_vhaddpd_ymm_ymm_ymm[127:64]) ∘ add_double(%ymm2_vhaddpd_ymm_ymm_ymm[63:0], %ymm2_vhaddpd_ymm_ymm_ymm[127:64]))

=====================================
-------------------------------------
Getting base circuit for callq .move_128_256_xmm8_xmm9_ymm1

Final state:
%rax/%rax: %rax_vhaddpd_ymm_ymm_ymm
%rdx/%rdx: %rdx_vhaddpd_ymm_ymm_ymm

%xmm0: (0x0₁₂₈ ∘ %ymm3_vhaddpd_ymm_ymm_ymm[255:128])[127:0]
%xmm1: (((%ymm9_vhaddpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddpd_ymm_ymm_ymm[255:128])[255:128] ∘ (add_double(%ymm3_vhaddpd_ymm_ymm_ymm[191:128], %ymm3_vhaddpd_ymm_ymm_ymm[255:192]) ∘ add_double(%ymm2_vhaddpd_ymm_ymm_ymm[191:128], %ymm2_vhaddpd_ymm_ymm_ymm[255:192])))[127:0][127:0] ∘ (0x0₁₂₈ ∘ (add_double(%ymm3_vhaddpd_ymm_ymm_ymm[63:0], %ymm3_vhaddpd_ymm_ymm_ymm[127:64]) ∘ add_double(%ymm2_vhaddpd_ymm_ymm_ymm[63:0], %ymm2_vhaddpd_ymm_ymm_ymm[127:64])))[127:0][127:0])[127:0]

-------------------------------------
=====================================
Computing circuit for vhaddpd %ymm3, %ymm2, %ymm1

.target:
callq .move_256_128_ymm3_xmm12_xmm13
vmovdqa %xmm13, %xmm0
callq .move_256_128_ymm2_xmm8_xmm9
haddpd %xmm0, %xmm9
vhaddpd %xmm3, %xmm2, %xmm8
callq .move_128_256_xmm8_xmm9_ymm1
retq 

Initial state:
%ymm1: %ymm1

State for specgen instruction: vhaddpd %ymm3, %ymm2, %ymm1:
%ymm1: ((%ymm9_vhaddpd_ymm_ymm_ymm[255:128] ∘ %ymm2_vhaddpd_ymm_ymm_ymm[255:128])[255:128] ∘ (add_double(%ymm3_vhaddpd_ymm_ymm_ymm[191:128], %ymm3_vhaddpd_ymm_ymm_ymm[255:192]) ∘ add_double(%ymm2_vhaddpd_ymm_ymm_ymm[191:128], %ymm2_vhaddpd_ymm_ymm_ymm[255:192])))[127:0][127:0] ∘ (0x0₁₂₈ ∘ (add_double(%ymm3_vhaddpd_ymm_ymm_ymm[63:0], %ymm3_vhaddpd_ymm_ymm_ymm[127:64]) ∘ add_double(%ymm2_vhaddpd_ymm_ymm_ymm[63:0], %ymm2_vhaddpd_ymm_ymm_ymm[127:64])))[127:0][127:0]

Final state
%ymm1: add_double(%ymm3[191:128], %ymm3[255:192]) ∘ add_double(%ymm2[191:128], %ymm2[255:192]) ∘ (add_double(%ymm3[63:0], %ymm3[127:64]) ∘ add_double(%ymm2[63:0], %ymm2[127:64]))

=====================================
Circuits:

%ymm1  : add_double(%ymm3[191:128], %ymm3[255:192]) ∘ add_double(%ymm2[191:128], %ymm2[255:192]) ∘ (add_double(%ymm3[63:0], %ymm3[127:64]) ∘ add_double(%ymm2[63:0], %ymm2[127:64]))

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv

*/