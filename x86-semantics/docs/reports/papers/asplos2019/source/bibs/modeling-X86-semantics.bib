Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Goel,
author = {Goel, Shilpi},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goel - Unknown - Analysis of x86 Application and System Programs via Machine-Code Verification.pdf:pdf},
pages = {1--34},
title = {{Analysis of x86 Application and System Programs via Machine-Code Verification}}
}
@article{Lim2013,
abstract = {This article describes the design and implementation of a system, called TSL (for Transformer Specification Language), that provides a systematic solution to the problem of creating retargetable tools for analyzing machine code. TSL is a tool generator---that is, a metatool---that automatically creates different abstract interpreters for machine-code instruction sets. The most challenging technical issue that we faced in designing TSL was how to automate the generation of the set of abstract transformers for a given abstract interpretation of a given instruction set. From a description of the concrete operational semantics of an instruction set, together with the datatypes and operations that define an abstract domain, TSL automatically creates the set of abstract transformers for the instructions of the instruction set. TSL advances the state-of-the-art in program analysis because it provides two dimensions of parameterizability: (i) a given analysis component can be retargeted to different instruction sets; (ii) multiple analysis components can be created automatically from a single specification of the concrete operational semantics of the language to be analyzed. TSL is an abstract-transformer-generator generator. The article describes the principles behind TSL, and discusses how one uses TSL to develop different abstract interpreters.},
author = {Lim, Junghee and Reps, Thomas},
doi = {10.1145/2450136.2450139},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lim, Reps - 2013 - TSL a system for generating abstract interpreters and its application to machine-code analysis.pdf:pdf},
issn = {01640925},
journal = {ACM Transactions on Programming Languages and Systems},
number = {1},
pages = {1--59},
title = {{TSL: a system for generating abstract interpreters and its application to machine-code analysis}},
url = {http://dl.acm.org/citation.cfm?id=2450139{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2450136.2450139},
volume = {35},
year = {2013}
}
@article{Chlipala2008,
abstract = {We report on an experience using the Coq proof assistant to develop a program verification tool with a machine-checked proof of full correctness. The verifier is able to prove memory safety of x86 machine code programs compiled from code that uses algebraic datatypes. The tool's soundness theorem is expressed in terms of the bit-level semantics of x86 programs, so its correctness depends on very few assumptions. We take advantage of Coq's support for programming with dependent types and modules in the structure of the development. The approach is based on developing a library of reusable functors for transforming a verifier at one level of abstraction into a verifier at a lower level. Using this library, it is possible to prototype a verifier based on a new type system with a minimal amount of work, while obtaining a very strong soundness theorem about the final product.},
author = {Chlipala, Adam},
doi = {10.1017/S0956796808006904},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chlipala - 2008 - Modular development of certified program verifiers with a proof assistant.pdf:pdf},
isbn = {1595933093},
issn = {09567968},
journal = {Journal of Functional Programming},
keywords = {dent types,interactive proof assistants,programming with depen-,proof-carrying code},
number = {5-6},
pages = {599--647},
title = {{Modular development of certified program verifiers with a proof assistant}},
volume = {18},
year = {2008}
}
@article{Kennedy2013,
abstract = {We describe a Coq formalization of a subset of the x86 architecture. One emphasis of the model is brevity: using dependent types, type classes and notation we give the x86 semantics a makeover that counters its reputation for baroqueness. We model bits},
author = {Kennedy, Andrew and Benton, Nick and Jensen, Jonas B. and Dagand, Pierre-Evariste},
doi = {10.1145/2505879.2505897},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy et al. - 2013 - Coq the world's best macro assembler.pdf:pdf},
isbn = {9781450321549},
issn = {00220000},
journal = {Proceedings of the 15th Symposium on Principles and Practice of Declarative Programming - PPDP '13},
pages = {13--24},
title = {{Coq: the world's best macro assembler?}},
url = {http://dl.acm.org/citation.cfm?doid=2505879.2505897},
year = {2013}
}
@article{Hasabnis2015b,
author = {Hasabnis, Niranjan},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasabnis - 2015 - Automatic Synthesis of Instruction Set Semantics and Its Application to Assembly to IR Translation.pdf:pdf},
isbn = {9781339454894},
number = {August},
pages = {1--73},
title = {{Automatic Synthesis of Instruction Set Semantics and Its Application to Assembly to IR Translation}},
year = {2015}
}
@article{Churchill,
author = {Churchill, Berkeley and Bastien, J F and Aiken, Alex},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Churchill, Bastien, Aiken - Unknown - Sound Loop Superoptimization for Google Native Client.pdf:pdf},
isbn = {9781450344654},
title = {{Sound Loop Superoptimization for Google Native Client}}
}
@article{Kang2018,
abstract = {Production compilers such as GCC and LLVM are large com-plex software systems, for which achieving a high level of reliability is hard. Although testing is an effective method for finding bugs, it alone cannot guarantee a high level of reliability. To provide a higher level of reliability, many ap-proaches that examine compilers' internal logics have been proposed. However, none of them have been successfully applied to major optimizations of production compilers. This paper presents Crellvm: a verified credible compila-tion (or equivalently, verified translation validation) frame-work for LLVM, which can be used as a systematic way of providing a high level of reliability for major optimiza-tions in LLVM. Specifically, we augment an LLVM optimizer to generate translation results together with their correct-ness proofs, which can then be checked by a proof checker formally verified in Coq. As case studies, we applied our approach to two major optimizations of LLVM: register pro-motion (mem2reg) and global value numbering (gvn), having found four miscompilation bugs (two in each). This result is notable because, to the best of our knowledge, no previous systematic approaches including random testing have found any bugs in the mem2reg and gvn passes. Moreover, except for the two bugs we have reported, we found only one con-firmed miscompilation bug for mem2reg in the LLVM bug tracker history.},
author = {Kang, Jeehoon and Kim, Yoonseung and Song, Youngju and Lee, Juneyoung and Park, Sanghoon and Shin, Mark Dongyeon and Kim, Yonghyun and Cho, Sungkeun and Choi, Joonwon and Hur, Chung-Kil and Yi, Kwangkeun},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kang et al. - 2018 - Crellvm Verified Credible Compilation for LLVM.pdf:pdf},
title = {{Crellvm: Verified Credible Compilation for LLVM}},
year = {2018}
}
@article{Hasabnis2016a,
abstract = {Binary analysis and instrumentation form the basis of many tools and frameworks for software debugging, security hard-ening, and monitoring. Accurate modeling of instruction semantics is paramount in this regard, as errors can lead to program crashes, or worse, bypassing of security checks. Semantic modeling is a daunting task for modern proces-sors such as x86 and ARM that support over a thousand instructions, many of them with complex semantics. This paper describes a new approach to automate this semantic modeling task. Our approach leverages instruction semantics knowledge that is already encoded into today's production compilers such as GCC and LLVM. Such an approach can greatly reduce manual effort, and more importantly, avoid errors introduced by manual modeling. Furthermore, it is applicable to any of the numerous architectures already sup-ported by the compiler. In this paper, we develop a new symbolic execution technique to extract instruction semantics from a compiler's source code. Unlike previous applications of symbolic execution that were focused on identifying a single program path that violates a property, our approach addresses the all paths problem, extracting the entire in-put/output behavior of the code generator. We have applied it successfully to the 120K lines of C-code used in GCC's code generator to extract x86 instruction semantics. To demonstrate architecture-neutrality, we have also applied it to AVR, a processor used in the popular Arduino platform.},
author = {Hasabnis, Niranjan and Sekar, R.},
doi = {10.1145/2950290.2950335},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasabnis, Sekar - 2016 - Extracting instruction semantics via symbolic execution of code generators.pdf:pdf},
isbn = {9781450342186},
journal = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering - FSE 2016},
keywords = {code generators,instruction-set semantics extraction,sym-},
pages = {301--313},
title = {{Extracting instruction semantics via symbolic execution of code generators}},
url = {http://dl.acm.org/citation.cfm?doid=2950290.2950335},
year = {2016}
}
@article{Bansal2006,
abstract = {Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all oppor- tunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the po- tential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peep- hole optimizers.We show experimentally that our optimizer is able to exploit performance opportunities not found by existing com- pilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.},
author = {Bansal, Sorav and Aiken, Alex},
doi = {10.1145/1168919.1168906},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Aiken - 2006 - Automatic generation of peephole superoptimizers.pdf:pdf},
isbn = {1595934510},
issn = {01635964},
journal = {ASPLOS 2006},
number = {5},
pages = {394},
title = {{Automatic generation of peephole superoptimizers}},
url = {http://portal.acm.org/citation.cfm?doid=1168919.1168906},
volume = {34},
year = {2006}
}
@article{Leroy2009,
abstract = {Purpose: We examined HIV testing behavior and its predictors among adolescents considered at high risk for HIV. Methods: Self-reports of HIV testing, knowledge, attitudes, and high-risk acts were examined among 272 adolescents aged 13-23 years (M = 18.7; SD = 2.3) attending community-based agencies that serve youth at high risk for HIV in Los Angeles, New York City, and San Francisco. Results: Evidence of adolescents' risk for HIV is reflected in a rate of 4.8{\%} seropositivity, 24{\%} injecting drug use, a mean of 4.3 (SD = 11.6) sexual partners during the previous 3 months, and 71{\%} condom use during vaginal/anal sex. HIV testing was common (63{\%}) and often repeated (M = 3.6, SD = 4.0). Knowledge of the meaning and consequences of testing was high (84{\%} correct). Contrary to service providers' expectations, youth were likely to return for their test results (90{\%} returned). Youth who were older, labeled themselves gay or bisexual, lived in Los Angeles or San Francisco, and those who injected drugs were significantly more likely, compared to peers, to get tested for HIV. Conclusions: These results suggest a need for more detailed observational studies of HIV testing behavior that include evaluation of characteristics of the youth, the testing site, and the attitudes and beliefs of providers offering HIV testing. (C) Society for Adolescent Medicine, 1997.},
author = {Leroy, Xavier},
doi = {10.1145/1538788.1538814},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leroy - 2009 - Formal verification of a realistic compiler.pdf:pdf},
isbn = {0092070302238},
issn = {00010782},
journal = {Acm},
number = {7},
pages = {107},
title = {{Formal verification of a realistic compiler}},
volume = {52},
year = {2009}
}
@article{EricSchulteJasonRuchtiMattNoonanDavidCiarletta2018,
abstract = {—In this paper, we propose ABC, a real-time smart-phone Authentication protocol utilizing the photo-response non-uniformity (PRNU) of the Built-in Camera. In contrast to previous works that require tens of images to build reliable PRNU features for conventional cameras, we are the first to observe that one image alone can uniquely identify a smartphone due to the unique PRNU of a smartphone image sensor. This new discovery makes the use of PRNU practical for smartphone authentication. While most existing hardware fingerprints are vulnerable against forgery attacks, ABC defeats forgery attacks by verifying a smartphone's PRNU identity through a challenge response protocol using a visible light communication channel. A user captures two time-variant QR codes and sends the two images to a server, which verifies the identity by fingerprint and image content matching. The time-variant QR codes can also defeat replay attacks. Our experiments with 16,000 images over 40 smartphones show that ABC can efficiently authenticate user devices with an error rate less than 0.5{\%}.},
author = {{Eric Schulte, Jason Ruchti, Matt Noonan, David Ciarletta}, Alexey Loginov},
doi = {10.14722/ndss.2018.23xxx},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eric Schulte, Jason Ruchti, Matt Noonan, David Ciarletta - 2018 - Evolving Exact Decompilation.pdf:pdf},
journal = {Ndss},
number = {February},
title = {{Evolving Exact Decompilation}},
url = {https://www.cs.unm.edu/{~}eschulte/data/bed.pdf{\%}0Ahttp://www.buffalo.edu/content/dam/www/news/photos/2017/12/ndss18-paper99.pdf},
year = {2018}
}
@article{Goel2017,
author = {Goel, Shilpi},
doi = {10.4204/EPTCS.249.1},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goel - 2017 - The x86isa Books Features, Usage, and Future Plans.pdf:pdf},
issn = {2075-2180},
journal = {Electronic Proceedings in Theoretical Computer Science},
pages = {1--17},
title = {{The x86isa Books: Features, Usage, and Future Plans}},
url = {http://arxiv.org/abs/1705.01225v1},
volume = {249},
year = {2017}
}
@article{Kim2017,
abstract = {—Binary lifting, which is to translate a binary exe-cutable to a high-level intermediate representation, is a primary step in binary analysis. Despite its importance, there are only few existing approaches to testing the correctness of binary lifters. Furthermore, the existing approaches suffer from low test cover-age, because they largely depend on random test case generation. In this paper, we present the design and implementation of the first systematic approach to testing binary lifters. We have evaluated the proposed system on 3 state-of-the-art binary lifters, and found 24 previously unknown semantic bugs. Our result demonstrates that writing a precise binary lifter is extremely difficult even for those heavily tested projects.},
author = {Kim, Soomin and Faerevaag, Markus and Jung, Minkyu and Jung, SeungIl and Oh, DongYeop and Lee, JongHyup and {Kil Cha}, Sang},
doi = {10.1109/ASE.2017.8115648},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2017 - Testing Intermediate Representations for Binary Analysis.pdf:pdf},
isbn = {9781538626849},
journal = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {353--364},
title = {{Testing Intermediate Representations for Binary Analysis}},
url = {https://dl.acm.org/citation.cfm?id=3155609{\%}0Ahttps://softsec.kaist.ac.kr/{~}soomink/paper/ase17main-mainp491-p.pdf},
year = {2017}
}
@article{Wang,
abstract = {—Software instrumentation techniques are widely used in program analysis tasks such as program profiling, vulnerability discovering, and security-oriented transforming. In this paper, we present an instrumentation tool called UROBOROS, which supports static instrumentation on stripped binaries. Due to the lack of relocation and debug information, reverse engineering of stripped binaries is challenging. Compared with the previous work, UROBOROS can provide complete, easy-to-use, transparent, and efficient static instrumentation on stripped binaries. UROBOROS supports complete instrumentation by stat-ically recovering the relocatable program (including both code and data sections) and the control flow structures from binary code. UROBOROS provides a rich API to access and manipulate different levels of the program structure. The instrumentation facilities of UROBOROS are easy-to-use, users with no binary rewriting and patching skills can directly manipulate stripped binaries to perform smooth program transformations. Distin-guished from most instrumentation tools that need to patch the instrumentation code as new sections, UROBOROS can directly inline the instrumentation code into the disassembled program, which provides transparent instrumentation on stripped binaries. For efficiency, in the rewritten output of existing tools, frequent control transfers between the attached and original sections can incur a considerable performance penalty. However, the output from UROBOROS incurs no extra cost because the original and in-strumentation code are connected by " fall-through " transfers. We perform comparative evaluations between UROBOROS and the state-of-the-art binary instrumentation tools, including DynInst and Pin. To demonstrate the versatility of UROBOROS, we also implement two real-world reengineering tasks which could be challenging for other instrumentation tools to accomplish. Our experimental results show that UROBOROS outperforms the existing binary instrumentation tools with better performance, lower labor cost, and a broader scope of applications.},
author = {Wang, Shuai and Wang, Pei and Wu, Dinghao},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Wang, Wu - Unknown - UROBOROS Instrumenting Stripped Binaries with Static Reassembling(2).pdf:pdf},
title = {{UROBOROS: Instrumenting Stripped Binaries with Static Reassembling}},
url = {https://faculty.ist.psu.edu/wu/papers/uroboros-api.pdf}
}
@article{Sepp2012,
abstract = {The analysis of executable code requires the reconstruction of instructions from a sequence of bytes (or words) and a specification of their semantics. Most front-ends addressing this problem only support a single architecture, are bound to a specific programming language, or are hard to maintain. In this work, we present a domain specific language (DSL) called GDSL (Generic Decoder Specification Language) for specifying maintainable instruction decoders and the translation of instructions to a semantics. We motivate its design by illustrating its use for the Intel x86 platform. A compiler is presented that generates C code that rivals hand-crafted decoder implementations.},
author = {Sepp, Alexander and Kranz, Julian and Simon, Axel},
doi = {10.1016/j.entcs.2012.11.006},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sepp, Kranz, Simon - 2012 - GDSL A Generic Decoder Specification Language for Interpreting Machine Language.pdf:pdf},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {binary analysis,executable analysis,instruction decoder,program semantics},
pages = {53--64},
title = {{GDSL: A Generic Decoder Specification Language for Interpreting Machine Language}},
url = {https://ac.els-cdn.com/S157106611200076X/1-s2.0-S157106611200076X-main.pdf?{\_}tid=b53f79e7-bbca-4bab-87cf-3257df5dce83{\&}acdnat=1525120242{\_}54967b3656aa17ff2c7925ecd3550d25},
volume = {289},
year = {2012}
}
@article{Paleari2010a,
abstract = {The output of a disassembler is used for many different purposes (e.g., debugging and reverse engineering). Therefore, disassemblers represent the first link of a long chain of stages on which any high-level analysis of machine code depends upon. In this paper we demonstrate that many disassemblers fail to decode certain instructions and thus that the first link of the chain is very weak. We present a methodology, called N-version disassembly, to verify the correctness of disassemblers, based on differential analysis. Given a set of n - 1 disassemblers, we use them to decode fragments of machine code and we compare their output against each other. To further corroborate the output of these disassemblers, we developed a special instruction decoder, the nth, that delegates the decoding to the CPU, the ideal decoder. We tested eight of the most popular disassemblers for Intel x86, and found bugs in each of them.},
author = {Paleari, Roberto and Martignoni, Lorenzo and {Fresi Roglia}, Giampaolo and Bruschi, Danilo},
doi = {10.1145/1831708.1831741},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paleari et al. - 2010 - N-version Disassembly Differential Testing of x86 Disassemblers(2).pdf:pdf},
isbn = {9781605588230},
journal = {Proceedings of the 19th international symposium on Software testing and analysis - ISSTA '10},
keywords = {automatic test generation,differential testing,software testing},
pages = {265},
title = {{N-version Disassembly: Differential Testing of x86 Disassemblers}},
url = {http://portal.acm.org/citation.cfm?doid=1831708.1831741},
year = {2010}
}
@article{Yee2009,
abstract = {This paper describes the design, implementation and evaluation of Native Client, a sandbox for untrusted x86 native code. Native Client aims to give browser-based applications the computational performance of native applications without compromising safety. Native Client uses software fault isolation and a secure runtime to direct system interaction and side effects through interfaces managed by Native Client. Native Client provides operating system portability for binary code while supporting performance-oriented features generally absent from Web application programming environments, such as thread support, instruction set extensions such as SSE, and use of compiler intrinsics and hand-coded assembler. We combine these properties in an open architecture that encourages community review and 3rd-party tools.},
author = {Yee, Bennet and Sehr, David and Dardyk, Gregory and Chen, J. Bradley and Muth, Robert and Ormandy, Tavis and Okasaka, Shiki and Narula, Neha and Fullagar, Nicholas},
doi = {10.1109/SP.2009.25},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yee et al. - 2009 - Native client A sandbox for portable, untrusted x86 native code.pdf:pdf},
isbn = {9780769536330},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
keywords = {NaCl},
mendeley-tags = {NaCl},
pages = {79--93},
title = {{Native client: A sandbox for portable, untrusted x86 native code}},
year = {2009}
}
@article{Reid2017a,
abstract = {Software and hardware are increasingly being formally verified against specifications, but how can we verify the specifications themselves? This paper explores what it means to formally verify a specification. We solve three challenges: (1) How to create a secondary, higher-level specification that can be effectively reviewed by processor designers who are not experts in formal verification; (2) How to avoid common-mode failures between the specifications; and (3) How to automatically verify the two specifications against each other. One of the most important specifications for software verification is the processor specification since it defines the behaviour of machine code and of hardware protection features used by operating systems. We demonstrate our approach on ARM's v8-M Processor Specification, which is intended to improve the security of Internet of Things devices. Thus, we focus on establishing the security guarantees the architecture is intended to provide. Despite the fact that the ARM v8-M specification had previously been extensively tested, we found twelve bugs (including two security bugs) that have all been fixed by ARM.},
author = {Reid, Alastair},
doi = {10.1145/3133912},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reid - 2017 - Who guards the guards formal validation of the Arm v8-m architecture specification.pdf:pdf},
issn = {24751421},
journal = {OOPSLA},
keywords = {Formal Verification,ISA,Specification},
number = {OOPSLA},
pages = {1--24},
title = {{Who guards the guards? formal validation of the Arm v8-m architecture specification}},
url = {https://doi.org/10.1145/3133912{\%}0Ahttp://dl.acm.org/citation.cfm?doid=3152284.3133912},
volume = {1},
year = {2017}
}
@article{Morrisett2012,
abstract = {Software-based fault isolation (SFI), as used in Google's Native Client (NaCl), relies upon a conceptually simple machine-code analysis to enforce a security policy. But for complicated architectures such as the x86, it is all too easy to get the details of the analysis wrong. We have built a new checker that is smaller, faster, and has a much reduced trusted computing base when compared to Google's original analysis. The key to our approach is automatically generating the bulk of the analysis from a declarative description which we relate to a formal model of a subset of the x86 instruction set architecture. The x86 model, developed in Coq, is of independent interest and should be usable for a wide range of machine-level verification tasks.},
author = {Morrisett, Greg and Tan, Gang and Tassarotti, Joseph and Tristan, Jean-Baptiste and Gan, Edward},
doi = {10.1145/2254064.2254111},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morrisett et al. - 2012 - RockSalt better, faster, stronger SFI for the x86.pdf:pdf},
isbn = {9781450312059},
issn = {0362-1340},
journal = {PLDI: Programming Languages Design and Implementation},
keywords = {2,4,d,domain-speci fi c languages,security,software engineer-,software fault isolation,veri fi cation},
pages = {395--404},
title = {{RockSalt: better, faster, stronger SFI for the x86}},
url = {http://doi.acm.org/10.1145/2254064.2254111{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2254111{\&}type=pdf},
year = {2012}
}
@inproceedings{Sarkar2008,
address = {New York, New York, USA},
author = {Sarkar, Susmit and Sewell, Peter and Nardelli, Francesco Zappa and Owens, Scott and Ridge, Tom and Braibant, Thomas and Myreen, Magnus O. and Alglave, Jade and Sarkar, Susmit and Sewell, Peter and Nardelli, Francesco Zappa and Owens, Scott and Ridge, Tom and Braibant, Thomas and Myreen, Magnus O. and Alglave, Jade},
booktitle = {Proceedings of the 36th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages - POPL '09},
doi = {10.1145/1480881.1480929},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarkar et al. - 2008 - The semantics of x86-CC multiprocessor machine code(2).pdf:pdf},
isbn = {9781605583792},
issn = {0362-1340},
keywords = {relaxed memory models,semantics},
number = {1},
pages = {379},
publisher = {ACM Press},
title = {{The semantics of x86-CC multiprocessor machine code}},
url = {http://portal.acm.org/citation.cfm?doid=1480881.1480929},
volume = {44},
year = {2008}
}
@article{Zhang2014,
abstract = {Program instrumentation techniques form the basis of many recent software security defenses, including defenses against common exploits and security policy enforcement. As com-pared to source-code instrumentation, binary instrumenta-tion is easier to use and more broadly applicable due to the ready availability of binary code. Two key features needed for security instrumentations are (a) it should be applied to all application code, including code contained in various system and application libraries, and (b) it should be non-bypassable. So far, dynamic binary instrumentation (DBI) techniques have provided these features, whereas static bi-nary instrumentation (SBI) techniques have lacked them. These features, combined with ease of use, have made DBI the de facto choice for security instrumentations. However, DBI techniques can incur high overheads in several common usage scenarios, such as application startups, system-calls, and many real-world applications. We therefore develop a new platform for secure static binary instrumentation (PSI) that overcomes these drawbacks of DBI techniques, while retaining the security, robustness and ease-of-use features. We illustrate the versatility of PSI by developing several instrumentation applications: basic block counting, shadow stack defense against control-flow hijack and return-oriented programming attacks, and system call and library policy en-forcement. While being competitive with the best DBI tools on CPU-intensive SPEC 2006 benchmark, PSI provides an order of magnitude reduction in overheads on a collection of real-world applications.},
author = {Zhang, Mingwei and Qiao, Rui and Hasabnis, Niranjan and Sekar, R.},
doi = {10.1145/2576195.2576208},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2014 - A platform for secure static binary instrumentation.pdf:pdf},
isbn = {9781450327640},
issn = {03621340},
journal = {Proceedings of the 10th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments - VEE '14},
pages = {129--140},
title = {{A platform for secure static binary instrumentation}},
url = {http://dl.acm.org/citation.cfm?doid=2576195.2576208},
year = {2014}
}
@article{Modern2018,
author = {Modern, Introduction and Validation, Translation},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Modern, Validation - 2018 - Cross-Language Program Equivalence with Application to.pdf:pdf},
number = {1},
title = {{Cross-Language Program Equivalence with Application to}},
volume = {1},
year = {2018}
}
@article{Robbins2016,
abstract = {Reconstructing the meaning of a program from its binary executable is known as reverse engineering; it has a wide range of applications in software security, exposing piracy, legacy systems, etc. Since reversing is ultimately a search for meaning, there is much interest in inferring a type (a meaning) for the elements of a binary in a consistent way. Unfortunately existing approaches do not guarantee any semantic relevance for their reconstructed types. This paper presents a new and semantically-founded approach that provides strong guarantees for the reconstructed types. Key to our approach is the derivation of a witness program in a high-level language alongside the reconstructed types. This witness has the same semantics as the binary, is type correct by construction, and it induces a (justifiable) type assignment on the binary. Moreover, the approach effectively yields a type-directed decompiler. We formalise and implement the approach for reversing MinX, an abstraction of x86, to MinC, a type-safe dialect of C with recursive datatypes. Our evaluation compiles a range of textbook C algorithms to MinX and then recovers the original structures.},
author = {Robbins, Ed and King, Andy and Schrijvers, Tom},
doi = {10.1145/2914770.2837633},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robbins, King, Schrijvers - 2016 - From MinX to MinC semantics-driven decompilation of recursive datatypes.pdf:pdf},
isbn = {1595930566},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {decompilation,recursive datatypes,reverse engineering},
number = {1},
pages = {191--203},
title = {{From MinX to MinC: semantics-driven decompilation of recursive datatypes}},
url = {http://dl.acm.org/citation.cfm?doid=2914770.2837633},
volume = {51},
year = {2016}
}
@article{Lim,
abstract = {This paper describes the design and implementation of a language for specifying the semantics of an instruction set, along with a run-time system to support the static analysis of executables written in that instruction set. The work advances the state of the art by creating multiple analysis phases from a specification of the concrete operational semantics of the language to be analyzed.},
author = {Lim, Junghee and Reps, Thomas},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lim, Reps - Unknown - A System for Generating Static Analyzers for Machine Instructions(2).pdf:pdf},
title = {{A System for Generating Static Analyzers for Machine Instructions}},
url = {http://www.rw.cdl.uni-saarland.de/teaching/spa10/papers/stami.pdf}
}
@article{Hasabnis2015,
author = {Hasabnis, Niranjan and Sekar, R},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasabnis, Sekar - 2015 - Automatic Generation of Assembly to IR Translators Using Compilers.pdf:pdf},
journal = {8th Workshop on Architectural and Microarchitectural Support for Binary Translation},
pages = {1--7},
title = {{Automatic Generation of Assembly to IR Translators Using Compilers}},
year = {2015}
}
@article{Schkufza2016,
abstract = {The optimization of short sequences of loop-free, fixed-point assembly code sequences is an important problem in high-performance computing. However, the competing constraints of transformation correctness and performance improve-ment often force even special purpose compilers to pro-duce sub-optimal code. We show that by encoding these constraints as terms in a cost function, and using a Markov Chain Monte Carlo sampler to rapidly explore the space of all possible code sequences, we are able to generate aggres-sively optimized versions of a given target code sequence. Beginning from binaries compiled by llvm −O0, we are able to produce provably correct code sequences that either match or outperform the code produced by gcc −O3, icc −O3, and in some cases expert handwritten assembly. 1. INTRODUCTION For many application domains there is considerable value in producing the most performant code possible. However, the traditional structure of a compiler's optimization phase is ill-suited to this task. Factoring the optimization problem into a collection of small subproblems that can be solved independently—although suitable for generating consis-tently good code—can lead to sub-optimal results. In many cases, the best possible code can only be obtained through the simultaneous consideration of mutually dependent issues such as instruction selection, register allocation, and target-dependent optimization. Previous approaches to this problem have focused on the exploration of all possibilities within some limited class of code sequences. In contrast to a traditional compiler, which uses performance constraints to drive the generation of a sin-gle sequence, these systems consider multiple sequences and select the one that is best able to satisfy those constraints. An attractive feature of this approach is completeness: if a code sequence exists that meets the desired constraints it is guar-anteed to be found. However, completeness also places prac-tical limitations on the type of code that can be considered. These techniques are either limited to sequences that are shorter than the threshold at which many interesting optimi-zations take place or code written in simplified languages. We overcome this limitation through the use of incom-plete search: the competing requirements of correctness and performance improvement are encoded as terms in a cost function which is defined over the space of all loop-free x86{\_}64 instruction sequences, and the optimization task is formulated as a cost minimization problem. While the search space is highly irregular and not amenable to exact optimization techniques, we show that the com-mon approach of employing a Markov Chain Monte Carlo (MCMC) sampler to explore the function and produce low-cost samples is sufficient for producing high-quality code.},
author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex},
doi = {10.1145/2863701},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schkufza, Sharma, Aiken - 2016 - Stochastic program optimization.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {2},
pages = {114--122},
title = {{Stochastic program optimization}},
url = {http://dl.acm.org/citation.cfm?doid=2886013.2863701},
volume = {59},
year = {2016}
}
@article{Srinivasan2015b,
author = {Srinivasan, Venkatesh and Reps, Thomas and Srinivasan, Venkatesh and Reps, Thomas},
doi = {10.1145/2858965.2814321},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivasan et al. - 2015 - Partial evaluation of machine code.pdf:pdf},
isbn = {978-1-4503-3689-5},
issn = {03621340},
journal = {OOPSLA 2015:Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
keywords = {BTA,IA-32 instruction set,Partial evaluation,machine code,machine-code synthesis,specialization},
number = {10},
pages = {860--879},
title = {{Partial evaluation of machine code}},
url = {http://dl.acm.org/citation.cfm?doid=2858965.2814321},
volume = {50},
year = {2015}
}
@article{Reid2017,
author = {Reid, Alastair},
doi = {10.1109/FMCAD.2016.7886675},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reid - 2017 - Trustworthy specifications of ARM{\textregistered} v8-A and v8-M system level architecture.pdf:pdf},
isbn = {9780983567868},
journal = {Proceedings of the 16th Conference on Formal Methods in Computer-Aided Design, FMCAD 2016},
pages = {161--168},
title = {{Trustworthy specifications of ARM{\textregistered} v8-A and v8-M system level architecture}},
year = {2017}
}
@article{Hasabnis2016,
author = {Hasabnis, Niranjan},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasabnis - 2016 - Lifting Assembly to Intermediate Representation A Novel Approach Leveraging Compilers ∗.pdf:pdf},
isbn = {9781450340915},
journal = {ASPLOS},
number = {Il},
title = {{Lifting Assembly to Intermediate Representation : A Novel Approach Leveraging Compilers ∗}},
year = {2016}
}

@inproceedings{Heule2016a,
 author = {Heule, Stefan and Schkufza, Eric and Sharma, Rahul and Aiken, Alex},
 title = {Stratified Synthesis: Automatically Learning the x86-64 Instruction Set},
 booktitle = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI '16},
 year = {2016},
 isbn = {978-1-4503-4261-2},
 location = {Santa Barbara, CA, USA},
 pages = {237--250},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2908080.2908121},
 doi = {10.1145/2908080.2908121},
 acmid = {2908121},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ISA specification, program synthesis, x86-64},
}

@article{Srinivasan2015,
abstract = {In this paper, we present a technique to synthesize machine-code in-structions from a semantic specification, given as a Quantifier-Free Bit-Vector (QFBV) logic formula. Our technique uses an instantia-tion of the Counter-Example Guided Inductive Synthesis (CEGIS) framework, in combination with search-space pruning heuristics to synthesize instruction-sequences. To counter the exponential cost inherent in enumerative synthesis, our technique uses a divide-and-conquer strategy to break the input QFBV formula into in-dependent sub-formulas, and synthesize instructions for the sub-formulas. Synthesizers created by our technique could be used to create semantics-based binary rewriting tools such as optimizers, partial evaluators, program obfuscators/de-obfuscators, etc. Our ex-periments for Intel's IA-32 instruction set show that, in compari-son to our baseline algorithm, our search-space pruning heuristics reduce the synthesis time by a factor of 473, and our divide-and-conquer strategy reduces the synthesis time by a further 3 to 5 or-ders of magnitude.},
author = {Srinivasan, Venkatesh and Reps, Thomas},
doi = {10.1145/2737924.2737960},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivasan, Reps - 2015 - Synthesis of machine code from semantics.pdf:pdf},
isbn = {9781450334686},
issn = {15232867},
journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation - PLDI 2015},
keywords = {1,2,about the properties of,analysis,another potential use of,automatic pro-,binaries,cegis,d,divide-and-conquer,i,ia-32 instruction set,machine-code synthesis,smt},
pages = {596--607},
title = {{Synthesis of machine code from semantics}},
url = {http://dl.acm.org/citation.cfm?doid=2737924.2737960},
year = {2015}
}
@article{Reid2016,
abstract = {Although testing is the most widely used technique to control the quality of software systems, it is a topic that, until relatively recently, has received scant attention from the computer research community. Although some pioneering work was already done a considerable time ago [Cho78,GG83,How78,Mye79], the testing of software systems has never become a mainstream activity of scientific research. The reasons that are given to explain this situation usually include arguments to the effect that testing as a technique is inferior to verification — testing can show only the presence of errors, not their absence — and that we should therefore concentrate on developing theory and tools for the latter. It has also been frequently said that testing is by its very nature a non-formal activity, where formal methods and related tools are at best of little use. The first argument is incorrect in the sense that it gives an incomplete picture of the situation. Testing is inferior to verification if the verification model can be assumed to be correct and if its complexity can be handled correctly by the person and or tool involved in the verification task. If these conditions are not fulfilled, which is frequently the case, then testing is often the only available technique to increase the confidence in the correctness of a system. In this talk we will show that the second argument is flawed as well. It is based on the identification of testing with robustness testing, where it is precisely the objective to find out how the system behaves under unspecified circumstances. This excludes the important activity of conformance testing, which tries to test the extent to which system behaviour conforms to its specification. It is precisely in this area where formal methods and tools can help to derive tests systematically from specifications, which is a great improvement over laborious, error-prone and costly manual test derivation. In our talk we show how the process algebraic testing theory due to De Nicola and Hennessy [DNH84,DeN87], originally conceived out of semantic considerations, may be used to obtain principles for test derivation. We will give an overview of the evolution of these ideas over the past ten years or so, starting with the conformance testing theory of simple synchronously communicating reactive systems [Bri88,Lan90] and leading to realistic systems that involve sophisticated asynchronous message passing mechanisms [Tre96,HT97]. Written accounts can be found in [BHT97,He98]. We discuss how such ideas have been used to obtain modern test derivation tools, such as TVEDA and TGV [Pha94, CGPT96,FJJV96], and the tool set that is currently being developed in the C{\^{o}}te-de-Resyste project [STW96]. The advantage of a test theory that is based on well-established process algebraic theory is that in principle there exists a clear link between testing and verification, which allows the areas to share ideas and algorithms [FJJV96,VT98]. Time allowing, we look at some of the methodological differences and commonalities between model checking techniques and testing, one of the differences being that of state space coverage, and an important commonality that of test property selection. In recent years the research into the use of formal methods and tools for testing reactive systems has seen a considerable growth. An overview of different approaches and school of thought can be found in [BPS98], reporting on the first ever Dagstuhl seminar devoted to testing. The formal treatment of conformance testing based on process algebra and/or concurrency theory is certainly not the only viable approach. An important school of thought is the FSM-testing theory grown out of the seminal work of Chow [Cho78], of which a good overview is given in [LY96]. Another interesting formal approach to testing is based on abstract data type theory [Gau95,BGM91].},
archivePrefix = {arXiv},
arxivId = {1301.4779},
author = {Reid, Alastair and Chen, Rick and Deligiannis, Anastasios and Gilday, David and Hoyes, David and Keen, Will and Pathirane, Ashan and Shepherd, Owen and Vrabel, Peter and Zaidi, Ali},
doi = {10.1007/978-3-319-41540-6_3},
eprint = {1301.4779},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reid et al. - 2016 - End-to-end verification of ARM{\textregistered} processors with ISA-formal.pdf:pdf},
isbn = {9783319415390},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {42--58},
pmid = {4520227},
title = {{End-to-end verification of ARM{\textregistered} processors with ISA-formal}},
volume = {9780},
year = {2016}
}
@article{Zhang,
abstract = {Control-Flow Integrity (CFI) has been recognized as an important low-level security property. Its enforcement can defeat most injected and existing code attacks, in-cluding those based on Return-Oriented Programming (ROP). Previous implementations of CFI have required compiler support or the presence of relocation or debug information in the binary. In contrast, we present a tech-nique for applying CFI to stripped binaries on x86/Linux. Ours is the first work to apply CFI to complex shared libraries such as glibc. Through experimental evalu-ation, we demonstrate that our CFI implementation is effective against control-flow hijack attacks, and elimi-nates the vast majority of ROP gadgets. To achieve this result, we have developed robust techniques for disas-sembly, static analysis, and transformation of large bina-ries. Our techniques have been tested on over 300MB of binaries (executables and shared libraries).},
author = {Zhang, Mingwei and Sekar, R},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Sekar - Unknown - Control Flow Integrity for COTS Binaries Control Flow Integrity for COTS Binaries.pdf:pdf},
isbn = {978-1-931971-03-4},
title = {{Control Flow Integrity for COTS Binaries Control Flow Integrity for COTS Binaries *}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity13/sec13-paper{\_}zhang.pdf}
}
@article{Hasabnis2015a,
author = {Hasabnis, Niranjan and Sekar, R},
file = {:home/sdasgup3/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasabnis, Sekar - 2015 - Introduction Automatic Generation of Assembly to IR Translators Using Compilers Rely on manual development.pdf:pdf},
journal = {PPT},
title = {{Introduction Automatic Generation of Assembly to IR Translators Using Compilers Rely on manual development Manual development is problematic • Instruction sets are complex Our approach • Use modern compilers to build assembly-to-IR translators Steps in ou}},
year = {2015}
}

@inproceedings{Stoke2013,
 author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex},
 title = {Stochastic Superoptimization},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '13},
 year = {2013},
 isbn = {978-1-4503-1870-9},
 location = {Houston, Texas, USA},
 pages = {305--316},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2451116.2451150},
 doi = {10.1145/2451116.2451150},
 acmid = {2451150},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {64-bit, binary, markov chain monte carlo, mcmc, smt, stochastic search, superoptimization, x86, x86-64},
}
